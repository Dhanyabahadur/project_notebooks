{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24f1b16",
   "metadata": {},
   "source": [
    "<h1><center> Email Automation - Canada Data</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e589f468",
   "metadata": {},
   "source": [
    "**Project Aim: Automated Email Response with LLM**\n",
    "\n",
    "This project aims to automate email responses using Large Language Models (LLM). The focus is on enhancing communication efficiency through natural language understanding and generation. Key goals include efficient email handling, NLP integration, customization, adaptability, and scalability. The project utilizes advanced LLM, such as GPT-3, to comprehend diverse language structures and generate human-like responses, making it a user-friendly solution for streamlined email communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d1b27",
   "metadata": {},
   "source": [
    "#### Read in the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3544adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "/home/ec2-user/.local/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import email\n",
    "import copy\n",
    "import pandas as pd\n",
    "import extract_msg\n",
    "import numpy as np\n",
    "import re\n",
    "import openai\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "from cleantext import clean\n",
    "from fuzzywuzzy import fuzz, process \n",
    "import time\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import dask.dataframe as dd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import boto3\n",
    "import yaml\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta,FR,TU,WE\n",
    "\n",
    "openai.api_key = \"ENTER_YOUR_API_KEY\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924c73e3",
   "metadata": {},
   "source": [
    "#### Function with OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd2b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "import openai\n",
    "\n",
    "# define a retry decorator\n",
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 30,\n",
    "    errors: tuple = (openai.error.RateLimitError,openai.error.APIError,openai.error.Timeout,openai.error.APIConnectionError,openai.error.ServiceUnavailableError),\n",
    "):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize variables\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # Retry on specific errors\n",
    "            except errors as e:\n",
    "                # Increment retries\n",
    "                num_retries += 1\n",
    "\n",
    "                # Check if max retries has been reached\n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    "\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Raise exceptions for any errors not specified\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "@retry_with_exponential_backoff\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generate completion using OpenAI ChatCompletion API.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddfd5ae",
   "metadata": {},
   "source": [
    "#### Read in the Excel wih Expected Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93bd42c",
   "metadata": {},
   "source": [
    "* expected and predicted suffix\n",
    "* zipcode, city and country columns added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/home/ec2-user/python/Dhanya/email_automation/data/master_data_canada_18thAug_v3.xlsx')\n",
    "df = df.rename(columns={c: 'expected_'+ c for c in df.columns if c not in ['body', 'emailid', 'Email', 'email_processed',\n",
    "                                        'Exclusion Indicator','Comments_BG', 'Comments_BG_Binary']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604c33c",
   "metadata": {},
   "source": [
    "#### Remove Emails that arent exactly LCL Quotation requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Exclusion Indicator'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a273573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Comments_BG_Binary'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92901d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Comments_BG_Binary'] != df['Exclusion Indicator']), ['emailid','email_processed','Exclusion Indicator','Comments_BG_Binary','Comments_BG']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a150d3",
   "metadata": {},
   "source": [
    "**Preprocessing Functions to remove superflous information from the emails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9f90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to remove the strings enclosed by either <> or []\n",
    "\n",
    "def a(test_str):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in test_str:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '<':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == '>'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            ret += i\n",
    "    return ret\n",
    "\n",
    "def clean_hashtags(tweet):\n",
    "    \"\"\"\n",
    "    Remove hashtags from the text.\n",
    "    \"\"\"\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet))\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet))\n",
    "    return new_tweet2\n",
    "\n",
    "def clean_regards(text):\n",
    "    \"\"\"\n",
    "    Remove closing regards from the text.\n",
    "    \"\"\"\n",
    "    text = re.split(\"best regards|regards|thanks & regards|thanks and regards|warm regards|kind regards|yours sincerely|sincerely|Atenciosamente, Regards, Saludos,\", text)[0]\n",
    "    return text\n",
    "\n",
    "def clean_tail(text):\n",
    "    \"\"\"\n",
    "    Clean unnecessary tail information from the text.\n",
    "    \"\"\"\n",
    "    # text = re.split('our office working hours', text)[0]\n",
    "    # text = re.split('important note:', text)[0]\n",
    "    # text = re.split('note:', text)[0]\n",
    "    text = re.split('phone:', text)[0]\n",
    "    #text = re.split('from:', text)[0]\n",
    "    text = re.split('mob no', text)[0]\n",
    "    text = re.split('mob no:|mobile|email:', text)[0]\n",
    "    \n",
    "    #text = text.replace('caution: dear recipient, gulf craft will never communicate change of bank d= etails by email, if you receive any such emails from our pleas= e do not respond and contact our sales by telephone (preferable landline to= landline) and report the same to helpdesk in a separate e= -mail.','')\n",
    "    #text = text.replace('caution: dear recipient, gulf craft will never communicate change of bank details by email, if you receive any such emails from our domain/user, please do not respond and contact our sales by telephone (preferable landline to landline) and report the same to helpdesk@gulfcraftinc.com in a separate e-mail.','')\n",
    "    #text = text.replace('caution: dear recipient, gulf craft will never communicate change of bank details by email, if you receive any such emails from our domain/user, please do not respond and contact our sales by telephone (preferable landline to landline) and report the same to helpdesk in a separate e-mail.','')\n",
    "    #text = text.replace('*we strictly dont entertain quotation which includes iranian vessel, routing via iran or iran flagged vessel','')\n",
    "    text = text.replace('**this mail is from outside our organization. treat hyperlinks and attachments in this mail with caution**','')\n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    \"\"\"\n",
    "    Remove punctuations, links, mentions, and new line characters from the text.\n",
    "    \"\"\"\n",
    "    text = text.replace('_x000D_','').replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower()\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    banned_list = string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    text = \" \".join(word.strip() for word in re.split('#|_', text))\n",
    "    return text\n",
    "\n",
    "def remove_mult_spaces(text):\n",
    "    \"\"\"\n",
    "    Remove multiple spaces from the text.\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "def filter_chars(text):\n",
    "    \"\"\"\n",
    "    Filter special characters such as & and $ present in some words.\n",
    "    \"\"\"\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) or ('&' in word) or ('/' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by applying various cleaning functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = a(text)\n",
    "    text = strip_all_entities(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    # text = clean_regards(text)\n",
    "    text = clean_tail(text)\n",
    "    text = clean_hashtags(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab23a2",
   "metadata": {},
   "source": [
    "Applying the preprocessing function on our emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email_processed'] = df['email_processed'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37bd8b",
   "metadata": {},
   "source": [
    "## Classification function to determine the type of Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(email):\n",
    "    prompt = f\"\"\"\n",
    "    The task is to categorize an email into one of four classes: \"Quotation Request\", \"Track and Trace Request\"\n",
    "    , \"Sailing Schedule Request\" and \"Random Conversation\".\n",
    "    A \"Quotation Request\" is a customer's request for pricing information, including origin and destination locations, and cargo details.\n",
    "    If the email is not a \"Quotation Request\", check for the next three classes.\n",
    "    A \"Sailing Schedule Request\" is a customer request for sailing schedule or vessel schedule indicated by keywords like \"sailing schedule request\", \"vessel availability\", \"next vessel\" or \"vessel schedule\".\n",
    "    A \"Track and Trace Request\" asks about the status, ETA or delivery status of a shipment. Look for key phrases like \"update us on the subject shipment\", \"status of cargo\", \"delivery status\" or \"when will it be delivered?\".\n",
    "    Any email which looks like a reply to a \"Quotation Request\" is a \"Random Conversation\".\n",
    "    Any email which looks like a reply to a \"Sailing Schedule Request\" is a \"Random Conversation\".\n",
    "    Any email which looks like a reply to a \"Track and Trace Request\" is a \"Random Conversation\".\n",
    "    Any email which does not fall under the above 3 classes is a \"Random Conversation\".\n",
    "    Give the result as only the class name and nothing else.\n",
    "    Review: ```{email}```\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b913a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = ['predicted_class']\n",
    "df[classify] = ''\n",
    "for i, value in enumerate(df['email_processed']):\n",
    "    result = make_classification(value)\n",
    "    df.loc[i, classify] = result\n",
    "    del(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_class'].value_counts() #.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e9b4f",
   "metadata": {},
   "source": [
    "#### Extracting the Quotation Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0509ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation = df.loc[df['predicted_class'] == 'Quotation Request'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8bf57",
   "metadata": {},
   "source": [
    "#### New Addition - Remove Email Signature / Footer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_from(email):\n",
    "    prompt = f\"\"\"\n",
    "    Remove the email signature block.\n",
    "\n",
    "    Remove any disclaimers found as well.\n",
    "      \n",
    "    Review: ```{email}```\n",
    "    \n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Subject line has to be retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87916e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, value in enumerate(df_quotation['email_processed']):\n",
    "    result = extract_information_from(value)\n",
    "    df_quotation.loc[i, 'email_processed_revised'] = result\n",
    "    del(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b8c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation['email_processed_revised'] = df_quotation['email_processed_revised'].str.replace('Review: ```',' ')\n",
    "df_quotation['email_processed_revised'] = df_quotation['email_processed_revised'].str.replace('Review:', ' ')\n",
    "df_quotation['email_processed_revised'] = df_quotation['email_processed_revised'].str.replace(' tml ', 'terminal')\n",
    "df_quotation['email_processed'] = df_quotation['email_processed_revised'].str.replace('^\\W+','' )\n",
    "df_quotation['email_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918b973",
   "metadata": {},
   "source": [
    "<h1><center> Read in the Cleaned and Processed Emails </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470b91e",
   "metadata": {},
   "source": [
    "#### Try RegEx for Origin/Destination Type Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quotation.loc[df_quotation['email_processed'].str.contains('shipment from turkey to toronto'),['email_processed'] ]\n",
    "\n",
    "# df_quotation[df_quotation[\"email_processed\"].str.contains(r'\\b(shipment from) (?s).*? (to).+\\b', regex=True, case=False)] \n",
    "\n",
    "# df_quotation[df_quotation[\"email_processed\"].str.contains(r'\\b(cfs) (?s).*? (to door).+\\b', regex=True, case=False)] \n",
    "\n",
    "# df_quotation[df_quotation[\"email_processed\"].str.contains(r'\\b(pol:) (?s).*? (pod:).+\\b', regex=True, case=False)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f66b8a",
   "metadata": {},
   "source": [
    "#### Read in the CFS Destination File - It gives us the Port Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_fri_dt1 = (datetime.now() + relativedelta(weekday=FR(-1))).strftime('%Y-%m-%d')\n",
    "mapping_data = dd.read_parquet('s3://ds-data-model/ECU_MASTER/CFSDestination/dt='+ last_fri_dt1,engine=\"fastparquet\").compute()\n",
    "mapping_data.rename(columns={'Country_Id_Name' : 'Country'}, inplace=True)\n",
    "mapping_data.drop_duplicates(inplace = True)\n",
    "mapping_data = mapping_data.loc[:,['Iata', 'Port','UnCode','Country']].apply(lambda x : x.str.strip().str.upper()) \n",
    "# Remove airport codes\n",
    "\n",
    "keywords_to_remove = ['APT', 'AIRPORT']\n",
    "#ports_to_remove = ['CANWP', 'CAVST']\n",
    "mapping_data.dropna(inplace=True)#.reset_index(drop=True)\n",
    "mapping_data.drop_duplicates(inplace=True)#.reset_index(drop=True)\n",
    "mapping_data = mapping_data.loc[~(mapping_data['Port'].str.contains('|'.join(keywords_to_remove), case=False)==True)]\n",
    "\n",
    "#mapping_data = mapping_data.loc[~(mapping_data['UnCode'].str.contains('|'.join(ports_to_remove), case=False, na=False)==True)]\n",
    "# Remove Ports that ends in .\n",
    "\n",
    "mapping_data = mapping_data[~mapping_data['Port'].str.endswith('.')]\n",
    "mapping_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97919d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1f06f",
   "metadata": {},
   "source": [
    "#### Function to extract Origin and Destination from email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0711952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     If the keywords like ('cfs', 'terminal', 'freezone','tml','term') are associated with origin or destination. Extract those along with these values.\n",
    "#     Keywords like ('dap','ddp') indicate delivery location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c511e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_from(email):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    Please provide origin location (full address) and destination location (full address) in the following format:\n",
    "    \n",
    "    Origin Location:\n",
    "    Destination Location:\n",
    "    \n",
    "    The response should include only 2 lines with the field mentioned above.\n",
    "    Don't include anything other than these 2 lines and 2 fields even if the field is missing and there is insufficient information.\n",
    "    \n",
    "    Review: ```{email}```\n",
    "    \n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbf4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, value in enumerate(df_quotation['email_processed']):\n",
    "    result = extract_information_from(value)\n",
    "    df_quotation.loc[i, 'predicted_origin_destination'] = result\n",
    "    del(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e5240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb37023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the extracted origin and destination column returned by ChatGPT\n",
    "\n",
    "df_quotation['predicted_origin_destination'] = df_quotation['predicted_origin_destination'].str.replace('Origin Location:\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4357519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the predicted origin and destination into two separate strings\n",
    "\n",
    "for i, value in enumerate(df_quotation['email_processed']):\n",
    "    input_string = df_quotation.loc[i, 'predicted_origin_destination']\n",
    "    split_parts = input_string.split('\\n', 1)\n",
    "    try:\n",
    "        if(len(split_parts)==2):\n",
    "            df_quotation.loc[i, 'predicted_origin'] = split_parts[0]\n",
    "            df_quotation.loc[i, 'predicted_destination'] = split_parts[1]\n",
    "        elif(len(split_parts==1)):\n",
    "            df_quotation.loc[i, 'predicted_destination'] = split_parts[0]\n",
    "    except:\n",
    "        df_quotation.loc[i, 'predicted_origin'] = ''\n",
    "        df_quotation.loc[i, 'predicted_destination'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a15bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input = df_quotation.copy()\n",
    "\n",
    "# cleaning the extracted origin and destination\n",
    "\n",
    "df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('Origin Location: ', '')\n",
    "df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('Origin Location:', '')\n",
    "\n",
    "df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('\\nDestination Location:','')\n",
    "df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('\\nDestination Location: ','')\n",
    "df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('Destination Location:','')\n",
    "df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5076656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98240d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No. of entries where Origin entries are null :',df_quotation_input[df_quotation_input['predicted_origin']==''].shape[0])\n",
    "print('No. of entries where Destination entries are null : ',df_quotation_input[df_quotation_input['predicted_destination']==''].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84df6f",
   "metadata": {},
   "source": [
    "#### Analyze the entries that ChatGPT has failed to predict the Origin and Destination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ad914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quotation_input.loc[(df_quotation_input['predicted_origin']=='') | (df_quotation_input['predicted_destination']==''),\n",
    "#                       ['emailid','email_processed','predicted_origin','predicted_destination']].to_excel('Null_Prediction.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9caba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.loc[(df_quotation_input['predicted_origin'] == '') | (df_quotation_input['predicted_destination'] == ''),\n",
    "                      ['emailid','email_processed','predicted_origin','predicted_destination']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dadf0f",
   "metadata": {},
   "source": [
    "**Removing instances where expected value is null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3bba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input = df_quotation_input[~(df_quotation_input['expected_Origin_type'].isnull())  &\n",
    "                  ~(df_quotation_input['expected_Destination_type'].isnull())]\n",
    "\n",
    "df_quotation_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input = df_quotation_input.loc[~(df_quotation_input['expected_emailfrom'].isnull())]\n",
    "df_quotation_input = df_quotation_input.loc[~(df_quotation_input['expected_emailto'].isnull())]\n",
    "df_quotation_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc542f4",
   "metadata": {},
   "source": [
    "#### Exlude irrelevant emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79139c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input = df_quotation_input[(df_quotation_input['Comments_BG_Binary'] !='EXCLUDE')]\n",
    "df_quotation_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ea27e",
   "metadata": {},
   "source": [
    "<h1><center> Metric 1 : Derive the OriginType and Destintion Type </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5eca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.lower()\n",
    "df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.lower()\n",
    "df_quotation_input['email_processed'] = df_quotation_input['email_processed'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24b029",
   "metadata": {},
   "source": [
    "#### Step 1 : Hardcoded rules to classify port and door from the unparsed origin and destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_door_keywords = ['door to door','door to port', 'door to cfs' , 'door to terminal', 'door to term',\n",
    "                        'door - door', 'door - port', 'door : door', 'door : port', \n",
    "                        'door-door', 'door-port', 'from door', 'pickup location',\n",
    "                        'from location',' pick up ', ' pickup ', 'pick-up', 'pickup address', 'pick up address', \n",
    "                       'pu charge', 'p/u', ' door pickup ', ' from location ',\n",
    "                       ' exw ', 'exworks','ex works', 'ex-works', 'ex - works', 'ex work','exwork',\n",
    "                        'supplier address','door/door']\n",
    "\n",
    "destination_door_keywords = ['port to door','door to door', 'cfs to door', 'terminal to door',\n",
    "                        'port - door', 'door - door', 'port-door', 'door-door','port : door','door : door',\n",
    "                        'shipment to door', ' dap ', 'delivery address', 'door delivery', 'delivery at place',\n",
    "                ' ddp ','ddp ' ' place of reciept ','destination delivery','final destination',\n",
    "            ' to location ','delivery to door', 'residential delivery', 'rate to door', 'delivery at door','door/door']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29280fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input['predicted_fromtype'] = np.where( df_quotation_input['email_processed'].str.contains('|'.join(origin_door_keywords)), 'door', 'port')\n",
    "df_quotation_input['predicted_totype'] = np.where( df_quotation_input['email_processed'].str.contains('|'.join(destination_door_keywords)), 'door', 'port')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda86bc",
   "metadata": {},
   "source": [
    "#### Update the Origin/ Destination classification Method indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34167259",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_quotation_input['predicted_fromtype'].unique())\n",
    "print(df_quotation_input['predicted_totype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40691db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin Type Match Mismatch\n",
    "\n",
    "df_quotation_input.loc[df_quotation_input['predicted_fromtype'] != df_quotation_input['expected_Origin_type'], ['emailid','email_processed',\n",
    "                                            'predicted_origin','predicted_fromtype','expected_Origin_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination Type Match Mismatch\n",
    "\n",
    "df_quotation_input.loc[df_quotation_input['predicted_totype'] != df_quotation_input['expected_Destination_type'], ['emailid','email_processed',\n",
    "                                            'predicted_destination','predicted_totype','expected_Destination_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38090dfa",
   "metadata": {},
   "source": [
    "- Both scenarios are misclassified due to ambiguity in the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197454e7",
   "metadata": {},
   "source": [
    "#### Intermediate Check for Origin/Destination Type match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a34221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin Type Match \n",
    "\n",
    "df_quotation_input[df_quotation_input['predicted_fromtype'] == df_quotation_input['expected_Origin_type']].shape[0]/df_quotation_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination Type Match\n",
    "\n",
    "df_quotation_input[df_quotation_input['predicted_totype'] == df_quotation_input['expected_Destination_type']].shape[0]/df_quotation_input.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9776429",
   "metadata": {},
   "source": [
    "#### Remove instances where the Expected Values are null (else it will distort the accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7db36f",
   "metadata": {},
   "source": [
    "- Origin / Destination Type is considered for now\n",
    "- We will have to add/ switch to other columns too as we progress testing metric by metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input[~(df_quotation_input['expected_Origin_type'].isnull())  &\n",
    "                  ~(df_quotation_input['expected_Destination_type'].isnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11600658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input = df_quotation_input[~(df_quotation_input['expected_Origin_type'].isnull())  &\n",
    "                  ~(df_quotation_input['expected_Destination_type'].isnull())]\n",
    "\n",
    "df_quotation_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a44fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.loc[df_quotation_input['expected_emailfrom'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85891cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input = df_quotation_input.loc[~(df_quotation_input['expected_emailfrom'].isnull())]\n",
    "df_quotation_input = df_quotation_input.loc[~(df_quotation_input['expected_emailto'].isnull())]\n",
    "df_quotation_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778d0fd",
   "metadata": {},
   "source": [
    "#### Add an indicator to identify if the Predicted and Expected values of Origin/Destination Type match or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8918424a",
   "metadata": {},
   "source": [
    "- We have to add this for other columns too as we progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac15030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input['OriginType_Match'] = np.where((df_quotation_input['predicted_fromtype'] == df_quotation_input['expected_Origin_type']),\n",
    "           'Matched', 'Not Matched' )\n",
    "\n",
    "df_quotation_input['DestinationType_Match'] = np.where((df_quotation_input['predicted_totype'] == df_quotation_input['expected_Destination_type']),\n",
    "           'Matched', 'Not Matched' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1218af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input['OriginType_Match'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e81e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input['DestinationType_Match'].value_counts(normalize= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09435559",
   "metadata": {},
   "source": [
    "<h1><center> Analyze the extracted origin and destination columns </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492043e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.loc[df_quotation_input['email_processed'].str.contains('henri'),['email_processed',\n",
    "                                    'predicted_fromtype','expected_Origin_type',\n",
    "                                    'predicted_totype','expected_Destination_type',\n",
    "                                    'predicted_origin','predicted_destination']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5a665",
   "metadata": {},
   "source": [
    "<h1><center> Measuring Accuracy for Origin / Destination Type </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098386b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto encode the columns to be measured \n",
    "\n",
    "df_quotation_input['expected_Origin_type'] = np.where(df_quotation_input['expected_Origin_type']=='port', 1, 0)\n",
    "df_quotation_input['predicted_fromtype'] = np.where(df_quotation_input['predicted_fromtype']=='port', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a856e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_confmat(actual, predicted):\n",
    "    # extract the different classes\n",
    "    classes = np.unique(actual)\n",
    "    # initialize the confusion matrix\n",
    "    confmat = np.zeros((len(classes), len(classes)))\n",
    "    # loop across the different combinations of actual / predicted classes\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            # count the number of instances in each combination of actual / predicted\n",
    "            confmat[i, j] = np.sum((actual==classes[i]) & (predicted == classes[j]))\n",
    "    return confmat\n",
    "\n",
    "print(comp_confmat(df_quotation_input['expected_Origin_type'], df_quotation_input['predicted_fromtype']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "OriginType_Confusion_Matrix = pd.crosstab(df_quotation_input['predicted_fromtype'], df_quotation_input['expected_Origin_type'])\n",
    "OriginType_Confusion_Matrix.columns = ['Expected - Door', 'Expected - Port']\n",
    "OriginType_Confusion_Matrix = OriginType_Confusion_Matrix.reset_index()\n",
    "OriginType_Confusion_Matrix['predicted_fromtype'] = np.where((OriginType_Confusion_Matrix['predicted_fromtype'] == 0), 'Predicted - Door', 'Predicted - Port')\n",
    "OriginType_Confusion_Matrix.rename(columns = {'predicted_fromtype' : 'OriginType'}, inplace = True)\n",
    "OriginType_Confusion_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf6b1e",
   "metadata": {},
   "source": [
    "#### Summary of Origin Type Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30291880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "OriginType_Accuracy = ((OriginType_Confusion_Matrix['Expected - Door'][0] + OriginType_Confusion_Matrix['Expected - Port'][1])/(OriginType_Confusion_Matrix['Expected - Door'][0] + OriginType_Confusion_Matrix['Expected - Port'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - Door'][1] + OriginType_Confusion_Matrix['Expected - Port'][1]))*100\n",
    "print('Accuracy of Origin Type : ' , OriginType_Accuracy)\n",
    "\n",
    "# Recall of Door Class within OriginType\n",
    "OriginType_DoorRecall = ((OriginType_Confusion_Matrix['Expected - Door'][0])/(OriginType_Confusion_Matrix['Expected - Door'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - Door'][1] ))*100\n",
    "print('Recall of Door class with Origin Type : ' , OriginType_DoorRecall)\n",
    "\n",
    "# Precision of Door Class within OriginType\n",
    "OriginType_DoorPrecision = ((OriginType_Confusion_Matrix['Expected - Door'][0])/(OriginType_Confusion_Matrix['Expected - Door'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - Port'][0] ))*100\n",
    "print('Precision of Door class with Origin Type : ' , OriginType_DoorPrecision)\n",
    "\n",
    "# Recall of Port Class within OriginType\n",
    "OriginType_PortRecall = ((OriginType_Confusion_Matrix['Expected - Port'][1])/(OriginType_Confusion_Matrix['Expected - Port'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - Port'][1] ))*100\n",
    "print('Recall of Port class with Origin Type : ' , OriginType_PortRecall)\n",
    "\n",
    "# Precision of Port Class within OriginType\n",
    "OriginType_PortPrecision = ((OriginType_Confusion_Matrix['Expected - Port'][1])/(OriginType_Confusion_Matrix['Expected - Port'][1] + \n",
    "OriginType_Confusion_Matrix['Expected - Door'][1] ))*100\n",
    "print('Precision of Port class with Origin Type : ' , OriginType_PortPrecision)\n",
    "\n",
    "# Orgin Type correct Match % using Email Keywords \n",
    "#origintype_emailmethod = df_quotation_input[(df_quotation_input['predicted_fromtype_from'] == 'email_keywords') & (df_quotation_input['OriginType_Match'] == 'Matched')].shape[0]/df_quotation_input[df_quotation_input['predicted_fromtype_from'] == 'email_keywords'].shape[0]\n",
    "#print('% of OriginType entries correctly matched using Email Keywords : ' , origintype_emailmethod*100)\n",
    "\n",
    "# Orgin Type correct Match % using ChatGPT \n",
    "#origintype_chatgptmethod = df_quotation_input[(df_quotation_input['predicted_fromtype_from'] != 'email_keywords') & (df_quotation_input['OriginType_Match'] == 'Matched')].shape[0]/df_quotation_input[df_quotation_input['predicted_fromtype_from'] != 'email_keywords'].shape[0]\n",
    "#print('% of OriginType entries correctly matched using ChatGPT : ' , origintype_chatgptmethod*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf6e9d",
   "metadata": {},
   "source": [
    "#### Analyze the mismatched Origin Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.loc[(df_quotation_input['OriginType_Match'] != 'Matched'), ['emailid','email_processed',\n",
    "                                            'predicted_origin','predicted_fromtype','expected_Origin_type']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4785b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.loc[\n",
    "                       (df_quotation_input['email_processed'].str.contains('henri')),['emailid','email_processed','expected_Origin_type','expected_Destination_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d51a3",
   "metadata": {},
   "source": [
    "#### Summary Destination Type Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto encode the Expected and Predicted OriginType Columns \n",
    "\n",
    "df_quotation_input['expected_Destination_type'] = np.where(df_quotation_input['expected_Destination_type']=='port', 1, 0)\n",
    "\n",
    "df_quotation_input['predicted_totype'] = np.where(df_quotation_input['predicted_totype']=='port', 1, 0)\n",
    "\n",
    "DestinationType_Confusion_Matrix = pd.crosstab(df_quotation_input['predicted_totype'], df_quotation_input['expected_Destination_type'])\n",
    "DestinationType_Confusion_Matrix.columns = ['Expected - Door', 'Expected - Port']\n",
    "DestinationType_Confusion_Matrix = DestinationType_Confusion_Matrix.reset_index()\n",
    "DestinationType_Confusion_Matrix['predicted_totype'] = np.where((DestinationType_Confusion_Matrix['predicted_totype'] == 0), 'Predicted - Door', 'Predicted - Port')\n",
    "DestinationType_Confusion_Matrix.rename(columns = {'predicted_totype' : 'DestinationType'}, inplace = True)\n",
    "DestinationType_Confusion_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "DestinationType_Accuracy = ((DestinationType_Confusion_Matrix['Expected - Door'][0] + DestinationType_Confusion_Matrix['Expected - Port'][1])/(DestinationType_Confusion_Matrix['Expected - Door'][0] + DestinationType_Confusion_Matrix['Expected - Port'][0] + \n",
    "DestinationType_Confusion_Matrix['Expected - Door'][1] + DestinationType_Confusion_Matrix['Expected - Port'][1]))*100\n",
    "print('Accuracy of Destination Type : ' , DestinationType_Accuracy)\n",
    "\n",
    "# Recall of Door Class within DestinationType\n",
    "DestinationType_DoorRecall = ((DestinationType_Confusion_Matrix['Expected - Door'][0])/(DestinationType_Confusion_Matrix['Expected - Door'][0] + \n",
    "DestinationType_Confusion_Matrix['Expected - Door'][1] ))*100\n",
    "print('Recall of Door class with Destination Type : ' , DestinationType_DoorRecall)\n",
    "\n",
    "# Precision of Door Class within DestinationType\n",
    "DestinationType_DoorPrecision = ((DestinationType_Confusion_Matrix['Expected - Door'][0])/(DestinationType_Confusion_Matrix['Expected - Door'][0] + \n",
    "DestinationType_Confusion_Matrix['Expected - Port'][0] ))*100\n",
    "print('Precision of Door class with Destination Type : ' , DestinationType_DoorPrecision)\n",
    "\n",
    "# Recall of Port Class within DestinationType\n",
    "DestinationType_PortRecall = ((DestinationType_Confusion_Matrix['Expected - Port'][1])/(DestinationType_Confusion_Matrix['Expected - Port'][0] + \n",
    "DestinationType_Confusion_Matrix['Expected - Port'][1] ))*100\n",
    "print('Recall of Port class with Destination Type : ' , DestinationType_PortRecall)\n",
    "\n",
    "# Precision of Port Class within DestinationType\n",
    "DestinationType_PortPrecision = ((DestinationType_Confusion_Matrix['Expected - Port'][1])/(DestinationType_Confusion_Matrix['Expected - Port'][1] + \n",
    "DestinationType_Confusion_Matrix['Expected - Door'][1] ))*100\n",
    "print('Precision of Port class with Destination Type : ' , DestinationType_PortPrecision)\n",
    "\n",
    "# Destination Type correct Match % using Email Keywords \n",
    "# destinationtype_emailmethod = df_quotation_input[(df_quotation_input['predicted_totype_from'] == 'email_keywords') & (df_quotation_input['DestinationType_Match'] == 'Matched')].shape[0]/df_quotation_input[df_quotation_input['predicted_totype_from'] == 'email_keywords'].shape[0]\n",
    "# print('% of DestinationType entries correctly matched using Email Keywords : ' , destinationtype_emailmethod*100)\n",
    "\n",
    "# # Destination Type correct Match % using ChatGPT \n",
    "# destinationtype_chatgptmethod = df_quotation_input[(df_quotation_input['predicted_totype_from'] != 'email_keywords') & (df_quotation_input['DestinationType_Match'] == 'Matched')].shape[0]/df_quotation_input[df_quotation_input['predicted_totype_from'] != 'email_keywords'].shape[0]\n",
    "# print('% of DestinationType entries correctly matched using ChatGPT : ' , destinationtype_chatgptmethod*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb28619",
   "metadata": {},
   "source": [
    "#### Analyze the mismatched Destintation Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2492c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotation_input.loc[(df_quotation_input['DestinationType_Match'] != 'Matched'), ['emailid','email_processed','predicted_destination','predicted_totype','expected_Destination_type']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp  = df_quotation_input[['email_processed','expected_Origin_type', 'expected_Destination_type',\n",
    "#                             'predicted_fromtype', 'predicted_totype',\n",
    "#        'predicted_fromtype_from', 'predicted_totype_from', 'OriginType_Match',\n",
    "#        'DestinationType_Match']]\n",
    "# temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quotation_input[['predicted_origin', 'expected_emailfrom', \n",
    "#                     'predicted_destination', 'expected_emailto']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544dbe5e",
   "metadata": {},
   "source": [
    "<h1><center> Clean the Predicted Origin and Destination Columns </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42320c0b",
   "metadata": {},
   "source": [
    "- These extracted columns contain superflous words/sentences that bring down the accuracy\n",
    "\n",
    "- Remove Keywords like :  'dap charges', 'door', 'not provided', 'not provided.', 'unknown' etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7cccb",
   "metadata": {},
   "source": [
    "#### Removing these hardcoded techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_quotation_input[['predicted_origin', 'predicted_destination']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Keywords from Origin\n",
    "\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('dap charges','')\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('door','')\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('not provided.','')\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('not provided','')\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('unknown','')\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('please advise address','')\n",
    "# df_quotation_input['predicted_origin'] = df_quotation_input['predicted_origin'].str.replace('(please advise address)','')\n",
    "\n",
    "# # Remove keywords from Destination\n",
    "\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('dap charges','')\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('door','')\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('not provided.','')\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('not provided','')\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('unknown','')\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('please advise address','')\n",
    "# df_quotation_input['predicted_destination'] = df_quotation_input['predicted_destination'].str.replace('(please advise address)','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa59de",
   "metadata": {},
   "source": [
    "###### Cleaning Steps :\n",
    "\n",
    "* **For Port**\n",
    "1. Get Uncode for port (both origin and destination)\n",
    "2. Clean the origin and destination \n",
    "3. fuzzy match with cfs destination file.\n",
    "4. look through values to decide score criteria\n",
    "5. if fuzzy match fails, then look for python library which can give us uncode\n",
    "6. then chatgpt prompt\n",
    "7. If the uncode is null after these steps, then classify it as door and do the preprocessing of door\n",
    "\n",
    "* **For Door**\n",
    "1. Use chatgpt prompt to extract city, zipcode and country\n",
    "2. If zipcode is missing, take help of library to get unique zipcode\n",
    "3. If the location doesn't have unique zipcode, classify it as port -> then fuzzy match for port and then chagpt\n",
    "4. if the location has city, zipcode and country. then do post processing for required output\n",
    "\n",
    "* Finally calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90025ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14eb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03177005",
   "metadata": {},
   "source": [
    "<h1><center> Metric 2 : Port/ Location Extraction and Cleaning - Origin + Destination </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca19a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping_data['Country'] = mapping_data['Country'].str.replace('TAIWAN, CHINA', 'TAIWAN')\n",
    "country_list = mapping_data['Country'].str.lower().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['predicted_fromtype'] = np.where(df['predicted_fromtype']=='1', 'port', 'door')\n",
    "# df['predicted_totype'] = np.where(df['predicted_totype']=='1', 'port', 'door')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112a8dd",
   "metadata": {},
   "source": [
    "#### Step 1 : Fuzzy Match with the CFS Mapping File to derive the Port UNCODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3974524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "def fuzzy_match(name, choices):\n",
    "    return process.extractOne(name, choices, scorer=fuzz.token_set_ratio)\n",
    "\n",
    "def to_string(x):\n",
    "    try:\n",
    "        return str(x)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "# 'predicted_origin', 'predicted_destination'\n",
    "\n",
    "def port_codes_mapping(df1):\n",
    "    df = df1.copy()\n",
    "    df['predicted_origin'] = df['predicted_origin'].fillna('')\n",
    "    df['predicted_destination'] = df['predicted_destination'].fillna('')\n",
    "    # stripping the values in order to remove empty space prsent before and after the string\n",
    "    df['predicted_origin'] = df['predicted_origin'].str.strip()\n",
    "    df['predicted_destination'] = df['predicted_destination'].str.strip()\n",
    "    df['predicted_origin'] = df['predicted_origin'].replace(np.nan, '')\n",
    "    df['predicted_destination'] = df['predicted_destination'].replace(np.nan, '')\n",
    "    df['predicted_origin_match'] = df['predicted_origin'].apply(fuzzy_match, args=(mapping_data['Port'],))\n",
    "    df[['predicted_origin_match_port', 'predicted_origin_match_score', \n",
    "        'predicted_origin_match_junk']] = pd.DataFrame(df['predicted_origin_match'].tolist(), index=df.index)\n",
    "    df = df.applymap(to_string)\n",
    "    df['predicted_destination_match'] = df['predicted_destination'].apply(fuzzy_match, args=(mapping_data['Port'],))\n",
    "    df[['predicted_destination_match_port', 'predicted_destination_match_score',\n",
    "       'predicted_destination_match_junk']] = pd.DataFrame(df['predicted_destination_match'].tolist(), index=df.index)\n",
    "    quotations_origin_match = pd.merge(df, mapping_data[['Port','UnCode']],how='left',\n",
    "                                      left_on='predicted_origin_match_port',\n",
    "                                      right_on='Port')\n",
    "    quotations_origin_match = quotations_origin_match.rename(columns={\n",
    "        'UnCode':'predicted_origin_uncode'})\n",
    "    quotations_destination_match = pd.merge(quotations_origin_match,\n",
    "                            mapping_data[['Port','UnCode']],how='left',\n",
    "                                left_on='predicted_destination_match_port',\n",
    "                                            right_on='Port')\n",
    "    quotations_destination_match = quotations_destination_match.rename(columns={\n",
    "        'UnCode':'predicted_destination_uncode'})\n",
    "    quotations_destination_match['predicted_origin_match_score'] = quotations_destination_match['predicted_origin_match_score'].astype(int)\n",
    "    quotations_destination_match['predicted_destination_match_score'] = quotations_destination_match['predicted_destination_match_score'].astype(int)\n",
    "    quotations_destination_match.loc[(quotations_destination_match['predicted_origin_match_score']>=65) & (~quotations_destination_match['predicted_origin'].isin(['not provided', 'unknown'])) & (~quotations_destination_match['predicted_origin'].isin(country_list)), 'predicted_origin_port'] = quotations_destination_match['predicted_origin_uncode']\n",
    "    quotations_destination_match.loc[(quotations_destination_match['predicted_destination_match_score']>=65) & (~quotations_destination_match['predicted_destination'].isin(['not provided', 'unknown'])) & (~quotations_destination_match['predicted_destination'].isin(country_list)), 'predicted_destination_port'] = quotations_destination_match['predicted_destination_uncode']\n",
    "    return quotations_destination_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['predicted_destination_match_score'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['predicted_origin_match_score'].astype(int).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52901be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = port_codes_mapping(df_quotation_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe139fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['predicted_origin','predicted_fromtype', 'predicted_origin_port', 'predicted_origin_match_score', \n",
    "        'predicted_origin_match_junk', 'predicted_destination','predicted_totype','predicted_destination_port', 'predicted_destination_match_score',\n",
    "       'predicted_destination_match_junk']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ef02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['predicted_fromtype'].unique())\n",
    "print(df['predicted_totype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925cf035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Auto encoded columns\n",
    "\n",
    "df['predicted_fromtype'] = np.where((df['predicted_fromtype'] == '1'), 'port', 'door')\n",
    "df['predicted_totype'] = np.where((df['predicted_totype'] == '1'), 'port', 'door')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['predicted_fromtype'].unique())\n",
    "print(df['predicted_totype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70895bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['expected_emailfrom','expected_emailto' ]] = df[['expected_emailfrom','expected_emailto' ]].apply(lambda x : x.str.strip().str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Origin Ports match score\n",
    "\n",
    "print('% of Origin Ports that have been mapped to a UNCODE with a fuzzy score of 100: ',(df[(df['predicted_fromtype'] =='port') & (df['predicted_origin_match_score'] == 100)].shape[0]/df[(df['predicted_fromtype'] =='port')].shape[0])*100)\n",
    "# Analyze the Origin Ports match score\n",
    "\n",
    "print('% of Destination Ports that have been mapped to a UNCODE with a fuzzy score of 100: ',(df[(df['predicted_totype'] =='port') & (df['predicted_destination_match_score'] == 100)].shape[0]/df[(df['predicted_totype'] =='port')].shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_fromtype'] == 'port') & (df['predicted_origin_port'] != df['expected_emailfrom']),['emailid','email_processed','predicted_fromtype','expected_Origin_type','predicted_origin',\n",
    "         'predicted_origin_uncode','predicted_origin_match_score','predicted_origin_port',\n",
    "                                              'expected_emailfrom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze the Origin Ports with a fuzzy score < 100\n",
    "\n",
    "# df.loc[(df['predicted_origin_match_score'] < 100) & (df['predicted_fromtype'] =='port'),['predicted_origin', 'predicted_origin_port', 'predicted_origin_match_score', \n",
    "#         'predicted_origin_match_junk', 'predicted_destination','predicted_destination_port', 'predicted_destination_match_score',\n",
    "#        'predicted_destination_match_junk']].sort_values(by = 'predicted_origin_match_score', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc384d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze the Destination Ports with a fuzzy score < 100\n",
    "\n",
    "# df.loc[(df['predicted_destination_match_score'] < 100) & (df['predicted_totype'] =='port'),['predicted_origin', 'predicted_origin_port', 'predicted_origin_match_score', \n",
    "#         'predicted_origin_match_junk', 'predicted_destination','predicted_destination_port', 'predicted_destination_match_score',\n",
    "#        'predicted_destination_match_junk']].sort_values(by = 'predicted_destination_match_score', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404dc9c",
   "metadata": {},
   "source": [
    "#### Step 2 : If any Origin/Destination Ports have no UNCODES found - Try looking up using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56492a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_port'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_code(location):\n",
    "    prompt = f\"\"\"\n",
    "    What is the UN/LOCCODE for {location} sea port? Result should be only the code and nothing else.\n",
    "    \"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2589edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_port'] = df['predicted_origin_port'].replace(np.nan, '')\n",
    "df['predicted_destination_port'] = df['predicted_destination_port'].replace(np.nan, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['predicted_fromtype','predicted_origin_port','predicted_origin','predicted_totype','predicted_destination','predicted_destination_port']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc50165",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predicted_destination\n",
    "# predicted_destination_match_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ad456",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, value in enumerate(df['email_processed']):\n",
    "    fromtype = df.loc[i, 'predicted_fromtype']\n",
    "    totype = df.loc[i, 'predicted_totype']\n",
    "    predicted_origin_port = df.loc[i, 'predicted_origin_port']\n",
    "    predicted_destination_port = df.loc[i, 'predicted_destination_port']\n",
    "    predicted_origin = df.loc[i, 'predicted_origin']\n",
    "    predicted_destination = df.loc[i, 'predicted_destination']\n",
    "    \n",
    "    if(('port' in fromtype.lower())&(predicted_origin_port!='')):\n",
    "        df.loc[i, 'predicted_emailfrom'] = predicted_origin_port\n",
    "\n",
    "    if(('port' in fromtype.lower())&(predicted_origin_port=='')):\n",
    "        df.loc[i, 'predicted_emailfrom'] = port_code(predicted_origin)\n",
    "        \n",
    "    if(('port' in totype.lower())&(predicted_destination_port!='')):\n",
    "        df.loc[i, 'predicted_emailto'] = predicted_destination_port\n",
    "        \n",
    "    if(('port' in totype.lower())&(predicted_destination_port=='')):\n",
    "        df.loc[i, 'predicted_emailto'] = port_code(predicted_destination)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_emailfrom'] = df['predicted_emailfrom'].str.replace(' ', '')\n",
    "df['predicted_emailto'] = df['predicted_emailto'].str.replace(' ', '')\n",
    "df['predicted_emailfrom'] = df['predicted_emailfrom'].astype(str)\n",
    "df['predicted_emailto'] = df['predicted_emailto'].astype(str)\n",
    "df['predicted_emailfrom'] = df['predicted_emailfrom'].apply(lambda x: '' if len(x)!=5 else x)\n",
    "df['predicted_emailto'] = df['predicted_emailto'].apply(lambda x: '' if len(x)!=5 else x)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3eedd",
   "metadata": {},
   "source": [
    "#### Validation of Origin Port Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa49e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origin Port UNCODE validation\n",
    "\n",
    "df.loc[(df['predicted_fromtype'] == 'port'),['predicted_fromtype','predicted_origin',\n",
    "         'predicted_origin_uncode','predicted_origin_match_score','predicted_origin_port',\n",
    "                                             'predicted_emailfrom', 'expected_emailfrom']].sort_values(by = 'predicted_origin_match_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf04ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (df[(df['predicted_fromtype'] == 'port') & (df['predicted_emailfrom']==df['expected_emailfrom'])].shape[0]/df[(df['predicted_fromtype'] == 'port')].shape[0])*100\n",
    "print('Expected and Predicted Origin UNCODES match {}% of the times when the Origin Type is Port'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55176e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ports where Expected vs Predicted UNCODEs on the Origin side don't match\n",
    "\n",
    "df.loc[(df['predicted_fromtype'] == 'port') & (df['predicted_emailfrom'] != df['expected_emailfrom']),['emailid','email_processed','predicted_fromtype','predicted_origin',\n",
    "         'predicted_origin_uncode','predicted_origin_match_score','predicted_origin_port',\n",
    "                                             'predicted_emailfrom', 'expected_emailfrom']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fbb7a",
   "metadata": {},
   "source": [
    "#### Valiadation of Destination Port Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392408a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ports where Expected vs Predicted UNCODEs on the Destination side don't match\n",
    "\n",
    "x = (df[(df['predicted_totype'] == 'port')  & (df['predicted_emailto']==df['expected_emailto'])].shape[0]/df[(df['predicted_totype'] == 'port')].shape[0])*100\n",
    "print('Expected and Predicted Destination UNCODES match {}% of the times when the Destination Type is Port'.format(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e89874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.loc[(df['predicted_totype'] == 'port') & (df['predicted_emailto'] != df['expected_emailto']),['email_processed','predicted_totype','predicted_destination',\n",
    "         'predicted_destination_uncode','predicted_destination_match_score','predicted_destination_port',\n",
    "                                             'predicted_emailto', 'expected_emailto']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b003b71",
   "metadata": {},
   "source": [
    "**new york, usa\t- these kind of case have been dealt below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41136004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Origin Ports match score\n",
    "\n",
    "print('% of Origin Ports that have been mapped to a UNCODE: ',(df[(df['predicted_fromtype'] =='port') & (df['predicted_emailfrom'] != '') ].shape[0]/df[(df['predicted_fromtype'] =='port')].shape[0])*100)\n",
    "# Analyze the Origin Ports match score\n",
    "\n",
    "print('% of Destination Ports that have been mapped a UNCODE: ',(df[(df['predicted_totype'] =='port') & (df['predicted_emailto'] != '')].shape[0]/df[(df['predicted_totype'] =='port')].shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61041488",
   "metadata": {},
   "source": [
    "#### Exception Handling Technique for Predicted Port Names when one City exists in Different Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_fromtype']=='port') & (df['predicted_origin'].str.contains('canada')) &  (df['predicted_origin'].str.contains('vancouver')) ,['predicted_fromtype','predicted_origin','predicted_emailfrom','expected_emailfrom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa46a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['predicted_fromtype'] == 'port') & (df['predicted_emailfrom']==df['expected_emailfrom'])].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac02fdd",
   "metadata": {},
   "source": [
    "### Exception Handling for Cities that exist in different Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137fec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Canada \n",
    "\n",
    "df['predicted_emailfrom'] = np.where((df['predicted_fromtype']=='port') & \n",
    "                                    (df['predicted_origin_port'].isin(['USVAN'])), 'CAVAN', df['predicted_emailfrom'])\n",
    "\n",
    "df['predicted_emailto'] = np.where((df['predicted_totype']=='port')&\n",
    "                                 (df['predicted_destination_port'].isin(['USVAN'])), 'CAVAN', df['predicted_emailto'])\n",
    "\n",
    "# Fix New York\n",
    "\n",
    "df['predicted_emailfrom'] = np.where((df['predicted_fromtype']=='port') & \n",
    "                                    (df['predicted_origin_port'].isin(['GBNWY'])), 'USNYC', df['predicted_emailfrom'])\n",
    "\n",
    "df['predicted_emailto'] = np.where((df['predicted_totype']=='port')&\n",
    "                                 (df['predicted_destination_port'].isin(['GBNWY'])), 'USNYC', df['predicted_emailto'])\n",
    "\n",
    "# Fix London\n",
    "\n",
    "df['predicted_emailfrom'] = np.where((df['predicted_fromtype']=='port') & \n",
    "                                    (df['predicted_origin_port'].isin(['ZAELS', 'GBLGP', 'GBLHR','USLDN'])), 'GBLON', df['predicted_emailfrom'])\n",
    "\n",
    "df['predicted_emailto'] = np.where((df['predicted_totype']=='port')&\n",
    "                                 (df['predicted_destination_port'].isin(['ZAELS', 'GBLGP', 'GBLHR','USLDN'])), 'GBLON', df['predicted_emailto'])\n",
    "\n",
    "# Fix Cartagena\n",
    "\n",
    "df['predicted_emailfrom'] = np.where((df['predicted_fromtype']=='port') & \n",
    "                                    (df['predicted_origin_port'].isin(['CLCGN', 'ESCAR'])), 'COCTG', df['predicted_emailfrom'])\n",
    "\n",
    "df['predicted_emailto'] = np.where((df['predicted_totype']=='port')&\n",
    "                                 (df['predicted_destination_port'].isin(['CLCGN', 'ESCAR'])), 'COCTG', df['predicted_emailto'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the Origin Prediction Null? - Looks like Prediction Origin is incorrect \n",
    "\n",
    "df.loc[(df['predicted_fromtype'] =='port') & (df['predicted_emailfrom'] == ''),['predicted_fromtype','predicted_origin',\n",
    "         'predicted_origin_uncode','predicted_origin_match_score','predicted_origin_port',\n",
    "                                             'predicted_emailfrom', 'expected_emailfrom'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c606d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the Destination Prediction Null ? Probably using only CFS Dest is a better approach\n",
    "\n",
    "df.loc[(df['predicted_totype'] =='port') & (df['predicted_emailto'] == ''),['predicted_totype','predicted_destination',\n",
    "         'predicted_destination_uncode','predicted_destination_match_score','predicted_destination_port',\n",
    "                                             'predicted_emailto', 'expected_emailto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1110bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (df[(df['predicted_fromtype'] == 'port') & (df['predicted_emailfrom']==df['expected_emailfrom'])].shape[0]/df[(df['predicted_fromtype'] == 'port')].shape[0])*100\n",
    "print('Expected and Predicted Origin UNCODES match {}% of the times when the Origin Type is Port'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (df[(df['predicted_totype'] == 'port')  & (df['predicted_emailto']==df['expected_emailto'])].shape[0]/df[(df['predicted_totype'] == 'port')].shape[0])*100\n",
    "print('Expected and Predicted Destination UNCODES match {}% of the times when the Destination Type is Port'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214773fd",
   "metadata": {},
   "source": [
    "#### Analysis of Mismatched Predicted vs Expected Ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_fromtype'] == 'port') & (df['predicted_emailfrom']!=df['expected_emailfrom']),['predicted_fromtype','predicted_origin',\n",
    "         'predicted_origin_uncode','predicted_origin_match_score','predicted_origin_port',\n",
    "                                             'predicted_emailfrom', 'expected_emailfrom']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea066bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_totype'] == 'port') & (df['predicted_emailto']!=df['expected_emailto']),['email_processed','predicted_totype','predicted_destination',\n",
    "         'predicted_destination_uncode','predicted_destination_match_score','predicted_destination_port',\n",
    "                                             'predicted_emailto', 'expected_emailto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fa3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'predicted_emailto':'predicted_port_destination',\n",
    "                    'predicted_emailfrom':'predicted_port_origin'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff351352",
   "metadata": {},
   "source": [
    "<h1><center> 2. Door Booking Cleaning :Extract City, Zip and Country ( Origin + Destination ) </h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f12532",
   "metadata": {},
   "source": [
    "### Method 2. : Use ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def door_address(location):\n",
    "    prompt = f\"\"\"\n",
    "    Extract city, zipcode and country from the given location.\n",
    "    \n",
    "    City: [city]\n",
    "    Zipcode:[zipcode]\n",
    "    Country:[country]\n",
    "        \n",
    "    The response should include only 3 lines with 3 fields mentioned above in the same order. Return fields with \"None\" if there is insufficient information.\n",
    "    \n",
    "    Review: ```{location}```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df96582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_dest_zip_match'] = np.where(df['predicted_destination'].str.contains('\\d') == True, 'Zip Found','Not Found')\n",
    "df['predicted_origin_zip_match'] = np.where(df['predicted_origin'].str.contains('\\d') == True, 'Zip Found','Not Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987914a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_door_columns = ['predicted_origin_city_zipcode_country']\n",
    "destination_door_columns = ['predicted_destination_city_zipcode_country']\n",
    "df[origin_door_columns] = ['']\n",
    "df[destination_door_columns] = ['']\n",
    "\n",
    "for i, value in enumerate(df['email_processed']):\n",
    "    fromtype = df.loc[i, 'predicted_fromtype']\n",
    "    totype = df.loc[i, 'predicted_totype']\n",
    "    predicted_origin = df.loc[i, 'predicted_origin']\n",
    "    predicted_destination = df.loc[i, 'predicted_destination']\n",
    "    predicted_origin_zip_match = df.loc[i, 'predicted_origin_zip_match']\n",
    "    predicted_dest_zip_match = df.loc[i, 'predicted_dest_zip_match']\n",
    "    \n",
    "    if (('door' in fromtype.lower()) | ('Zip Found' in predicted_origin_zip_match))  :\n",
    "        result_origin = door_address(predicted_origin)\n",
    "        df.loc[i, origin_door_columns] = result_origin\n",
    "        \n",
    "    if (('door' in totype.lower()) | ('Zip Found' in predicted_dest_zip_match)) :\n",
    "        result_destination = door_address(predicted_destination)\n",
    "        df.loc[i, destination_door_columns] = result_destination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16185c6c",
   "metadata": {},
   "source": [
    "#### Parse, Clean and Analyse the Predicted City, Zip and Country ( Origin )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50362240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_city'] = df['predicted_origin_city_zipcode_country'].str.split('\\n').str[0].str.replace('City: ','').replace('',np.nan).str.strip().fillna('None')\n",
    "df['predicted_origin_zipcode'] = df['predicted_origin_city_zipcode_country'].str.split('\\n').str[1].str.replace('Zipcode: ','').replace('',np.nan).str.strip().fillna('None')\n",
    "df['predicted_origin_country'] = df['predicted_origin_city_zipcode_country'].str.split('\\n').str[2].str.replace('Country: ','').replace('',np.nan).str.strip().fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_fromtype'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbcce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check for null values\n",
    "\n",
    "print(df.loc[(df['predicted_fromtype'] == 'door') & ((df['predicted_origin_city'] == 'None') | \n",
    "   (df['predicted_origin_zipcode'] == 'None') | (df['predicted_origin_country'] == 'None')) , \n",
    "  ['predicted_origin_city_zipcode_country', 'predicted_origin_city', \n",
    "    'predicted_origin_zipcode', 'predicted_origin_country','expected_emailfrom']].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0113a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_fromtype'] == 'door') & ((df['predicted_origin_city'] == 'None') | \n",
    "   (df['predicted_origin_zipcode'] == 'None') | (df['predicted_origin_country'] == 'None')), \n",
    "  ['email_processed','predicted_origin', 'predicted_origin_city', \n",
    "    'predicted_origin_zipcode', 'predicted_origin_country','expected_emailfrom']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f441ed7",
   "metadata": {},
   "source": [
    "#### Parse, Clean and Analyse the Predicted City, Zip and Country ( Door )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_city'] = df['predicted_destination_city_zipcode_country'].str.split('\\n').str[0].str.replace('City: ','').replace('',np.nan).str.strip().fillna('None')\n",
    "df['predicted_destination_zipcode'] = df['predicted_destination_city_zipcode_country'].str.split('\\n').str[1].str.replace('Zipcode: ','').replace('',np.nan).str.strip().fillna('None')\n",
    "df['predicted_destination_country'] = df['predicted_destination_city_zipcode_country'].str.split('\\n').str[2].str.replace('Country: ','').replace('',np.nan).str.strip().fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f5e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check for null values\n",
    "\n",
    "print(   df.loc[ (df['predicted_totype'] == 'door') & ((df['predicted_destination_city'] == 'None') | \n",
    "          (df['predicted_destination_zipcode'] == 'None') | \n",
    "          (df['predicted_destination_country'] == 'None')) , \n",
    "          ['email_processed','predicted_destination', 'predicted_destination_city', \n",
    "          'predicted_destination_zipcode', 'predicted_destination_country','expected_emailto']].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_totype'] == 'door') & ((df['predicted_destination_city'] == 'None') | \n",
    "   (df['predicted_destination_zipcode'] == 'None') | (df['predicted_destination_country'] == 'None')), \n",
    "  ['email_processed','predicted_destination', 'predicted_destination_city', \n",
    "    'predicted_destination_zipcode', 'predicted_destination_country','expected_emailto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.strip().str.lower()\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].str.strip().str.lower()\n",
    "\n",
    "df['predicted_destination_zipcode'] = df['predicted_destination_zipcode'].str.strip().str.lower()\n",
    "df['predicted_origin_zipcode'] = df['predicted_origin_zipcode'].str.strip().str.lower()\n",
    "\n",
    "df['predicted_destination_city'] = df['predicted_destination_city'].str.strip().str.lower()\n",
    "df['predicted_origin_city'] = df['predicted_origin_city'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db682fa",
   "metadata": {},
   "source": [
    "#### Exception Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba46d2c",
   "metadata": {},
   "source": [
    "#### Scenario 1 :  When a territory has been predicted instead of the Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c699a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b858c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the required Country Format\n",
    "\n",
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.lower().str.replace('united states', 'u.s.a.')\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].str.lower().str.replace('united states', 'u.s.a.')\n",
    "\n",
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.lower().str.replace('taiwan', 'taiwan, china')\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].str.lower().str.replace('taiwan', 'taiwan, china')\n",
    "\n",
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.lower().str.replace('uk', 'united kingdom')\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].str.lower().str.replace('uk', 'united kingdom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a458b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country'] = np.where(df['predicted_origin_country'].str.lower().isin(country_list), df['predicted_origin_country'], 'none')\n",
    "df['predicted_destination_country'] = np.where(df['predicted_destination_country'].str.lower().isin(country_list), df['predicted_destination_country'], 'none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f184436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d65ab",
   "metadata": {},
   "source": [
    "#### Scenario 2 : When a territory has been predicted instead of the Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0487271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_zipcode'] = np.where(df['predicted_destination_zipcode'].str.contains('\\d') == True, df['predicted_destination_zipcode'], 'none')\n",
    "df['predicted_origin_zipcode'] = np.where(df['predicted_origin_zipcode'].str.contains('\\d') == True, df['predicted_origin_zipcode'], 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_zipcode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a670d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_zipcode'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a217f6",
   "metadata": {},
   "source": [
    "\n",
    "## Method 2 : Use Inbuilt Geopy Library to determine the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"https\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24063bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rev_predicted_destination'] = df['predicted_destination_city'] + ' ' + df['predicted_destination_zipcode'] + ' ' + df['predicted_destination_country']\n",
    "df['rev_predicted_origin'] = df['predicted_origin_city'] + ' ' + df['predicted_origin_zipcode'] + ' ' + df['predicted_origin_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ab477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rev_predicted_destination'] = df['rev_predicted_destination'].str.replace('none','').str.strip()\n",
    "df['rev_predicted_origin'] = df['rev_predicted_origin'].str.replace('none','').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d155e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['rev_predicted_destination','predicted_destination','predicted_destination_city','predicted_destination_zipcode','predicted_destination_country']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d05992",
   "metadata": {},
   "source": [
    "#### Origin Side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['predicted_orig_geoaddress'] = df['rev_predicted_origin'].str.lower().apply(lambda x: geolocator.geocode(x,  language='en')).astype(str)\n",
    "df['predicted_orig_geoaddress'] = df['predicted_orig_geoaddress'].str.lower().str.strip()\n",
    "df['predicted_origin_zipcode1'] = df['predicted_orig_geoaddress'].str.split(',').str[-2].str.strip()\n",
    "df['predicted_origin_country1'] = df['predicted_orig_geoaddress'].str.split(',').str[-1].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5527f",
   "metadata": {},
   "source": [
    "#### Destination Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['predicted_dest_geoaddress'] = df['rev_predicted_destination'].str.lower().apply(lambda x: geolocator.geocode(x,  language='en')).astype(str)\n",
    "df['predicted_dest_geoaddress'] = df['predicted_dest_geoaddress'].str.lower().str.strip()\n",
    "df['predicted_destination_zipcode1'] = df['predicted_dest_geoaddress'].str.split(',').str[-2].str.strip()\n",
    "df['predicted_destination_country1'] = df['predicted_dest_geoaddress'].str.split(',').str[-1].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71f6e7",
   "metadata": {},
   "source": [
    "#### Cleaning Country and Zip on both sides as done above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2340b",
   "metadata": {},
   "source": [
    "- Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36af8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a74ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276afbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country1'] = df['predicted_origin_country1'].str.replace('united states', 'u.s.a.')\n",
    "df['predicted_origin_country1'] = np.where(df['predicted_origin_country1'].isin(country_list), df['predicted_origin_country1'], 'none')\n",
    "\n",
    "df['predicted_origin_country1'] = df['predicted_origin_country1'].str.replace('taiwan', 'taiwan, china')\n",
    "df['predicted_origin_country1'] = np.where(df['predicted_origin_country1'].isin(country_list), df['predicted_origin_country1'], 'none')\n",
    "\n",
    "df['predicted_origin_country1'] = df['predicted_origin_country1'].str.replace('uk', 'united kingdom')\n",
    "df['predicted_origin_country1'] = np.where(df['predicted_origin_country1'].isin(country_list), df['predicted_origin_country1'], 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country1'] = df['predicted_destination_country1'].str.replace('united states', 'u.s.a.')\n",
    "df['predicted_destination_country1'] = np.where(df['predicted_destination_country1'].isin(country_list), df['predicted_destination_country1'], 'none')\n",
    "\n",
    "df['predicted_destination_country1'] = df['predicted_destination_country1'].str.replace('taiwan', 'taiwan, china')\n",
    "df['predicted_destination_country1'] = np.where(df['predicted_destination_country1'].isin(country_list), df['predicted_destination_country1'], 'none')\n",
    "\n",
    "df['predicted_destination_country1'] = df['predicted_destination_country1'].str.replace('uk', 'united kingdom')\n",
    "df['predicted_destination_country1'] = np.where(df['predicted_destination_country1'].isin(country_list), df['predicted_destination_country1'], 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country1'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28020d6a",
   "metadata": {},
   "source": [
    "- Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab2c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_zipcode1'] = np.where(df['predicted_destination_zipcode1'].str.contains('\\d') == True, df['predicted_destination_zipcode1'], 'none')\n",
    "df['predicted_origin_zipcode1'] = np.where(df['predicted_origin_zipcode1'].str.contains('\\d') == True, df['predicted_origin_zipcode1'], 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_zipcode1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_zipcode1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Zip on the Origin and Destination sides\n",
    "\n",
    "df['predicted_origin_zipcode'] = np.where( ((df['predicted_fromtype'] == 'door') | (df['predicted_origin_zip_match'] == 'Zip Found')) & (df['predicted_origin_country'] !='none') & (df['predicted_origin_city']!='none')\n",
    "         & (df['predicted_origin_zipcode']=='none'), df['predicted_origin_zipcode1'], df['predicted_origin_zipcode'])\n",
    "\n",
    "# Fill Zip on the Destination side \n",
    "\n",
    "df['predicted_destination_zipcode'] = np.where( ((df['predicted_totype'] == 'door') | (df['predicted_dest_zip_match'] == 'Zip Found')) & (df['predicted_destination_country'] !='none') & (df['predicted_destination_city']!='none')\n",
    "          & (df['predicted_destination_zipcode']=='none'), df['predicted_destination_zipcode1'], df['predicted_destination_zipcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Country on the Origin side\n",
    "\n",
    "df['predicted_origin_country'] = np.where( ((df['predicted_fromtype'] == 'door') | (df['predicted_origin_zip_match'] == 'Zip Found')) & (df['predicted_origin_city'] !='none') & (df['predicted_origin_zipcode']!='none')\n",
    "        & (df['predicted_origin_country']=='none'), df['predicted_origin_country1'], df['predicted_origin_country'])\n",
    "\n",
    "# Fill Country on the Destination side\n",
    "\n",
    "df['predicted_destination_country'] = np.where( ((df['predicted_totype'] == 'door') | (df['predicted_dest_zip_match'] == 'Zip Found')) & (df['predicted_destination_city'] !='none') & (df['predicted_destination_zipcode']!='none')\n",
    "        & (df['predicted_destination_country']=='none'), df['predicted_destination_country1'], df['predicted_destination_country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['predicted_destination'].str.contains('tainan')][['predicted_totype','predicted_destination','predicted_dest_zip_match','predicted_destination_city','predicted_destination_zipcode','predicted_destination_country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f8c49",
   "metadata": {},
   "source": [
    "### Finally : Check the remaining Null cases and use ChatGPT to solve for Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_fromtype'] == 'door') & ((df['predicted_origin_city'] == 'none') | \n",
    "   (df['predicted_origin_zipcode'] == 'None') | (df['predicted_origin_country'] == 'none')), \n",
    "  ['email_processed','predicted_origin', 'predicted_origin_city', \n",
    "    'predicted_origin_zipcode', 'predicted_origin_country','expected_emailfrom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac69774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_totype'] == 'door') & ((df['predicted_destination_city'] == 'none') | \n",
    "   (df['predicted_destination_zipcode'] == 'none') | (df['predicted_destination_country'] == 'none')), \n",
    "  ['email_processed', 'rev_predicted_destination', 'predicted_destination', 'predicted_destination_city', \n",
    "    'predicted_destination_zipcode', 'predicted_destination_country1','expected_emailto']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rev_predicted_destination'] = df['predicted_destination_city'] + ' ' + df['predicted_destination_zipcode']\n",
    "# df['rev_predicted_origin'] = df['predicted_origin_city'] + ' ' + df['predicted_origin_zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a70cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_address(location):\n",
    "    prompt = f\"\"\"\n",
    "    Return the Country\n",
    "    \n",
    "    Country:[country]\n",
    "        \n",
    "    Return fields with \"None\" if there is insufficient information.\n",
    "    \n",
    "    Review: ```{location}```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_dest_country_chatgpt'] = df['predicted_destination'].apply(country_address)\n",
    "df['predicted_dest_country_chatgpt'] = df['predicted_dest_country_chatgpt'].str.replace('Country: ', '').str.strip().str.lower()\n",
    "df['predicted_dest_country_chatgpt'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_orig_country_chatgpt'] = df['predicted_origin'].apply(country_address)\n",
    "df['predicted_orig_country_chatgpt'] = df['predicted_orig_country_chatgpt'].str.replace('Country: ', '').str.strip().str.lower()\n",
    "# add a line if not in country list return none\n",
    "# maybe split country , city , zip question within the door address function itself\n",
    "df['predicted_orig_country_chatgpt'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415529ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Country on the Origin side\n",
    "\n",
    "df['predicted_origin_country'] = np.where( ((df['predicted_fromtype'] == 'door') | (df['predicted_origin_zip_match'] == 'Zip Found')) & (df['predicted_origin_city'] !='none') & (df['predicted_origin_zipcode']!='none')\n",
    "        & (df['predicted_origin_country']=='none'),df['predicted_orig_country_chatgpt'], df['predicted_origin_country'])\n",
    "\n",
    "# Fill Country on the Destination side\n",
    "\n",
    "df['predicted_destination_country'] = np.where( ((df['predicted_totype'] == 'door') | (df['predicted_dest_zip_match'] == 'Zip Found')) & (df['predicted_destination_city'] !='none') & (df['predicted_destination_zipcode']!='none')\n",
    "        & (df['predicted_destination_country']=='none'), df['predicted_dest_country_chatgpt'], df['predicted_destination_country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_fromtype'] == 'door') & ((df['predicted_origin_city'] == 'none') | \n",
    "   (df['predicted_origin_zipcode'] == 'None') | (df['predicted_origin_country'] == 'none')), \n",
    "  ['email_processed','predicted_origin', 'predicted_origin_city', \n",
    "    'predicted_origin_zipcode', 'predicted_origin_country','expected_emailfrom']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c47d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['predicted_totype'] == 'door') & ((df['predicted_destination_city'] == 'none') | \n",
    "   (df['predicted_destination_zipcode'] == 'none') | (df['predicted_destination_country'] == 'none')), \n",
    "  ['email_processed', 'rev_predicted_destination', 'predicted_destination', 'predicted_destination_city', \n",
    "    'predicted_destination_zipcode', 'predicted_destination_country1','expected_emailto']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899cb77",
   "metadata": {},
   "source": [
    "#### Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1cb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_destination'].str.contains('anchorage, ak 99516'),['predicted_totype','predicted_destination_city','predicted_destination_zipcode','predicted_destination_country','predicted_destination_country1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abddca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_destination'].str.contains('tainan'),['predicted_totype','predicted_destination_city','predicted_destination_zipcode','predicted_destination_country','predicted_destination_country1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80087a90",
   "metadata": {},
   "source": [
    "* We were able to extract these cases earlier - WEST VANCOUVER - V7W - CANADA - we have to include keyword dap in prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02695d",
   "metadata": {},
   "source": [
    "**Post Processing for zip code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adebde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.strip()\n",
    "df['predicted_destination_city'] = df['predicted_destination_city'].str.strip()\n",
    "df['predicted_destination_zipcode'] = df['predicted_destination_zipcode'].str.strip()\n",
    "\n",
    "df['predicted_origin_zipcode'] = df['predicted_origin_zipcode'].str.strip()\n",
    "df['predicted_origin_city'] = df['predicted_origin_city'].str.strip()\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].str.strip()\n",
    "\n",
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.lower()\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaca96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # access only first part of the zipcode\n",
    "\n",
    "first_part = ['canada','united kingdom','uk', 'bc', 'on', 'qc', 'on', 'bc', 'ab', 'mb']\n",
    "canada = ['canada', 'bc', 'on', 'qc', 'mb', 'ab']\n",
    "no_space = ['netherlands','czech republic']\n",
    "no_zipcode = ['bahrain','chile','el salvador','ghana','hong kong','jordan','malaysia','morocco','pakistan','panama','qatar','saudi arabia','sri lanka','u.a.e.','ukraine','vietnam', ' u.a.e.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492077b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_origin_country'].str.lower().isin(first_part),'predicted_origin_zipcode']=df['predicted_origin_zipcode'].str.split(' ').str[0]\n",
    "df.loc[df['predicted_origin_country'].str.lower().isin(canada),'predicted_origin_zipcode']=df['predicted_origin_zipcode'].str.split('-').str[0]\n",
    "df.loc[df['predicted_origin_country'].str.lower().isin(canada),'predicted_origin_zipcode']=df['predicted_origin_zipcode'].str[:3]\n",
    "df.loc[df['predicted_origin_country'].str.lower().isin(no_space),'predicted_origin_zipcode']=df['predicted_origin_zipcode'].str.replace(' ','')\n",
    "df.loc[df['predicted_origin_country'].str.lower().isin(no_zipcode),'predicted_origin_door_location']=  df['predicted_origin_city'] + ' - ' + df['predicted_origin_country']\n",
    "\n",
    "df['predicted_destination_country'] = df['predicted_destination_country'].str.lower()\n",
    "df.loc[df['predicted_destination_country'].str.lower().isin(first_part),'predicted_destination_zipcode']=df['predicted_destination_zipcode'].str.split(' ').str[0]\n",
    "df.loc[df['predicted_destination_country'].str.lower().isin(canada),'predicted_destination_zipcode']=df['predicted_destination_zipcode'].str.split('-').str[0]\n",
    "df.loc[df['predicted_destination_country'].str.lower().isin(canada),'predicted_destination_zipcode']=df['predicted_destination_zipcode'].str[:3]\n",
    "df.loc[df['predicted_destination_country'].str.lower().isin(no_space),'predicted_destination_zipcode']=df['predicted_destination_zipcode'].str.replace(' ','')\n",
    "df.loc[df['predicted_destination_country'].str.lower().isin(no_zipcode),'predicted_destination_door_location']=  df['predicted_destination_city'] + ' - ' + df['predicted_destination_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country'] = df['predicted_origin_country'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_zipcode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2146e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_origin_zipcode'] = df['predicted_origin_zipcode'].str.replace('non', 'None')\n",
    "df['predicted_origin_zipcode'] = df['predicted_origin_zipcode'].str.replace('nonee', 'None')\n",
    "df['predicted_origin_zipcode'] = df['predicted_origin_zipcode'].fillna('None')\n",
    "df['predicted_origin_city'] = df['predicted_origin_city'].fillna('None')\n",
    "df['predicted_origin_country'] = df['predicted_origin_country'].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2430733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_destination_zipcode'] = df['predicted_destination_zipcode'].str.replace('non', 'None')\n",
    "df['predicted_destination_zipcode'] = df['predicted_destination_zipcode'].str.replace('nonee', 'None')\n",
    "df['predicted_destination_zipcode'] = df['predicted_destination_zipcode'].fillna('None')\n",
    "df['predicted_destination_city'] = df['predicted_destination_city'].fillna('None')\n",
    "df['predicted_destination_country'] = df['predicted_destination_country'].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_destination'].str.contains('anchorage, ak 99516'),['predicted_destination_country']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a7507",
   "metadata": {},
   "source": [
    "### Create a Single column for Predicted Origin and Predicted Destination respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f6bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_door_destination'] = df['predicted_destination_city'].str.strip().str.upper() + ' - ' + df['predicted_destination_zipcode'].str.strip().str.upper() + ' - '+ df['predicted_destination_country'].str.strip().str.upper()\n",
    "df['predicted_door_origin'] = df['predicted_origin_city'].str.strip().str.upper() + ' - ' + df['predicted_origin_zipcode'].str.strip().str.upper() + ' - '+ df['predicted_origin_country'].str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_destination'].str.contains('anchorage, ak 99516'),['predicted_door_destination','predicted_destination_country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When Origin is a Door\n",
    "\n",
    "df.loc[:,['predicted_totype','predicted_origin','predicted_destination','predicted_port_destination','predicted_door_destination','expected_emailto']]#.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4978ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When Origin is a Port \n",
    "\n",
    "df.loc[:,['predicted_fromtype','predicted_origin','predicted_destination','predicted_port_origin','predicted_door_origin','expected_emailfrom']]#.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6b687",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set Final Predicted Destination column \n",
    "\n",
    "df['predicted_Destination_Final'] = np.where( ((df['predicted_totype'] == 'door') ) , df['predicted_door_destination'], df['predicted_port_destination'])\n",
    "\n",
    "# Set Final Predicted Origin column # \n",
    "\n",
    "df['predicted_Origin_Final'] = np.where( ((df['predicted_fromtype'] == 'door') ) , df['predicted_door_origin'], df['predicted_port_origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'expected_emailto':'expected_Destination_Final',\n",
    "                     'expected_emailfrom':'expected_Origin_Final'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd43b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Final Destination\n",
    "\n",
    "df.loc[:,['predicted_totype','predicted_destination', 'predicted_destination_port','predicted_door_destination','predicted_Destination_Final','expected_Destination_Final']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validating Final Origin\n",
    "\n",
    "df.loc[:,['predicted_fromtype','predicted_origin','predicted_origin_port','predicted_door_origin','predicted_Origin_Final','expected_Origin_Final']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71579685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[ 'predicted_Origin_Final',\n",
    "   'expected_Origin_Final', 'predicted_Destination_Final','expected_Destination_Final' ]] = df[[ 'predicted_Origin_Final',\n",
    "                                                                 'expected_Origin_Final','predicted_Destination_Final','expected_Destination_Final']].apply(lambda x: x.str.strip().str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_Origin_Final'].str.contains('- NONE -') \n",
    "       | df['predicted_Origin_Final'].str.contains('NONE -')\n",
    "       | df['predicted_Origin_Final'].str.contains('- NONE')\n",
    "       | df['predicted_Origin_Final'].str.contains('- NON -') \n",
    "       | df['predicted_Origin_Final'].str.contains('NON -')\n",
    "       | df['predicted_Origin_Final'].str.contains('- NON'),['predicted_Origin_Final','predicted_destination_uncode']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fc957",
   "metadata": {},
   "source": [
    "<h1><center> Compute the Accuracy of the Origin + Destination UNCODEs/Addresses </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f97c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[[ 'predicted_Origin_Final',\n",
    "   'expected_Origin_Final', 'predicted_Destination_Final','expected_Destination_Final' ]] =df_output[[ 'predicted_Origin_Final',\n",
    "   'expected_Origin_Final', 'predicted_Destination_Final','expected_Destination_Final' ]].apply(lambda x : x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f702ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Harcode Changes to expected temporarily\n",
    "\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('CALGARY - T2G3C1 - AB','CALGARY - T2G - CANADA' )\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('DARLINGTON - DL1 - UK','DARLINGTON - DL1 - UNITED KINGDOM')\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('MONTOIR DE BRETAGNE - 44550 - FRANCE','MONTOIR-DE-BRETAGNE - 44550 - FRANCE' )\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('MIAMI - 33147 - FLORIDA','MIAMI - 33147 - U.S.A.' )\n",
    "# df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('SAINT HUBERT - J3Y - QC','SAINT-HUBERT - J3Y - CANADA' )\n",
    "# df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('SAINTE CATHERINE - J5C - CANADA','SAINTE-CATHERINE - J5C - CANADA' )\n",
    "# df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('LEDUC (NISKU) - T9E - CANADA','LEDUC - T9E - CANADA' )\n",
    "# df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('ANCHORAGE - 99516 - U.S.A','ANCHORAGE - 99516 - U.S.A.')\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('BRADFORD ON - L3Z - CANADA','BRADFORD - L3Z - CANADA')\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('SUNDERN - 59846 - GERMANY','SUNDERN-AMECKE - 59846 - GERMANY')\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('SAINTE CLAIRE - G0R - CANADA','SAINTE-CLAIRE - G0R - CANADA')\n",
    "df_output['expected_Origin_Final'] = df_output['expected_Origin_Final'].str.replace('TAINAN CITY - 70841 - TAIWAN','TAINAN - 70841 - TAIWAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c5661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Harcode Changes to expected temporarily\n",
    "\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('MONTOIR-DE-BRETAGNE - 44550 - FRANCE','MONTOIR DE BRETAGNE - 44550 - FRANCE' )\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('DARLINGTON - DL1 - UK','DARLINGTON - DL1 - UNITED KINGDOM')\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('TAINAN - 70841 - TAIWAN','TAINAN CITY - 70841 - TAIWAN' )\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('MONTOIR DE BRETAGNE - 44550 - FRANCE','MONTOIR-DE-BRETAGNE - 44550 - FRANCE' )\n",
    "# df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace( 'SAINT HUBERT - J3Y - QC','SAINT-HUBERT - J3Y - CANADA')\n",
    "# df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('SAINTE-CATHERINE - J5C - CANADA','SAINTE CATHERINE - J5C - CANADA')\n",
    "# df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('LEDUC (NISKU) - T9E - CANADA','LEDUC - T9E - CANADA')\n",
    "# df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('MUNICH - 81669 - GERMANY','MÜNCHEN - 81669 - GERMANY')\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('MIAMI - 33147 - FLORIDA','MIAMI - 33147 - U.S.A.' )\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('BRADFORD ON - L3Z - CANADA','BRADFORD - L3Z - CANADA')\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('SAINTE CLAIRE - G0R - CANADA','SAINTE-CLAIRE - G0R - CANADA')\n",
    "df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace('TAINAN CITY - 70841 - TAIWAN','TAINAN - 70841 - TAIWAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0309314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_output['expected_Destination_Final'] = df_output['expected_Destination_Final'].str.replace( 'SAINT HUBERT - J3Y - QC','SAINT-HUBERT - J3Y - CANADA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['predicted_Destination_Final'] = df_output['predicted_Destination_Final'].str.replace('UNITED STATES','U.S.A.')\n",
    "df_output['predicted_Origin_Final'] = df_output['predicted_Origin_Final'].str.replace('UNITED STATES','U.S.A.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['predicted_Destination_Final'] = df_output['predicted_Destination_Final'].str.replace('UNITED STATES','U.S.A.')\n",
    "df_output['predicted_Origin_Final'] = df_output['predicted_Origin_Final'].str.replace('UNITED STATES','U.S.A.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[[ 'predicted_Origin_Final',\n",
    "   'expected_Origin_Final', 'predicted_Destination_Final','expected_Destination_Final' ]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870f1df",
   "metadata": {},
   "source": [
    "#### Create an indicator to determine full or partial lane match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['Lane_Match_Status'] = np.where((df_output['predicted_Origin_Final'] == df_output['expected_Origin_Final']) & \n",
    "                             (df_output['predicted_Destination_Final'] == df_output['expected_Destination_Final']) , \n",
    "                             'Full Match', 'Partail Match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1188a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['Origin_Match_Status'] = np.where(\n",
    "    (df_output['predicted_Origin_Final'] == df_output['expected_Origin_Final']) , 'Match', 'Not Matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['Destination_Match_Status'] = np.where(\n",
    "         (df_output['predicted_Destination_Final'] == df_output['expected_Destination_Final']), 'Match', 'Not Matched')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba11e3e",
   "metadata": {},
   "source": [
    "#### 1. Origin Match %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a552b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = (df_output[df_output['predicted_Origin_Final'] == df_output['expected_Origin_Final']].shape[0]/df_output.shape[0])*100\n",
    "print('Origin Matches {}% of times :'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94304b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Mismatched entries\n",
    "\n",
    "df_output.loc[(df_output['Origin_Match_Status'] == 'Not Matched'),['emailid','email_processed','predicted_fromtype','predicted_origin','predicted_origin_port','predicted_door_origin','predicted_Origin_Final','expected_Origin_Final','predicted_origin_uncode']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4f1da",
   "metadata": {},
   "source": [
    "#### 2. Destination Match %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efdf9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = (df_output[df_output['predicted_Destination_Final'] == df_output['expected_Destination_Final']].shape[0]/df_output.shape[0])*100\n",
    "print('Destination Matches {}% of times :'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['email_processed'].str.contains('elmira'),['predicted_fromtype','predicted_origin','predicted_origin_zipcode', 'predicted_origin_zipcode1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Mismatched entries\n",
    "\n",
    "df_output.loc[(df_output['Destination_Match_Status'] == 'Not Matched'),['emailid','email_processed','predicted_totype','expected_Destination_type','predicted_destination','predicted_destination_port','predicted_door_destination','predicted_Destination_Final','expected_Destination_Final','predicted_destination_uncode']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geolocator.geocode('CORNWALL K6J 3E6').raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d038808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Mismatched entries\n",
    "\n",
    "df_output.loc[df_output['Destination_Match_Status'] == 'Not Matched',['emailid','email_processed','predicted_totype','expected_Destination_type','predicted_destination','predicted_destination_port','predicted_door_destination','predicted_Destination_Final','expected_Destination_Final','predicted_destination_uncode']]['email_processed'].to_excel('test.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df63a89",
   "metadata": {},
   "source": [
    "#### 3. Lane Match % "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "c  = (df_output[(df_output['predicted_Origin_Final'] == df_output['expected_Origin_Final']) & \n",
    "         (df_output['predicted_Destination_Final'] == df_output['expected_Destination_Final'])].shape[0]/df_output.shape[0])*100\n",
    "print('Entire Lane Matches {}% of times :'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec64422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[(df_output['predicted_Origin_Final'] == df_output['expected_Origin_Final']) & \n",
    "         (df_output['predicted_Destination_Final'] == df_output['expected_Destination_Final'])].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af60c6",
   "metadata": {},
   "source": [
    "* 22 P2P\n",
    "* 16 involve door locations -> zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48021ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.loc[df['predicted_destination'].str.contains('anchorage, ak 99516'),['predicted_Destination_Final','predicted_door_destination','predicted_destination_country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ca53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Origin Address/Port Matches {}% of times'.format(round(a,2)))\n",
    "print('Destination Address/Port Matches {}% of times'.format(round(b,2)))\n",
    "print('Entire Lane Matches {}% of times'.format(round(c,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3c5fd",
   "metadata": {},
   "source": [
    "<h1><center> Analyze the intermediate output </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['expected_Destination_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe308f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['expected_Origin_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['expected_Destination_type'] = np.where(df_output['expected_Destination_type']  == '1', 'port', 'door')\n",
    "df_output['expected_Origin_type'] = np.where(df_output['expected_Origin_type']  == '1', 'port', 'door')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25548c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[['emailid','email_processed','predicted_fromtype', 'expected_Origin_type','predicted_origin','predicted_origin_port','predicted_door_origin','predicted_Origin_Final','expected_Origin_Final','predicted_origin_uncode',\n",
    "  'predicted_totype','expected_Destination_type','predicted_destination','predicted_destination_port','predicted_door_destination','predicted_Destination_Final','expected_Destination_Final','predicted_destination_uncode',\n",
    "  'Origin_Match_Status','Destination_Match_Status' ,'Lane_Match_Status']].to_excel('EA_Output_21stAug.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46faba",
   "metadata": {},
   "source": [
    "<h1><center> Binary Metrics </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95d065dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>body</th>\n",
       "      <th>emailid</th>\n",
       "      <th>Email</th>\n",
       "      <th>email_processed</th>\n",
       "      <th>expected_Origin</th>\n",
       "      <th>expected_Origin_Final</th>\n",
       "      <th>expected_Origin_type</th>\n",
       "      <th>expected_Destination</th>\n",
       "      <th>expected_Destination_Final</th>\n",
       "      <th>expected_Destination_type</th>\n",
       "      <th>expected_Weight</th>\n",
       "      <th>expected_Dimensions</th>\n",
       "      <th>expected_Volume</th>\n",
       "      <th>expected_Quantity</th>\n",
       "      <th>expected_Isstackable</th>\n",
       "      <th>expected_Ishazardous</th>\n",
       "      <th>expected_Istoploaded</th>\n",
       "      <th>expected_Response Status</th>\n",
       "      <th>Exclusion Indicator</th>\n",
       "      <th>Comments_BG</th>\n",
       "      <th>Comments_BG_Binary</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>email_processed_revised</th>\n",
       "      <th>predicted_origin_destination</th>\n",
       "      <th>predicted_origin</th>\n",
       "      <th>predicted_destination</th>\n",
       "      <th>predicted_fromtype</th>\n",
       "      <th>predicted_totype</th>\n",
       "      <th>OriginType_Match</th>\n",
       "      <th>DestinationType_Match</th>\n",
       "      <th>predicted_origin_match</th>\n",
       "      <th>predicted_origin_match_port</th>\n",
       "      <th>predicted_origin_match_score</th>\n",
       "      <th>predicted_origin_match_junk</th>\n",
       "      <th>predicted_destination_match</th>\n",
       "      <th>predicted_destination_match_port</th>\n",
       "      <th>predicted_destination_match_score</th>\n",
       "      <th>predicted_destination_match_junk</th>\n",
       "      <th>Port_x</th>\n",
       "      <th>predicted_origin_uncode</th>\n",
       "      <th>Port_y</th>\n",
       "      <th>predicted_destination_uncode</th>\n",
       "      <th>predicted_origin_port</th>\n",
       "      <th>predicted_destination_port</th>\n",
       "      <th>predicted_port_destination</th>\n",
       "      <th>predicted_port_origin</th>\n",
       "      <th>predicted_dest_zip_match</th>\n",
       "      <th>predicted_origin_zip_match</th>\n",
       "      <th>predicted_origin_city_zipcode_country</th>\n",
       "      <th>predicted_destination_city_zipcode_country</th>\n",
       "      <th>predicted_origin_city</th>\n",
       "      <th>predicted_origin_zipcode</th>\n",
       "      <th>predicted_origin_country</th>\n",
       "      <th>predicted_destination_city</th>\n",
       "      <th>predicted_destination_zipcode</th>\n",
       "      <th>predicted_destination_country</th>\n",
       "      <th>rev_predicted_destination</th>\n",
       "      <th>rev_predicted_origin</th>\n",
       "      <th>predicted_orig_geoaddress</th>\n",
       "      <th>predicted_origin_zipcode1</th>\n",
       "      <th>predicted_origin_country1</th>\n",
       "      <th>predicted_dest_geoaddress</th>\n",
       "      <th>predicted_destination_zipcode1</th>\n",
       "      <th>predicted_destination_country1</th>\n",
       "      <th>predicted_dest_country_chatgpt</th>\n",
       "      <th>predicted_orig_country_chatgpt</th>\n",
       "      <th>predicted_origin_door_location</th>\n",
       "      <th>predicted_destination_door_location</th>\n",
       "      <th>predicted_door_destination</th>\n",
       "      <th>predicted_door_origin</th>\n",
       "      <th>predicted_Destination_Final</th>\n",
       "      <th>predicted_Origin_Final</th>\n",
       "      <th>Lane_Match_Status</th>\n",
       "      <th>Origin_Match_Status</th>\n",
       "      <th>Destination_Match_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>VS: LCL Quote Request | From location : Britis...</td>\n",
       "      <td>Simon Holt &lt;Simon.holt@nordic-on.com&gt;</td>\n",
       "      <td>VS: LCL Quote Request | From location : Britis...</td>\n",
       "      <td>vs: lcl quote request | from location : britis...</td>\n",
       "      <td>british columbia - penticton - v2a - canada</td>\n",
       "      <td>PENTICTON - V2A - CANADA</td>\n",
       "      <td>door</td>\n",
       "      <td>deham , hamburg , germany</td>\n",
       "      <td>DEHAM</td>\n",
       "      <td>port</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Offers found</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Quotation Request</td>\n",
       "      <td>vs: lcl quote request | from location : briti...</td>\n",
       "      <td>British Columbia - Penticton - V2A - Canada\\n\\...</td>\n",
       "      <td>british columbia - penticton - v2a - canada</td>\n",
       "      <td>deham, hamburg, germany</td>\n",
       "      <td>door</td>\n",
       "      <td>port</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Matched</td>\n",
       "      <td>('PALMERSTON NORTH', 42, 567)</td>\n",
       "      <td>PALMERSTON NORTH</td>\n",
       "      <td>42</td>\n",
       "      <td>567</td>\n",
       "      <td>(HAMBURG, 100, 1827)</td>\n",
       "      <td>HAMBURG</td>\n",
       "      <td>100</td>\n",
       "      <td>1827</td>\n",
       "      <td>PALMERSTON NORTH</td>\n",
       "      <td>NZPMR</td>\n",
       "      <td>HAMBURG</td>\n",
       "      <td>DEHAM</td>\n",
       "      <td></td>\n",
       "      <td>DEHAM</td>\n",
       "      <td>DEHAM</td>\n",
       "      <td></td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Zip Found</td>\n",
       "      <td>City: Penticton\\nZipcode: V2A\\nCountry: Canada</td>\n",
       "      <td></td>\n",
       "      <td>penticton</td>\n",
       "      <td>v2a</td>\n",
       "      <td>canada</td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>penticton v2a canada</td>\n",
       "      <td>penticton, regional district of okanagan-simil...</td>\n",
       "      <td>none</td>\n",
       "      <td>canada</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>germany</td>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>PENTICTON - V2A - CANADA</td>\n",
       "      <td>DEHAM</td>\n",
       "      <td>PENTICTON - V2A - CANADA</td>\n",
       "      <td>Full Match</td>\n",
       "      <td>Match</td>\n",
       "      <td>Match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Santos**This mail is from outside our organiza...</td>\n",
       "      <td>Linda-Nancy Colizza &lt;Linda-Nancy.Colizza@hellm...</td>\n",
       "      <td>Santos**This mail is from outside our organiza...</td>\n",
       "      <td>santos good morning, need rate please, termina...</td>\n",
       "      <td>terminal montreal</td>\n",
       "      <td>CAMTR</td>\n",
       "      <td>port</td>\n",
       "      <td>tml santos</td>\n",
       "      <td>BRSSZ</td>\n",
       "      <td>port</td>\n",
       "      <td>2146 kg</td>\n",
       "      <td>44 x44 x42</td>\n",
       "      <td>162624</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>No</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Offers found</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Quotation Request</td>\n",
       "      <td>\"Santos good morning, need rate please, term...</td>\n",
       "      <td>Terminal Montreal TML Santos\\n\\nDestination Lo...</td>\n",
       "      <td>terminal montreal tml santos</td>\n",
       "      <td>unknown</td>\n",
       "      <td>port</td>\n",
       "      <td>port</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Matched</td>\n",
       "      <td>('SANTOS', 100, 1296)</td>\n",
       "      <td>SANTOS</td>\n",
       "      <td>100</td>\n",
       "      <td>1296</td>\n",
       "      <td>(QUEENSTOWN, 59, 557)</td>\n",
       "      <td>QUEENSTOWN</td>\n",
       "      <td>59</td>\n",
       "      <td>557</td>\n",
       "      <td>SANTOS</td>\n",
       "      <td>BRSSZ</td>\n",
       "      <td>QUEENSTOWN</td>\n",
       "      <td>NZZQN</td>\n",
       "      <td>BRSSZ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>BRSSZ</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>i'm sorry, but i cannot determine the country ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td></td>\n",
       "      <td>BRSSZ</td>\n",
       "      <td>Partail Match</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Not Matched</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>RFQ / ITN  door  Winnipeg, MB to CFS Shanghai,...</td>\n",
       "      <td>Sally Chieng &lt;sallyc@itn-logistics.ca&gt;</td>\n",
       "      <td>RFQ / ITN  door  Winnipeg, MB to CFS Shanghai,...</td>\n",
       "      <td>rfq / itn door winnipeg, mb to cfs shanghai, c...</td>\n",
       "      <td>95 alexander st. winnipeg mb</td>\n",
       "      <td>WINNIPEG - R3B - CANADA</td>\n",
       "      <td>door</td>\n",
       "      <td>cfs shanghai, china</td>\n",
       "      <td>CNSHA</td>\n",
       "      <td>port</td>\n",
       "      <td>3991.61 kg</td>\n",
       "      <td>48x48 x50</td>\n",
       "      <td>460800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>No</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Quotation Request</td>\n",
       "      <td>rfq / itn door winnipeg, mb to cfs shanghai, ...</td>\n",
       "      <td>95 Alexander St. Winnipeg, MB\\n\\nDestination L...</td>\n",
       "      <td>95 alexander st. winnipeg, mb</td>\n",
       "      <td>cfs shanghai, china</td>\n",
       "      <td>door</td>\n",
       "      <td>port</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Matched</td>\n",
       "      <td>('WINNIPEG, MB', 100, 1356)</td>\n",
       "      <td>WINNIPEG, MB</td>\n",
       "      <td>100</td>\n",
       "      <td>1356</td>\n",
       "      <td>(SHANGHAI, 100, 1398)</td>\n",
       "      <td>SHANGHAI</td>\n",
       "      <td>100</td>\n",
       "      <td>1398</td>\n",
       "      <td>WINNIPEG, MB</td>\n",
       "      <td>CAWNP</td>\n",
       "      <td>SHANGHAI</td>\n",
       "      <td>CNSHA</td>\n",
       "      <td>CAWNP</td>\n",
       "      <td>CNSHA</td>\n",
       "      <td>CNSHA</td>\n",
       "      <td></td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Zip Found</td>\n",
       "      <td>City: Winnipeg\\nZipcode: None\\nCountry: MB</td>\n",
       "      <td></td>\n",
       "      <td>winnipeg</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td>winnipeg</td>\n",
       "      <td>winnipeg, division no. 11, manitoba, canada</td>\n",
       "      <td>none</td>\n",
       "      <td>canada</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>china</td>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>WINNIPEG - NONEE - NONE</td>\n",
       "      <td>CNSHA</td>\n",
       "      <td>WINNIPEG - NONEE - NONE</td>\n",
       "      <td>Partail Match</td>\n",
       "      <td>Not Matched</td>\n",
       "      <td>Match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RE: QUOTE from Toronto terminal to London term...</td>\n",
       "      <td>Michelle Guo &lt;michelle.guo@sparxlogistics.com&gt;</td>\n",
       "      <td>RE: QUOTE from Toronto terminal to London term...</td>\n",
       "      <td>re: quote from toronto terminal to london term...</td>\n",
       "      <td>toronto terminal</td>\n",
       "      <td>CATOR</td>\n",
       "      <td>port</td>\n",
       "      <td>london terminal</td>\n",
       "      <td>GBLON</td>\n",
       "      <td>port</td>\n",
       "      <td>1859.75kgs</td>\n",
       "      <td>48*40*70 in,  48*40*70 in, 48 *40*60 in, 48*40...</td>\n",
       "      <td>9.12 cbm</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Quotation Request</td>\n",
       "      <td>re: quote from toronto terminal to london ter...</td>\n",
       "      <td>Toronto Terminal\\n\\nDestination Location:\\nLon...</td>\n",
       "      <td>toronto terminal</td>\n",
       "      <td>london terminal</td>\n",
       "      <td>port</td>\n",
       "      <td>port</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Matched</td>\n",
       "      <td>('TORONTO, ON', 82, 1354)</td>\n",
       "      <td>TORONTO, ON</td>\n",
       "      <td>82</td>\n",
       "      <td>1354</td>\n",
       "      <td>(LONDON, 100, 1154)</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>100</td>\n",
       "      <td>1154</td>\n",
       "      <td>TORONTO, ON</td>\n",
       "      <td>CATOR</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>GBLON</td>\n",
       "      <td>CATOR</td>\n",
       "      <td>GBLON</td>\n",
       "      <td>GBLON</td>\n",
       "      <td>CATOR</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>GBLON</td>\n",
       "      <td>CATOR</td>\n",
       "      <td>Full Match</td>\n",
       "      <td>Match</td>\n",
       "      <td>Match</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Rate request LCL cargo NON stackable  Cfs term...</td>\n",
       "      <td>HLIWA Bozena &lt;Bozena.HLIWA@bollore.com&gt;</td>\n",
       "      <td>Rate request LCL cargo NON stackable  Cfs term...</td>\n",
       "      <td>rate request lcl cargo non stackable cfs termi...</td>\n",
       "      <td>cfs terminal antwerp, be</td>\n",
       "      <td>BEANR</td>\n",
       "      <td>port</td>\n",
       "      <td>cfs terminal vancouver, bc</td>\n",
       "      <td>CAVAN</td>\n",
       "      <td>port</td>\n",
       "      <td>880 kg</td>\n",
       "      <td>0.80x 1.20 x 1.15 h m</td>\n",
       "      <td>2.20 cbm</td>\n",
       "      <td>2.0</td>\n",
       "      <td>No</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>No Offers found</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Do not exclude</td>\n",
       "      <td>Quotation Request</td>\n",
       "      <td>rate request lcl cargo non stackable cfs ter...</td>\n",
       "      <td>CFS Terminal Antwerp, BE\\n\\nDestination Locati...</td>\n",
       "      <td>cfs terminal antwerp, be</td>\n",
       "      <td>cfs terminal vancouver, bc</td>\n",
       "      <td>port</td>\n",
       "      <td>port</td>\n",
       "      <td>Matched</td>\n",
       "      <td>Matched</td>\n",
       "      <td>('ANTWERP', 100, 881)</td>\n",
       "      <td>ANTWERP</td>\n",
       "      <td>100</td>\n",
       "      <td>881</td>\n",
       "      <td>(VANCOUVER, BC, 100, 1355)</td>\n",
       "      <td>VANCOUVER, BC</td>\n",
       "      <td>100</td>\n",
       "      <td>1355</td>\n",
       "      <td>ANTWERP</td>\n",
       "      <td>BEANR</td>\n",
       "      <td>VANCOUVER, BC</td>\n",
       "      <td>CAVAN</td>\n",
       "      <td>BEANR</td>\n",
       "      <td>CAVAN</td>\n",
       "      <td>CAVAN</td>\n",
       "      <td>BEANR</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>Nonee</td>\n",
       "      <td>none</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>canada</td>\n",
       "      <td>belgium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>NONE - NONEE - NONE</td>\n",
       "      <td>CAVAN</td>\n",
       "      <td>BEANR</td>\n",
       "      <td>Full Match</td>\n",
       "      <td>Match</td>\n",
       "      <td>Match</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                               body  \\\n",
       "0          0  VS: LCL Quote Request | From location : Britis...   \n",
       "1          2  Santos**This mail is from outside our organiza...   \n",
       "2          3  RFQ / ITN  door  Winnipeg, MB to CFS Shanghai,...   \n",
       "3          4  RE: QUOTE from Toronto terminal to London term...   \n",
       "4          5  Rate request LCL cargo NON stackable  Cfs term...   \n",
       "\n",
       "                                             emailid  \\\n",
       "0              Simon Holt <Simon.holt@nordic-on.com>   \n",
       "1  Linda-Nancy Colizza <Linda-Nancy.Colizza@hellm...   \n",
       "2             Sally Chieng <sallyc@itn-logistics.ca>   \n",
       "3     Michelle Guo <michelle.guo@sparxlogistics.com>   \n",
       "4            HLIWA Bozena <Bozena.HLIWA@bollore.com>   \n",
       "\n",
       "                                               Email  \\\n",
       "0  VS: LCL Quote Request | From location : Britis...   \n",
       "1  Santos**This mail is from outside our organiza...   \n",
       "2  RFQ / ITN  door  Winnipeg, MB to CFS Shanghai,...   \n",
       "3  RE: QUOTE from Toronto terminal to London term...   \n",
       "4  Rate request LCL cargo NON stackable  Cfs term...   \n",
       "\n",
       "                                     email_processed  \\\n",
       "0  vs: lcl quote request | from location : britis...   \n",
       "1  santos good morning, need rate please, termina...   \n",
       "2  rfq / itn door winnipeg, mb to cfs shanghai, c...   \n",
       "3  re: quote from toronto terminal to london term...   \n",
       "4  rate request lcl cargo non stackable cfs termi...   \n",
       "\n",
       "                               expected_Origin     expected_Origin_Final  \\\n",
       "0  british columbia - penticton - v2a - canada  PENTICTON - V2A - CANADA   \n",
       "1                            terminal montreal                     CAMTR   \n",
       "2                 95 alexander st. winnipeg mb   WINNIPEG - R3B - CANADA   \n",
       "3                            toronto terminal                      CATOR   \n",
       "4                    cfs terminal antwerp, be                      BEANR   \n",
       "\n",
       "  expected_Origin_type          expected_Destination  \\\n",
       "0                 door     deham , hamburg , germany   \n",
       "1                 port                    tml santos   \n",
       "2                 door           cfs shanghai, china   \n",
       "3                 port               london terminal   \n",
       "4                 port   cfs terminal vancouver, bc    \n",
       "\n",
       "  expected_Destination_Final expected_Destination_type expected_Weight  \\\n",
       "0                      DEHAM                      port             nan   \n",
       "1                      BRSSZ                      port         2146 kg   \n",
       "2                      CNSHA                      port      3991.61 kg   \n",
       "3                      GBLON                      port      1859.75kgs   \n",
       "4                      CAVAN                      port          880 kg   \n",
       "\n",
       "                                 expected_Dimensions expected_Volume  \\\n",
       "0                                                nan             nan   \n",
       "1                                         44 x44 x42          162624   \n",
       "2                                          48x48 x50          460800   \n",
       "3  48*40*70 in,  48*40*70 in, 48 *40*60 in, 48*40...        9.12 cbm   \n",
       "4                              0.80x 1.20 x 1.15 h m        2.20 cbm   \n",
       "\n",
       "  expected_Quantity expected_Isstackable expected_Ishazardous  \\\n",
       "0               nan                  nan                  nan   \n",
       "1               2.0                  nan                   No   \n",
       "2               4.0                  nan                   No   \n",
       "3               4.0                  nan                  nan   \n",
       "4               2.0                   No                  nan   \n",
       "\n",
       "  expected_Istoploaded expected_Response Status Exclusion Indicator  \\\n",
       "0                  nan          No Offers found      Do not exclude   \n",
       "1                  nan          No Offers found      Do not exclude   \n",
       "2                  nan                      nan      Do not exclude   \n",
       "3                  nan                      nan      Do not exclude   \n",
       "4                  nan          No Offers found      Do not exclude   \n",
       "\n",
       "      Comments_BG Comments_BG_Binary    predicted_class  \\\n",
       "0  Do not exclude     Do not exclude  Quotation Request   \n",
       "1  Do not exclude     Do not exclude  Quotation Request   \n",
       "2  Do not exclude     Do not exclude  Quotation Request   \n",
       "3  Do not exclude     Do not exclude  Quotation Request   \n",
       "4  Do not exclude     Do not exclude  Quotation Request   \n",
       "\n",
       "                             email_processed_revised  \\\n",
       "0   vs: lcl quote request | from location : briti...   \n",
       "1    \"Santos good morning, need rate please, term...   \n",
       "2   rfq / itn door winnipeg, mb to cfs shanghai, ...   \n",
       "3   re: quote from toronto terminal to london ter...   \n",
       "4    rate request lcl cargo non stackable cfs ter...   \n",
       "\n",
       "                        predicted_origin_destination  \\\n",
       "0  British Columbia - Penticton - V2A - Canada\\n\\...   \n",
       "1  Terminal Montreal TML Santos\\n\\nDestination Lo...   \n",
       "2  95 Alexander St. Winnipeg, MB\\n\\nDestination L...   \n",
       "3  Toronto Terminal\\n\\nDestination Location:\\nLon...   \n",
       "4  CFS Terminal Antwerp, BE\\n\\nDestination Locati...   \n",
       "\n",
       "                              predicted_origin       predicted_destination  \\\n",
       "0  british columbia - penticton - v2a - canada     deham, hamburg, germany   \n",
       "1                 terminal montreal tml santos                     unknown   \n",
       "2                95 alexander st. winnipeg, mb         cfs shanghai, china   \n",
       "3                             toronto terminal             london terminal   \n",
       "4                     cfs terminal antwerp, be  cfs terminal vancouver, bc   \n",
       "\n",
       "  predicted_fromtype predicted_totype OriginType_Match DestinationType_Match  \\\n",
       "0               door             port          Matched               Matched   \n",
       "1               port             port          Matched               Matched   \n",
       "2               door             port          Matched               Matched   \n",
       "3               port             port          Matched               Matched   \n",
       "4               port             port          Matched               Matched   \n",
       "\n",
       "          predicted_origin_match predicted_origin_match_port  \\\n",
       "0  ('PALMERSTON NORTH', 42, 567)            PALMERSTON NORTH   \n",
       "1          ('SANTOS', 100, 1296)                      SANTOS   \n",
       "2    ('WINNIPEG, MB', 100, 1356)                WINNIPEG, MB   \n",
       "3      ('TORONTO, ON', 82, 1354)                 TORONTO, ON   \n",
       "4          ('ANTWERP', 100, 881)                     ANTWERP   \n",
       "\n",
       "   predicted_origin_match_score predicted_origin_match_junk  \\\n",
       "0                            42                         567   \n",
       "1                           100                        1296   \n",
       "2                           100                        1356   \n",
       "3                            82                        1354   \n",
       "4                           100                         881   \n",
       "\n",
       "  predicted_destination_match predicted_destination_match_port  \\\n",
       "0        (HAMBURG, 100, 1827)                          HAMBURG   \n",
       "1       (QUEENSTOWN, 59, 557)                       QUEENSTOWN   \n",
       "2       (SHANGHAI, 100, 1398)                         SHANGHAI   \n",
       "3         (LONDON, 100, 1154)                           LONDON   \n",
       "4  (VANCOUVER, BC, 100, 1355)                    VANCOUVER, BC   \n",
       "\n",
       "   predicted_destination_match_score  predicted_destination_match_junk  \\\n",
       "0                                100                              1827   \n",
       "1                                 59                               557   \n",
       "2                                100                              1398   \n",
       "3                                100                              1154   \n",
       "4                                100                              1355   \n",
       "\n",
       "             Port_x predicted_origin_uncode         Port_y  \\\n",
       "0  PALMERSTON NORTH                   NZPMR        HAMBURG   \n",
       "1            SANTOS                   BRSSZ     QUEENSTOWN   \n",
       "2      WINNIPEG, MB                   CAWNP       SHANGHAI   \n",
       "3       TORONTO, ON                   CATOR         LONDON   \n",
       "4           ANTWERP                   BEANR  VANCOUVER, BC   \n",
       "\n",
       "  predicted_destination_uncode predicted_origin_port  \\\n",
       "0                        DEHAM                         \n",
       "1                        NZZQN                 BRSSZ   \n",
       "2                        CNSHA                 CAWNP   \n",
       "3                        GBLON                 CATOR   \n",
       "4                        CAVAN                 BEANR   \n",
       "\n",
       "  predicted_destination_port predicted_port_destination predicted_port_origin  \\\n",
       "0                      DEHAM                      DEHAM                         \n",
       "1                                                                       BRSSZ   \n",
       "2                      CNSHA                      CNSHA                         \n",
       "3                      GBLON                      GBLON                 CATOR   \n",
       "4                      CAVAN                      CAVAN                 BEANR   \n",
       "\n",
       "  predicted_dest_zip_match predicted_origin_zip_match  \\\n",
       "0                Not Found                  Zip Found   \n",
       "1                Not Found                  Not Found   \n",
       "2                Not Found                  Zip Found   \n",
       "3                Not Found                  Not Found   \n",
       "4                Not Found                  Not Found   \n",
       "\n",
       "            predicted_origin_city_zipcode_country  \\\n",
       "0  City: Penticton\\nZipcode: V2A\\nCountry: Canada   \n",
       "1                                                   \n",
       "2      City: Winnipeg\\nZipcode: None\\nCountry: MB   \n",
       "3                                                   \n",
       "4                                                   \n",
       "\n",
       "  predicted_destination_city_zipcode_country predicted_origin_city  \\\n",
       "0                                                        penticton   \n",
       "1                                                             none   \n",
       "2                                                         winnipeg   \n",
       "3                                                             none   \n",
       "4                                                             none   \n",
       "\n",
       "  predicted_origin_zipcode predicted_origin_country  \\\n",
       "0                      v2a                   canada   \n",
       "1                    Nonee                     none   \n",
       "2                    Nonee                     none   \n",
       "3                    Nonee                     none   \n",
       "4                    Nonee                     none   \n",
       "\n",
       "  predicted_destination_city predicted_destination_zipcode  \\\n",
       "0                       none                         Nonee   \n",
       "1                       none                         Nonee   \n",
       "2                       none                         Nonee   \n",
       "3                       none                         Nonee   \n",
       "4                       none                         Nonee   \n",
       "\n",
       "  predicted_destination_country rev_predicted_destination  \\\n",
       "0                          none                             \n",
       "1                          none                             \n",
       "2                          none                             \n",
       "3                          none                             \n",
       "4                          none                             \n",
       "\n",
       "   rev_predicted_origin                          predicted_orig_geoaddress  \\\n",
       "0  penticton v2a canada  penticton, regional district of okanagan-simil...   \n",
       "1                                                                     none   \n",
       "2              winnipeg        winnipeg, division no. 11, manitoba, canada   \n",
       "3                                                                     none   \n",
       "4                                                                     none   \n",
       "\n",
       "  predicted_origin_zipcode1 predicted_origin_country1  \\\n",
       "0                      none                    canada   \n",
       "1                      none                      none   \n",
       "2                      none                    canada   \n",
       "3                      none                      none   \n",
       "4                      none                      none   \n",
       "\n",
       "  predicted_dest_geoaddress predicted_destination_zipcode1  \\\n",
       "0                      none                           none   \n",
       "1                      none                           none   \n",
       "2                      none                           none   \n",
       "3                      none                           none   \n",
       "4                      none                           none   \n",
       "\n",
       "  predicted_destination_country1  \\\n",
       "0                           none   \n",
       "1                           none   \n",
       "2                           none   \n",
       "3                           none   \n",
       "4                           none   \n",
       "\n",
       "                      predicted_dest_country_chatgpt  \\\n",
       "0                                            germany   \n",
       "1  i'm sorry, but i cannot determine the country ...   \n",
       "2                                              china   \n",
       "3                                               none   \n",
       "4                                             canada   \n",
       "\n",
       "  predicted_orig_country_chatgpt predicted_origin_door_location  \\\n",
       "0                         canada                            NaN   \n",
       "1                           none                            NaN   \n",
       "2                         canada                            NaN   \n",
       "3                         canada                            NaN   \n",
       "4                        belgium                            NaN   \n",
       "\n",
       "  predicted_destination_door_location predicted_door_destination  \\\n",
       "0                                 NaN        NONE - NONEE - NONE   \n",
       "1                                 NaN        NONE - NONEE - NONE   \n",
       "2                                 NaN        NONE - NONEE - NONE   \n",
       "3                                 NaN        NONE - NONEE - NONE   \n",
       "4                                 NaN        NONE - NONEE - NONE   \n",
       "\n",
       "      predicted_door_origin predicted_Destination_Final  \\\n",
       "0  PENTICTON - V2A - CANADA                       DEHAM   \n",
       "1       NONE - NONEE - NONE                               \n",
       "2   WINNIPEG - NONEE - NONE                       CNSHA   \n",
       "3       NONE - NONEE - NONE                       GBLON   \n",
       "4       NONE - NONEE - NONE                       CAVAN   \n",
       "\n",
       "     predicted_Origin_Final Lane_Match_Status Origin_Match_Status  \\\n",
       "0  PENTICTON - V2A - CANADA        Full Match               Match   \n",
       "1                     BRSSZ     Partail Match         Not Matched   \n",
       "2   WINNIPEG - NONEE - NONE     Partail Match         Not Matched   \n",
       "3                     CATOR        Full Match               Match   \n",
       "4                     BEANR        Full Match               Match   \n",
       "\n",
       "  Destination_Match_Status  \n",
       "0                    Match  \n",
       "1              Not Matched  \n",
       "2                    Match  \n",
       "3                    Match  \n",
       "4                    Match  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('Processed_EmailOutput21stAugust.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae2038",
   "metadata": {},
   "source": [
    "## Hazardous vs Non Hazardous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc6394aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nan', 'no'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['expected_Ishazardous'] = df['expected_Ishazardous'].str.lower()\n",
    "df['expected_Ishazardous'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3477e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan    39\n",
       "no     10\n",
       "Name: expected_Ishazardous, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['expected_Ishazardous'].value_counts() #.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8062ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ocean rate request - lcl (door delivery) good afternoon!if you have door service, please quote the below shipment to door. if not, please quote to anchorage port only. can you please provide an lcl rate quotation based on the following specifications (to door consignee as listed): origin: toronto, on destination: anchorage, ak 99516 commodity: plastic recycling containers(non-hazardous) 5 skids at 1,212.0 kg 16.122 cbm 4 x 48x48x84 1 x 52x48x84 please ensure you provide all charges (excluding only customs formalities), as well as transit time, and cut off dates/sailing dates. if you have any questions, please do not hesitate to contact me.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['expected_Ishazardous']=='no']['email_processed'].values[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c027e1",
   "metadata": {},
   "source": [
    "* For non-hazardous, many a times the expected value is mapped based on intuition like \"wild rice is non hazardous\", \"range rover can't be hazardous material\". so for materials like these there were no keywords present in the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc3e44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_haz = ['non dg','non-dg','non - haz','non-haz','non haz',\n",
    "           'non-hazardous', 'non hazardous', 'hazardous no']\n",
    "\n",
    "haz = ['dg','haz', 'hazardous']\n",
    "\n",
    "# 'hazmats' is to be added, haven't seen any case so haven't added\n",
    "\n",
    "def check_non_binary_in_row_with_space(strings_list, text_row):\n",
    "    for string in strings_list:\n",
    "        if(f\" {string} \" in text_row or f\"{string} \" in text_row or f\" {string}\" in text_row):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_binary_in_row_with_space(strings_list, text_row):\n",
    "    for string in strings_list:\n",
    "        if(f\" {string} \" in text_row or f\"{string} \" in text_row or f\" {string}\" in text_row):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "df['IsHazardousMaterial'] = np.nan\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    email = row['email_processed']\n",
    "    if(check_non_binary_in_row_with_space(non_haz, email)):\n",
    "        df.loc[i, 'IsHazardousMaterial'] = False\n",
    "    elif((df.loc[i, 'IsHazardousMaterial'] != False)&(check_binary_in_row_with_space(haz, email))):\n",
    "        df.loc[i, 'IsHazardousMaterial'] = True\n",
    "    else:\n",
    "        df.loc[i, 'IsHazardousMaterial'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39cc501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    47\n",
       "True      2\n",
       "Name: IsHazardousMaterial, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['IsHazardousMaterial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4346f0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan    39\n",
       "no     10\n",
       "Name: expected_Ishazardous, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['expected_Ishazardous'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b0d43",
   "metadata": {},
   "source": [
    "I had not taken into account dg materials as hazardous in expected values, so making the change here based on predicted value. This is a temporary fix, ideally we should change the expected value in excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac71c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['IsHazardousMaterial']==True, 'expected_Ishazardous'] = 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c53808",
   "metadata": {},
   "source": [
    "**I haven't taken into account dg as hazardous in expected values, so I have to change it**\n",
    "\n",
    "* Otherwise the above logic for predicted values are fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feefe5ec",
   "metadata": {},
   "source": [
    "## Stackable vs Non Stackable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc88c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_stack = ['non stackable', 'stackable no', \n",
    "                          'not stackable', 'non-stackable',\n",
    "            'nonstackable','non - stackable',\n",
    "             'not stackable','not-stackable','not - stackable',\n",
    "            'non stack']\n",
    "stack = ['stackable yes', 'stackable', 'stack']\n",
    "\n",
    "def check_non_binary_in_row_with_space(strings_list, text_row):\n",
    "    for string in strings_list:\n",
    "        if(f\" {string} \" in text_row or f\"{string} \" in text_row or f\" {string}\" in text_row):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_binary_in_row_with_space(strings_list, text_row):\n",
    "    for string in strings_list:\n",
    "        if(f\" {string} \" in text_row or f\"{string} \" in text_row or f\" {string}\" in text_row):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "df['IsStackable'] = np.nan\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    email = row['email_processed']\n",
    "    if(check_non_binary_in_row_with_space(non_stack, email)):\n",
    "        df.loc[i, 'IsStackable'] = False\n",
    "    elif((df.loc[i, 'IsStackable'] != False)&(check_binary_in_row_with_space(stack, email))):\n",
    "        df.loc[i, 'IsStackable'] = True\n",
    "    else:\n",
    "        df.loc[i, 'IsStackable'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2d28baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     41\n",
       "False     8\n",
       "Name: IsStackable, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['IsStackable'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52900b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan    37\n",
       "No     10\n",
       "yes     2\n",
       "Name: expected_Isstackable, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['expected_Isstackable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c13e8",
   "metadata": {},
   "source": [
    "**I have assumed that a home theater is stackable, although no keyword is mentioned in the email**\n",
    "\n",
    "* Apart from these corner cases the predicted logic is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61be119",
   "metadata": {},
   "source": [
    "## Top Loadable vs Non Top Loadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe1e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_top = ['top loaded no', 'not toploadable']\n",
    "top = ['top loadable', 'top load only', \n",
    "                        'top load']\n",
    "\n",
    "df['IsToploaded'] = np.nan\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    email = row['email_processed']\n",
    "    if(check_non_binary_in_row_with_space(non_top, email)):\n",
    "        df.loc[i, 'IsToploaded'] = False\n",
    "    elif((df.loc[i, 'IsToploaded'] != False)&(check_binary_in_row_with_space(top, email))):\n",
    "        df.loc[i, 'IsToploaded'] = True\n",
    "    else:\n",
    "        df.loc[i, 'IsToploaded'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39c24d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    49\n",
       "Name: IsToploaded, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['IsToploaded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65ff86ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan    46\n",
       "No      1\n",
       "yes     1\n",
       "Yes     1\n",
       "Name: expected_Istoploaded, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['expected_Istoploaded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c0870e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rate request | vancouver, canada to highton, australia hi there, can we please get a rate for the following: please include door delivery, customs clearance, and dthc separately. from: vancouver, canada to: mark chapman 15 nelson ave highton vic 3216 australia 1 pallet 75x51x67 high (x1) commodity: commercial shipment - home theatre equipment'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['expected_Istoploaded']=='No']['email_processed'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d76f16",
   "metadata": {},
   "source": [
    "**Again here in the expected values i have assumed that a home thetre is not toploadable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38a9b745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nan', 'No', 'yes', 'Yes'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['expected_Istoploaded'].value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33aa38",
   "metadata": {},
   "source": [
    "Hardcoding this change in predicted value for home theatre, this we can discuss with business if they can provide a list of items and corresponding attributes of binary variable. Then we can confirm if a home theatre can be considered a toploadable item or not.\n",
    "\n",
    "* There is only one entry corresponding to home theatre item, so changing it to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bbd197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['email_processed'].str.contains('home theatre'), 'IsToploaded'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fe0d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"IsToploaded\": 'IsTopLoaded'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d855d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(['IsStackable', 'IsHazardousMaterial', 'IsTopLoaded'])\n",
    "df_output = df.rename(columns={c: c+'_predicted' for c in preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18193fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output['expected_Isstackable'] = df_output['expected_Isstackable'].str.lower()\n",
    "df_output['expected_Isstackable'] = df_output['expected_Isstackable'].fillna('yes')\n",
    "df_output[\"expected_Isstackable\"] = df_output[\"expected_Isstackable\"].apply(lambda x: False if x.lower()==\"no\"  else True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb9d5d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 79)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_output.drop_duplicates(subset=['email_processed'], keep='first')\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad277dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.92% match of stackable\n"
     ]
    }
   ],
   "source": [
    "# 'IsStackable_predicted', 'expected_Isstackable'\n",
    "matched, un_matched = df[df['expected_Isstackable']==df['IsStackable_predicted']].shape[0],df[df['expected_Isstackable']!=df['IsStackable_predicted']].shape[0]\n",
    "a = matched/(matched+un_matched)\n",
    "print('{}% match of stackable'.format(round(a*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9eea01",
   "metadata": {},
   "source": [
    "### Accuracy of Binary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7c1ba0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsStackable_predicted</th>\n",
       "      <th>Expected - nonstackable</th>\n",
       "      <th>Expected - stackable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Predicted - nonstackable</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Predicted - stackable</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      IsStackable_predicted  Expected - nonstackable  Expected - stackable\n",
       "0  Predicted - nonstackable                        8                     0\n",
       "1     Predicted - stackable                        2                    39"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto encode the columns to be measured \n",
    "\n",
    "df['expected_Isstackable'] = np.where(df['expected_Isstackable']==True, 1, 0)\n",
    "df['IsStackable_predicted'] = np.where(df['IsStackable_predicted']==True, 1, 0)\n",
    "OriginType_Confusion_Matrix = pd.crosstab(df['IsStackable_predicted'], df['expected_Isstackable'])\n",
    "OriginType_Confusion_Matrix.columns = ['Expected - nonstackable', 'Expected - stackable']\n",
    "OriginType_Confusion_Matrix = OriginType_Confusion_Matrix.reset_index()\n",
    "OriginType_Confusion_Matrix['IsStackable_predicted'] = np.where((OriginType_Confusion_Matrix['IsStackable_predicted'] == 0), 'Predicted - nonstackable', 'Predicted - stackable')\n",
    "OriginType_Confusion_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03492a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of IsStackable :  95.91836734693877\n",
      "Recall of non stackable :  80.0\n",
      "Precision of non stackable class :  100.0\n",
      "Recall of stackable class :  100.0\n",
      "Precision of stackable class :  95.1219512195122\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "OriginType_Accuracy = ((OriginType_Confusion_Matrix['Expected - nonstackable'][0] + OriginType_Confusion_Matrix['Expected - stackable'][1])/(OriginType_Confusion_Matrix['Expected - nonstackable'][0] + OriginType_Confusion_Matrix['Expected - stackable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - nonstackable'][1] + OriginType_Confusion_Matrix['Expected - stackable'][1]))*100\n",
    "print('Accuracy of IsStackable : ' , OriginType_Accuracy)\n",
    "\n",
    "# Recall of Door Class within OriginType\n",
    "OriginType_DoorRecall = ((OriginType_Confusion_Matrix['Expected - nonstackable'][0])/(OriginType_Confusion_Matrix['Expected - nonstackable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - nonstackable'][1] ))*100\n",
    "print('Recall of non stackable : ' , OriginType_DoorRecall)\n",
    "\n",
    "# Precision of Door Class within OriginType\n",
    "OriginType_DoorPrecision = ((OriginType_Confusion_Matrix['Expected - nonstackable'][0])/(OriginType_Confusion_Matrix['Expected - nonstackable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - stackable'][0] ))*100\n",
    "print('Precision of non stackable class : ' , OriginType_DoorPrecision)\n",
    "\n",
    "# Recall of Port Class within OriginType\n",
    "OriginType_PortRecall = ((OriginType_Confusion_Matrix['Expected - stackable'][1])/(OriginType_Confusion_Matrix['Expected - stackable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - stackable'][1] ))*100\n",
    "print('Recall of stackable class : ' , OriginType_PortRecall)\n",
    "\n",
    "# Precision of Port Class within OriginType\n",
    "OriginType_PortPrecision = ((OriginType_Confusion_Matrix['Expected - stackable'][1])/(OriginType_Confusion_Matrix['Expected - stackable'][1] + \n",
    "OriginType_Confusion_Matrix['Expected - nonstackable'][1] ))*100\n",
    "print('Precision of stackable class : ' , OriginType_PortPrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a0cd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['expected_Ishazardous'] = df['expected_Ishazardous'].astype(str).str.lower()\n",
    "df['expected_Ishazardous'] = df['expected_Ishazardous'].fillna('no')\n",
    "df[\"expected_Ishazardous\"] = df[\"expected_Ishazardous\"].apply(lambda x: True if \"yes\" in x.lower().strip()  else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "489f729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% match of hazardous\n"
     ]
    }
   ],
   "source": [
    "# 'IsStackable_predicted', 'expected_Isstackable'\n",
    "matched, un_matched = df[df['expected_Ishazardous']==df['IsHazardousMaterial_predicted']].shape[0],df[df['expected_Ishazardous']!=df['IsHazardousMaterial_predicted']].shape[0]\n",
    "a = matched/(matched+un_matched)\n",
    "print('{}% match of hazardous'.format(round(a*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d75eac9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsHazardousMaterial_predicted</th>\n",
       "      <th>Expected - nonhazardous</th>\n",
       "      <th>Expected - hazardous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Predicted - nonhazardous</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Predicted - hazardous</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IsHazardousMaterial_predicted  Expected - nonhazardous  Expected - hazardous\n",
       "0      Predicted - nonhazardous                       47                     0\n",
       "1         Predicted - hazardous                        0                     2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto encode the columns to be measured \n",
    "\n",
    "df['expected_Ishazardous'] = np.where(df['expected_Ishazardous']==True, 1, 0)\n",
    "df['IsHazardousMaterial_predicted'] = np.where(df['IsHazardousMaterial_predicted']==True, 1, 0)\n",
    "OriginType_Confusion_Matrix = pd.crosstab(df['IsHazardousMaterial_predicted'], df['expected_Ishazardous'])\n",
    "OriginType_Confusion_Matrix.columns = ['Expected - nonhazardous', 'Expected - hazardous']\n",
    "#OriginType_Confusion_Matrix['Expected - hazardous'] = 0\n",
    "OriginType_Confusion_Matrix = OriginType_Confusion_Matrix.reset_index()\n",
    "OriginType_Confusion_Matrix['IsHazardousMaterial_predicted'] = np.where((OriginType_Confusion_Matrix['IsHazardousMaterial_predicted'] == 0), 'Predicted - nonhazardous', 'Predicted - hazardous')\n",
    "OriginType_Confusion_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98334118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of IsHazardous :  100.0\n",
      "Recall of nonhazardous :  100.0\n",
      "Precision of non hazardous class :  100.0\n",
      "Recall of hazardous class :  100.0\n",
      "Precision of hazardous class :  100.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "OriginType_Accuracy = ((OriginType_Confusion_Matrix['Expected - nonhazardous'][0] + OriginType_Confusion_Matrix['Expected - hazardous'][1])/(OriginType_Confusion_Matrix['Expected - nonhazardous'][0] + OriginType_Confusion_Matrix['Expected - hazardous'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - nonhazardous'][1] + OriginType_Confusion_Matrix['Expected - hazardous'][1]))*100\n",
    "print('Accuracy of IsHazardous : ' , OriginType_Accuracy)\n",
    "\n",
    "# Recall of Door Class within OriginType\n",
    "OriginType_DoorRecall = ((OriginType_Confusion_Matrix['Expected - nonhazardous'][0])/(OriginType_Confusion_Matrix['Expected - nonhazardous'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - nonhazardous'][1] ))*100\n",
    "print('Recall of nonhazardous : ' , OriginType_DoorRecall)\n",
    "\n",
    "# Precision of Door Class within OriginType\n",
    "OriginType_DoorPrecision = ((OriginType_Confusion_Matrix['Expected - nonhazardous'][0])/(OriginType_Confusion_Matrix['Expected - nonhazardous'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - hazardous'][0] ))*100\n",
    "print('Precision of non hazardous class : ' , OriginType_DoorPrecision)\n",
    "\n",
    "# Recall of Port Class within OriginType\n",
    "OriginType_PortRecall = ((OriginType_Confusion_Matrix['Expected - hazardous'][1])/(OriginType_Confusion_Matrix['Expected - hazardous'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - hazardous'][1] ))*100\n",
    "print('Recall of hazardous class : ' , OriginType_PortRecall)\n",
    "\n",
    "# Precision of Port Class within OriginType\n",
    "OriginType_PortPrecision = ((OriginType_Confusion_Matrix['Expected - hazardous'][1])/(OriginType_Confusion_Matrix['Expected - hazardous'][1] + \n",
    "OriginType_Confusion_Matrix['Expected - nonhazardous'][1] ))*100\n",
    "print('Precision of hazardous class : ' , OriginType_PortPrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a7e6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['expected_Istoploaded'] = df['expected_Istoploaded'].str.lower()\n",
    "df.loc[df['expected_Istoploaded']=='nan', 'expected_Istoploaded'] = 'yes'\n",
    "df[\"expected_Istoploaded\"] = df[\"expected_Istoploaded\"].apply(lambda x: True if \"yes\" in x.lower().strip() else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40a31565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% match of toploadable\n"
     ]
    }
   ],
   "source": [
    "# 'expected_Istoploaded', 'IsTopLoaded_predicted'\n",
    "matched, un_matched = df[df['expected_Istoploaded']==df['IsTopLoaded_predicted']].shape[0],df[df['expected_Istoploaded']!=df['IsTopLoaded_predicted']].shape[0]\n",
    "a = matched/(matched+un_matched)\n",
    "print('{}% match of toploadable'.format(round(a*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "517db5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsTopLoaded_predicted</th>\n",
       "      <th>Expected - nontoploadable</th>\n",
       "      <th>Expected - toploadable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Predicted - nontoploadable</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Predicted - toploadable</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IsTopLoaded_predicted  Expected - nontoploadable  \\\n",
       "0  Predicted - nontoploadable                          1   \n",
       "1     Predicted - toploadable                          0   \n",
       "\n",
       "   Expected - toploadable  \n",
       "0                       0  \n",
       "1                      48  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto encode the columns to be measured \n",
    "\n",
    "df['expected_Istoploaded'] = np.where(df['expected_Istoploaded']==True, 1, 0)\n",
    "df['IsTopLoaded_predicted'] = np.where(df['IsTopLoaded_predicted']==True, 1, 0)\n",
    "\n",
    "\n",
    "OriginType_Confusion_Matrix = pd.crosstab(df['IsTopLoaded_predicted'], df['expected_Istoploaded'])\n",
    "OriginType_Confusion_Matrix.columns = ['Expected - nontoploadable', 'Expected - toploadable']\n",
    "OriginType_Confusion_Matrix = OriginType_Confusion_Matrix.reset_index()\n",
    "OriginType_Confusion_Matrix['IsTopLoaded_predicted'] = np.where((OriginType_Confusion_Matrix['IsTopLoaded_predicted'] == 0), 'Predicted - nontoploadable', 'Predicted - toploadable')\n",
    "# OriginType_Confusion_Matrix.loc[-1] = ['Predicted - nontoploadable',0, 0]  # adding a row\n",
    "# OriginType_Confusion_Matrix.index = OriginType_Confusion_Matrix.index + 1  # shifting index\n",
    "# OriginType_Confusion_Matrix = OriginType_Confusion_Matrix.sort_index()  # sorting by index\n",
    "OriginType_Confusion_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34752604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of istoploadable :  100.0\n",
      "Recall of non toploadable :  100.0\n",
      "Precision of non toploadable class :  100.0\n",
      "Recall of toploadable class :  100.0\n",
      "Precision of toploadable class :  100.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "OriginType_Accuracy = ((OriginType_Confusion_Matrix['Expected - nontoploadable'][0] + OriginType_Confusion_Matrix['Expected - toploadable'][1])/(OriginType_Confusion_Matrix['Expected - nontoploadable'][0] + OriginType_Confusion_Matrix['Expected - toploadable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - nontoploadable'][1] + OriginType_Confusion_Matrix['Expected - toploadable'][1]))*100\n",
    "print('Accuracy of istoploadable : ' , OriginType_Accuracy)\n",
    "\n",
    "# Recall of Door Class within OriginType\n",
    "OriginType_DoorRecall = ((OriginType_Confusion_Matrix['Expected - nontoploadable'][0])/(OriginType_Confusion_Matrix['Expected - nontoploadable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - nontoploadable'][1] ))*100\n",
    "print('Recall of non toploadable : ' , OriginType_DoorRecall)\n",
    "\n",
    "# Precision of Door Class within OriginType\n",
    "OriginType_DoorPrecision = ((OriginType_Confusion_Matrix['Expected - nontoploadable'][0])/(OriginType_Confusion_Matrix['Expected - nontoploadable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - toploadable'][0] ))*100\n",
    "print('Precision of non toploadable class : ' , OriginType_DoorPrecision)\n",
    "\n",
    "# Recall of Port Class within OriginType\n",
    "OriginType_PortRecall = ((OriginType_Confusion_Matrix['Expected - toploadable'][1])/(OriginType_Confusion_Matrix['Expected - toploadable'][0] + \n",
    "OriginType_Confusion_Matrix['Expected - toploadable'][1] ))*100\n",
    "print('Recall of toploadable class : ' , OriginType_PortRecall)\n",
    "\n",
    "# Precision of Port Class within OriginType\n",
    "OriginType_PortPrecision = ((OriginType_Confusion_Matrix['Expected - toploadable'][1])/(OriginType_Confusion_Matrix['Expected - toploadable'][1] + \n",
    "OriginType_Confusion_Matrix['Expected - nontoploadable'][1] ))*100\n",
    "print('Precision of toploadable class : ' , OriginType_PortPrecision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a84d7d",
   "metadata": {},
   "source": [
    "<h1><center> Pattern Matching for the Remaining Non-Binary variables </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c444a9a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[['email_processed']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6963a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_letters(text):\n",
    "    return ''.join([char for char in text if not char.isalpha()])\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    clean_text = re.sub(pattern, '', text)\n",
    "    return clean_text\n",
    "# remove_special_characters, remove_non_digits\n",
    "def remove_non_digits(text):\n",
    "    clean_text = re.sub(r'[^0-9.]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def process_dimensions(text):\n",
    "    # single quote implies inch, so replacing it to standard format\n",
    "    text = text.replace(\"'\", '\"')\n",
    "# r'\\d+\"\\s*x\\s*\\d+\"\\s*x\\s*\\d+\"\\s*|\"\\s*x\\s*\\d+\"\\s*x\\s*\\d+\"\\s*\n",
    "    regex_patterns = [\n",
    "        r'\\d+\\s*x\\s*(\\d+x\\d+x\\d+)',\n",
    "        r'(?<=\\d\\s)(\\d+\\s\\d+\\s\\d+)',\n",
    "        r'(\\d+)\"x(\\d+x\\d+)',\n",
    "        r'\"?(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)[\\s]+(\\d+(?:\\.\\d+)?)[\\s]+(\\d+(?:\\.\\d+)?)',\n",
    "        r'\"?(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)\"?',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*\\((\\w+)\\)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)',\n",
    "        r'\\d+(?:\\s*x\\s*\\d+)',\n",
    "        r'\\d+\\s*\\*\\s*\\d+\\s*\\*\\s*\\d+\\s*in',\n",
    "        r'\\d+\"\\s*x\\s*\\d+\"\\s*x\\s*\\d+\\s*in',\n",
    "        r'\\d+\\s*\\*\\s*\\d+\\s*\\*\\s*\\d+',\n",
    "        r'\\d+\\s*\"\\s*x\\s*\\d+\"\\s*x\\s*\\d+\"\\s*|\"\\s*x\\s*\\d+\"\\s*x\\s*\\d+\"\\s*',\n",
    "        r'\\d+\"\\s*x\\s*\\d+\\s*x\\s*\\d+',\n",
    "        r'(\\d+)\"x(\\d+)x(\\d+)',\n",
    "        r'(\\d+)\\(h\\)x(\\d+)x(\\d+)',\n",
    "        r'(\\d+(\\.\\d+)?)\\s*x\\s*(\\d+(\\.\\d+)?)\\s*x\\s*(\\d+(\\.\\d+)?)\\s*cms',\n",
    "        r'l:\\s*(\\d+\\.\\d+)\\s*\\|\\s*w:\\s*(\\d+\\.\\d+)\\s*\\|\\s*h:\\s*(\\d+\\.\\d+)\\s*cm',\n",
    "        r'(\\d+)x(\\d+)x(\\d+)',\n",
    "        r'height\\s*:\\s*([\\d.]+)\\s*inches\\s*width\\s*:\\s*([\\d.]+)\\s*inches\\s*length\\s*:\\s*([\\d.]+)\\s*inches',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*[/\\s]*(\\d+(?:\\.\\d+)?)\\s*[/\\s]*(\\d+(?:\\.\\d+)?)\\s*(\\w+)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*[\\sxX]\\s*(\\d+(?:\\.\\d+)?)\\s*[\\sxX]\\s*(\\d+(?:\\.\\d+)?)\\s*(\\w+)',\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*x\\s*(\\d+(?:\\.\\d+)?)']\n",
    "\n",
    "    dims = []\n",
    "    for pattern in regex_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            dims.extend(matches)\n",
    "            break\n",
    "\n",
    "    return dims\n",
    "\n",
    "# df['email_processed'] = df['email_processed'].astype(str)\n",
    "# df['email_processed'] = df['email_processed'].apply(preprocess_text)\n",
    "# df['email_processed'] = df['email_processed'].str.replace('Review: ```',' ')\n",
    "# df['email_processed'] = df['email_processed'].str.replace('Review:', ' ')\n",
    "# df['email_processed'] = df['email_processed'].str.replace(' tml ', 'terminal')\n",
    "# df['email_processed'] = df['email_processed'].str.replace('^\\W+','' )\n",
    "# df['email_processed'] = df['email_processed'].str.lower()\n",
    "\n",
    "\n",
    "# change email_processed - run only on body\n",
    "df['predicted_Dimensions_raw'] = df['email_processed'].apply(process_dimensions)\n",
    "\n",
    "# Removing Square brackets - list\n",
    "\n",
    "df['predicted_Dimensions'] = [','.join(map(str, l)) for l in df['predicted_Dimensions_raw']]\n",
    "\n",
    "\n",
    "# # Function to determine unit based on value\n",
    "# def assign_unit(value):\n",
    "#     if \"'\" in value:\n",
    "#         return \"inch\"\n",
    "#     else:\n",
    "#         return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b34a740d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailid</th>\n",
       "      <th>email_processed</th>\n",
       "      <th>predicted_Dimensions_raw</th>\n",
       "      <th>predicted_Dimensions</th>\n",
       "      <th>expected_Dimensions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simon Holt &lt;Simon.holt@nordic-on.com&gt;</td>\n",
       "      <td>vs: lcl quote request | from location : britis...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linda-Nancy Colizza &lt;Linda-Nancy.Colizza@hellm...</td>\n",
       "      <td>santos good morning, need rate please, termina...</td>\n",
       "      <td>[(44, 44, 42)]</td>\n",
       "      <td>('44', '44', '42')</td>\n",
       "      <td>44 x44 x42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sally Chieng &lt;sallyc@itn-logistics.ca&gt;</td>\n",
       "      <td>rfq / itn door winnipeg, mb to cfs shanghai, c...</td>\n",
       "      <td>[(48, 48, 50)]</td>\n",
       "      <td>('48', '48', '50')</td>\n",
       "      <td>48x48 x50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle Guo &lt;michelle.guo@sparxlogistics.com&gt;</td>\n",
       "      <td>re: quote from toronto terminal to london term...</td>\n",
       "      <td>[48*40*70 in, 48*40*70 in, 48 *40*60 in, 48*40...</td>\n",
       "      <td>48*40*70 in,48*40*70 in,48 *40*60 in,48*40*76 in</td>\n",
       "      <td>48*40*70 in,  48*40*70 in, 48 *40*60 in, 48*40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HLIWA Bozena &lt;Bozena.HLIWA@bollore.com&gt;</td>\n",
       "      <td>rate request lcl cargo non stackable cfs termi...</td>\n",
       "      <td>[(0.80, 1.20, 1.15)]</td>\n",
       "      <td>('0.80', '1.20', '1.15')</td>\n",
       "      <td>0.80x 1.20 x 1.15 h m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>rate request | vancouver, canada to highton, a...</td>\n",
       "      <td>[(75, 51, 67)]</td>\n",
       "      <td>('75', '51', '67')</td>\n",
       "      <td>75x51x67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>rate request | vancouver, canada to darlington...</td>\n",
       "      <td>[(122, 102, 161), (122, 102, 121)]</td>\n",
       "      <td>('122', '102', '161'),('122', '102', '121')</td>\n",
       "      <td>122 x 102 x 161 cm, 122 x 102 x 121 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jude Menezes &lt;jude@zodiacimpex.com&gt;</td>\n",
       "      <td>lcl rate request toronto to singapore you don'...</td>\n",
       "      <td>[(89, 112, 132), (91.5, 91.5, 104)]</td>\n",
       "      <td>('89', '112', '132'),('91.5', '91.5', '104')</td>\n",
       "      <td>89 x 112 x 132 cms, 91.5 x 91.5 x 104 cms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sandro Avilez Caldart &lt;savilez@willsonintl.com&gt;</td>\n",
       "      <td>lcl import | term toronto, on, ca to port of s...</td>\n",
       "      <td>[130\" x 90\" x 98 in, 80\" x 58\" x 84 in]</td>\n",
       "      <td>130\" x 90\" x 98 in,80\" x 58\" x 84 in</td>\n",
       "      <td>130\" x 90\" x 98 in, 80\" x 58\" x 84 in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sarah | 1UP Cargo Pricing &lt;Sarah@1upcargo.com&gt;</td>\n",
       "      <td>fw: canada exw rates form door to nansha,guang...</td>\n",
       "      <td>[122*102*211]</td>\n",
       "      <td>122*102*211</td>\n",
       "      <td>122*102*211cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Edgar Azar &lt;edgar.azar@coleintl.com&gt;</td>\n",
       "      <td>fob lcl rate request /jebel ali to toronto hel...</td>\n",
       "      <td>[(120, 100, 141)]</td>\n",
       "      <td>('120', '100', '141')</td>\n",
       "      <td>120x100x141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Sales Milan (IT-MIL ECU Worldwide)\" &lt;SalesMil...</td>\n",
       "      <td>edmonton via montreal dsv hi dear we have to q...</td>\n",
       "      <td>[245*160*105, 160*130*105, 228*140*105]</td>\n",
       "      <td>245*160*105,160*130*105,228*140*105</td>\n",
       "      <td>cm 245*160*105,  cm 160*130*105, cm 228*140*105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Denislav Chipev (BG-SOF ECU Worldwide)\"_x000D...</td>\n",
       "      <td>dap to v7w-1w1 west vancouver, british columbi...</td>\n",
       "      <td>[(1, 5, 0, kg)]</td>\n",
       "      <td>('1', '5', '0', 'kg')</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>dap ratehello could i have dap charges for the...</td>\n",
       "      <td>[(120.00, 80.00, 150.00)]</td>\n",
       "      <td>('120.00', '80.00', '150.00')</td>\n",
       "      <td>l: 120.00 | w: 80.00 | h: 150.00 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Muna Khalifa (DHL CA)\" &lt;muna.khalifa@dhl.com&gt;</td>\n",
       "      <td>quote request ** po12 newmont project hello al...</td>\n",
       "      <td>[13 \"x 13\"x 4\" , 10\"x 10\"x 4\" ]</td>\n",
       "      <td>13 \"x 13\"x 4\" ,10\"x 10\"x 4\"</td>\n",
       "      <td>13 \"x 13\"x 4\" ,  10\"x 10\"x 4\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>hawai you don't often get email from sbgaudet ...</td>\n",
       "      <td>[(82, 48, 6), (82, 48, 6)]</td>\n",
       "      <td>('82', '48', '6'),('82', '48', '6')</td>\n",
       "      <td>82x48x6in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gaston Rioux &lt;ricointernational@videotron.ca&gt;</td>\n",
       "      <td>urgent lcl quote require from ex-works to term...</td>\n",
       "      <td>[(80, 120, 190)]</td>\n",
       "      <td>('80', '120', '190')</td>\n",
       "      <td>80 x 120 x 190 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Maria DeBellis &lt;debellism@delmar.ca&gt;</td>\n",
       "      <td>re: re lcl service from france to canada you d...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Krizia  Mejia\" &lt;krizia.mejia@etheinsen.com.do&gt;</td>\n",
       "      <td>rate request // lcl /canada - rd // raver rovr...</td>\n",
       "      <td>[(72.2, 71.4, 183)]</td>\n",
       "      <td>('72.2', '71.4', '183')</td>\n",
       "      <td>height : 72.2 inches width : 71.4 inches lengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sendy Duplan &lt;Sendy@Falconfreight.ca&gt;</td>\n",
       "      <td>rate request // 3 dg skids @ 1984 kgs // from ...</td>\n",
       "      <td>[(4, 4, 4)]</td>\n",
       "      <td>('4', '4', '4')</td>\n",
       "      <td>4x4x4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Trinet Team Kilo &lt;kilo@gotrinet.com&gt;</td>\n",
       "      <td>urgent rate request //techpack you don't often...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maria Aragao &lt;Maria.Aragao@craneww.com&gt;</td>\n",
       "      <td>rfq ocean freight - helsinki finland to anjou ...</td>\n",
       "      <td>[(94, 25, 33), (94, 49, 33)]</td>\n",
       "      <td>('94', '25', '33'),('94', '49', '33')</td>\n",
       "      <td>94 x 25 x 33, 94 x 49 x 33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Ramon Souza (BR-SAO ECU Worldwide)\" &lt;RamonSou...</td>\n",
       "      <td>tfa cargo logistics - cotação exportação marít...</td>\n",
       "      <td>[35x1]</td>\n",
       "      <td>35x1</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Karina Alonso (ES-BCN ECU Worldwide)\" &lt;Karina...</td>\n",
       "      <td>hi all pls could you send us your better dap r...</td>\n",
       "      <td>[360 110 110, 360 110 110]</td>\n",
       "      <td>360 110 110,360 110 110</td>\n",
       "      <td>360x110x110, 360x110x110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Operation - OTX Canada &lt;operation@otxcanada.com&gt;</td>\n",
       "      <td>montreal to port klang some people who receive...</td>\n",
       "      <td>[(47, 41, 92), (47, 41, 92), (43, 29, 64)]</td>\n",
       "      <td>('47', '41', '92'),('47', '41', '92'),('43', '...</td>\n",
       "      <td>47x41x92 inch, 47x41x92 inch, 43x29x64 inch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Grace Curro &lt;Grace@atlanticandpacific.com&gt;</td>\n",
       "      <td>urgent - pls advise lcl quote ref:tor- | from ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Rhea Pereira (DHL CA)\" &lt;rhea.pereira@dhl.com&gt;</td>\n",
       "      <td>rate request- network kw hi ecu, please advise...</td>\n",
       "      <td>[(43, 30x16)]</td>\n",
       "      <td>('43', '30x16')</td>\n",
       "      <td>43\"x30x16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gelena | Sealion Cargo &lt;gelena@sealioncargo.com&gt;</td>\n",
       "      <td>ltl request toronto-new zealand/ q19989 hello,...</td>\n",
       "      <td>[10\" x 4\" x 4\" ]</td>\n",
       "      <td>10\" x 4\" x 4\"</td>\n",
       "      <td>10' x 4' x 4'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Anastasiia Howard &lt;anastasiia.howard@leggett.com&gt;</td>\n",
       "      <td>lcl / po-1302 / sundern-amecke to london, cana...</td>\n",
       "      <td>[(80, 60, 80)]</td>\n",
       "      <td>('80', '60', '80')</td>\n",
       "      <td>80x60x80cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Bob Everett (UK-LON ECU Worldwide)\" &lt;BobEvere...</td>\n",
       "      <td>fw: freight quote - jkf - canada - sea - ac go...</td>\n",
       "      <td>[(3, 6, 0, bob)]</td>\n",
       "      <td>('3', '6', '0', 'bob')</td>\n",
       "      <td>120 x 80 x100 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>exw rate hello could i have exw charges for th...</td>\n",
       "      <td>[(122, 102, 145)]</td>\n",
       "      <td>('122', '102', '145')</td>\n",
       "      <td>122x102x145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Junaid Jawed &lt;Junaid.Jawed@chrobinson.com&gt;</td>\n",
       "      <td>export quote required from toronto to southamp...</td>\n",
       "      <td>[(67, 67, 88)]</td>\n",
       "      <td>('67', '67', '88')</td>\n",
       "      <td>67x67x88(h) inches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mouhcine - Brytor International &lt;mouhcine@ims....</td>\n",
       "      <td>dthc ! good day, i would need a rate from cfs ...</td>\n",
       "      <td>[(3, 0, 0, kg)]</td>\n",
       "      <td>('3', '0', '0', 'kg')</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>David Keung &lt;David.Keung@hellmann.com&gt;</td>\n",
       "      <td>dg shipment to guayaquil ( quotation &amp; booking...</td>\n",
       "      <td>[(48, 40, 43), (48, 40, 40)]</td>\n",
       "      <td>('48', '40', '43'),('48', '40', '40')</td>\n",
       "      <td>48x40x43, 48x40x40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>dap rate hello could i have dap charges for th...</td>\n",
       "      <td>[(100, 120, 220)]</td>\n",
       "      <td>('100', '120', '220')</td>\n",
       "      <td>100x120x220 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Linda Hernandez &lt;pricing@allegrofreight.com&gt;</td>\n",
       "      <td>lcl quote ref 011858 hi, please quote. commodi...</td>\n",
       "      <td>[(102, 122, 127)]</td>\n",
       "      <td>('102', '122', '127')</td>\n",
       "      <td>102x122x127 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>\"Berner, Nadine / Kuehne + Nagel / Tor ZS-BA\"\\...</td>\n",
       "      <td>kn rate request/ vancouver to barraquilla, co/...</td>\n",
       "      <td>[(140, 62, 58)]</td>\n",
       "      <td>('140', '62', '58')</td>\n",
       "      <td>140x62x58 inches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Mike Gray &lt;info@mglogistics.ca&gt;</td>\n",
       "      <td>ocean rate request - lcl (door delivery) good ...</td>\n",
       "      <td>[48x48x84, 52x48x84]</td>\n",
       "      <td>48x48x84,52x48x84</td>\n",
       "      <td>48x48x84, 52x48x84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>JALI Ussama &lt;ussama.jali@bollore.com&gt;</td>\n",
       "      <td>uj lcl dap munchen hi team, please quote from ...</td>\n",
       "      <td>[(47, 41, 92), (47, 41, 92), (37, 68, 92), (78...</td>\n",
       "      <td>('47', '41', '92'),('47', '41', '92'),('37', '...</td>\n",
       "      <td>47x41x92, 47x41x92, 37x68x92, 78x41x70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>\"JANI, TWISHA\" &lt;twisha.jani@dbschenker.com&gt;</td>\n",
       "      <td>re: new booking from montreal to shanghai hi a...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Operation - OTX Canada &lt;operation@otxcanada.com&gt;</td>\n",
       "      <td>re: lcl montreal to port klang some people who...</td>\n",
       "      <td>[(47, 41, 92), (47, 41, 92), (43, 29, 64)]</td>\n",
       "      <td>('47', '41', '92'),('47', '41', '92'),('43', '...</td>\n",
       "      <td>47x41x92 inch, 47x41x92 inch, 43x29x64 inch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>\"matt@aufreight.com\" &lt;matt@aufreight.com&gt;</td>\n",
       "      <td>re: export quote you don't often get email fro...</td>\n",
       "      <td>[(53, 44, 14), (53, 44, 76), (53, 44, 21)]</td>\n",
       "      <td>('53', '44', '14'),('53', '44', '76'),('53', '...</td>\n",
       "      <td>53x44x14, 53x44x76, 52x44x21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Brian | 1UP Cargo &lt;brian@1upcargo.com&gt;</td>\n",
       "      <td>re: alonso : exw : lcl : canada to india :21 h...</td>\n",
       "      <td>[(71, h, 44, 44)]</td>\n",
       "      <td>('71', 'h', '44', '44')</td>\n",
       "      <td>71(h)x44x44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>\"leparent@fffc.ca\" &lt;leparent@fffc.ca&gt;</td>\n",
       "      <td>rate request terminal toronto to door tainan c...</td>\n",
       "      <td>[110\" x 58\" x 34\" ]</td>\n",
       "      <td>110\" x 58\" x 34\"</td>\n",
       "      <td>110\"x58\"x34\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Alisha Lokhandwala &lt;Alisha.Lokhandwala@flyingf...</td>\n",
       "      <td>req ca to uk ocean q7556, sfi hello francois a...</td>\n",
       "      <td>[(48, 40, 44)]</td>\n",
       "      <td>('48', '40', '44')</td>\n",
       "      <td>48x40x44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>\"Denislav Chipev (BG-SOF ECU Worldwide)\"\\n\\t&lt;D...</td>\n",
       "      <td>dap charges l4m 4y8, canada / 3 pcs hi, please...</td>\n",
       "      <td>[(220, 175, 140, cm), (310, 220, 165, cm), (31...</td>\n",
       "      <td>('220', '175', '140', 'cm'),('310', '220', '16...</td>\n",
       "      <td>220x175x140 cm, 310x220x165 cm, 310x220x160 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Diego - Brytor International &lt;diego@ims.brytor...</td>\n",
       "      <td>re: urgent request -lcl quote ref:mtr- | from ...</td>\n",
       "      <td>[(5274, 9, 0, cph)]</td>\n",
       "      <td>('5274', '9', '0', 'cph')</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>\"matt@aufreight.com\" &lt;matt@aufreight.com&gt;</td>\n",
       "      <td>re: export question you don't often get email ...</td>\n",
       "      <td>[(134, 112, 35), (134, 112, 193), (134, 112, 53)]</td>\n",
       "      <td>('134', '112', '35'),('134', '112', '193'),('1...</td>\n",
       "      <td>134x112x35, 134x112x193, 134x112x53 cm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Kaushik Patel &lt;Kaushik.Patel@lei.ca&gt;</td>\n",
       "      <td>cfs toronto to door 44550 montoir-de-bretagne ...</td>\n",
       "      <td>[(122, 81, 46)]</td>\n",
       "      <td>('122', '81', '46')</td>\n",
       "      <td>122x81x46 cm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              emailid  \\\n",
       "0               Simon Holt <Simon.holt@nordic-on.com>   \n",
       "1   Linda-Nancy Colizza <Linda-Nancy.Colizza@hellm...   \n",
       "2              Sally Chieng <sallyc@itn-logistics.ca>   \n",
       "3      Michelle Guo <michelle.guo@sparxlogistics.com>   \n",
       "4             HLIWA Bozena <Bozena.HLIWA@bollore.com>   \n",
       "5               LCL Operations <lcl@goworldcargo.com>   \n",
       "6               LCL Operations <lcl@goworldcargo.com>   \n",
       "7                 Jude Menezes <jude@zodiacimpex.com>   \n",
       "8     Sandro Avilez Caldart <savilez@willsonintl.com>   \n",
       "9      Sarah | 1UP Cargo Pricing <Sarah@1upcargo.com>   \n",
       "10               Edgar Azar <edgar.azar@coleintl.com>   \n",
       "11  \"Sales Milan (IT-MIL ECU Worldwide)\" <SalesMil...   \n",
       "12  \"Denislav Chipev (BG-SOF ECU Worldwide)\"_x000D...   \n",
       "13  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "14     \"Muna Khalifa (DHL CA)\" <muna.khalifa@dhl.com>   \n",
       "15                                                  0   \n",
       "16      Gaston Rioux <ricointernational@videotron.ca>   \n",
       "17               Maria DeBellis <debellism@delmar.ca>   \n",
       "18    \"Krizia  Mejia\" <krizia.mejia@etheinsen.com.do>   \n",
       "19              Sendy Duplan <Sendy@Falconfreight.ca>   \n",
       "20               Trinet Team Kilo <kilo@gotrinet.com>   \n",
       "21            Maria Aragao <Maria.Aragao@craneww.com>   \n",
       "22  \"Ramon Souza (BR-SAO ECU Worldwide)\" <RamonSou...   \n",
       "23  \"Karina Alonso (ES-BCN ECU Worldwide)\" <Karina...   \n",
       "24   Operation - OTX Canada <operation@otxcanada.com>   \n",
       "25         Grace Curro <Grace@atlanticandpacific.com>   \n",
       "26     \"Rhea Pereira (DHL CA)\" <rhea.pereira@dhl.com>   \n",
       "27   Gelena | Sealion Cargo <gelena@sealioncargo.com>   \n",
       "28  Anastasiia Howard <anastasiia.howard@leggett.com>   \n",
       "29  \"Bob Everett (UK-LON ECU Worldwide)\" <BobEvere...   \n",
       "30  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "31         Junaid Jawed <Junaid.Jawed@chrobinson.com>   \n",
       "32  Mouhcine - Brytor International <mouhcine@ims....   \n",
       "33             David Keung <David.Keung@hellmann.com>   \n",
       "34  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "35       Linda Hernandez <pricing@allegrofreight.com>   \n",
       "36  \"Berner, Nadine / Kuehne + Nagel / Tor ZS-BA\"\\...   \n",
       "37                    Mike Gray <info@mglogistics.ca>   \n",
       "38              JALI Ussama <ussama.jali@bollore.com>   \n",
       "39        \"JANI, TWISHA\" <twisha.jani@dbschenker.com>   \n",
       "40   Operation - OTX Canada <operation@otxcanada.com>   \n",
       "41          \"matt@aufreight.com\" <matt@aufreight.com>   \n",
       "42             Brian | 1UP Cargo <brian@1upcargo.com>   \n",
       "43              \"leparent@fffc.ca\" <leparent@fffc.ca>   \n",
       "44  Alisha Lokhandwala <Alisha.Lokhandwala@flyingf...   \n",
       "45  \"Denislav Chipev (BG-SOF ECU Worldwide)\"\\n\\t<D...   \n",
       "46  Diego - Brytor International <diego@ims.brytor...   \n",
       "47          \"matt@aufreight.com\" <matt@aufreight.com>   \n",
       "48               Kaushik Patel <Kaushik.Patel@lei.ca>   \n",
       "\n",
       "                                      email_processed  \\\n",
       "0   vs: lcl quote request | from location : britis...   \n",
       "1   santos good morning, need rate please, termina...   \n",
       "2   rfq / itn door winnipeg, mb to cfs shanghai, c...   \n",
       "3   re: quote from toronto terminal to london term...   \n",
       "4   rate request lcl cargo non stackable cfs termi...   \n",
       "5   rate request | vancouver, canada to highton, a...   \n",
       "6   rate request | vancouver, canada to darlington...   \n",
       "7   lcl rate request toronto to singapore you don'...   \n",
       "8   lcl import | term toronto, on, ca to port of s...   \n",
       "9   fw: canada exw rates form door to nansha,guang...   \n",
       "10  fob lcl rate request /jebel ali to toronto hel...   \n",
       "11  edmonton via montreal dsv hi dear we have to q...   \n",
       "12  dap to v7w-1w1 west vancouver, british columbi...   \n",
       "13  dap ratehello could i have dap charges for the...   \n",
       "14  quote request ** po12 newmont project hello al...   \n",
       "15  hawai you don't often get email from sbgaudet ...   \n",
       "16  urgent lcl quote require from ex-works to term...   \n",
       "17  re: re lcl service from france to canada you d...   \n",
       "18  rate request // lcl /canada - rd // raver rovr...   \n",
       "19  rate request // 3 dg skids @ 1984 kgs // from ...   \n",
       "20  urgent rate request //techpack you don't often...   \n",
       "21  rfq ocean freight - helsinki finland to anjou ...   \n",
       "22  tfa cargo logistics - cotação exportação marít...   \n",
       "23  hi all pls could you send us your better dap r...   \n",
       "24  montreal to port klang some people who receive...   \n",
       "25  urgent - pls advise lcl quote ref:tor- | from ...   \n",
       "26  rate request- network kw hi ecu, please advise...   \n",
       "27  ltl request toronto-new zealand/ q19989 hello,...   \n",
       "28  lcl / po-1302 / sundern-amecke to london, cana...   \n",
       "29  fw: freight quote - jkf - canada - sea - ac go...   \n",
       "30  exw rate hello could i have exw charges for th...   \n",
       "31  export quote required from toronto to southamp...   \n",
       "32  dthc ! good day, i would need a rate from cfs ...   \n",
       "33  dg shipment to guayaquil ( quotation & booking...   \n",
       "34  dap rate hello could i have dap charges for th...   \n",
       "35  lcl quote ref 011858 hi, please quote. commodi...   \n",
       "36  kn rate request/ vancouver to barraquilla, co/...   \n",
       "37  ocean rate request - lcl (door delivery) good ...   \n",
       "38  uj lcl dap munchen hi team, please quote from ...   \n",
       "39  re: new booking from montreal to shanghai hi a...   \n",
       "40  re: lcl montreal to port klang some people who...   \n",
       "41  re: export quote you don't often get email fro...   \n",
       "42  re: alonso : exw : lcl : canada to india :21 h...   \n",
       "43  rate request terminal toronto to door tainan c...   \n",
       "44  req ca to uk ocean q7556, sfi hello francois a...   \n",
       "45  dap charges l4m 4y8, canada / 3 pcs hi, please...   \n",
       "46  re: urgent request -lcl quote ref:mtr- | from ...   \n",
       "47  re: export question you don't often get email ...   \n",
       "48  cfs toronto to door 44550 montoir-de-bretagne ...   \n",
       "\n",
       "                             predicted_Dimensions_raw  \\\n",
       "0                                                  []   \n",
       "1                                      [(44, 44, 42)]   \n",
       "2                                      [(48, 48, 50)]   \n",
       "3   [48*40*70 in, 48*40*70 in, 48 *40*60 in, 48*40...   \n",
       "4                                [(0.80, 1.20, 1.15)]   \n",
       "5                                      [(75, 51, 67)]   \n",
       "6                  [(122, 102, 161), (122, 102, 121)]   \n",
       "7                 [(89, 112, 132), (91.5, 91.5, 104)]   \n",
       "8             [130\" x 90\" x 98 in, 80\" x 58\" x 84 in]   \n",
       "9                                       [122*102*211]   \n",
       "10                                  [(120, 100, 141)]   \n",
       "11            [245*160*105, 160*130*105, 228*140*105]   \n",
       "12                                    [(1, 5, 0, kg)]   \n",
       "13                          [(120.00, 80.00, 150.00)]   \n",
       "14                    [13 \"x 13\"x 4\" , 10\"x 10\"x 4\" ]   \n",
       "15                         [(82, 48, 6), (82, 48, 6)]   \n",
       "16                                   [(80, 120, 190)]   \n",
       "17                                                 []   \n",
       "18                                [(72.2, 71.4, 183)]   \n",
       "19                                        [(4, 4, 4)]   \n",
       "20                                                 []   \n",
       "21                       [(94, 25, 33), (94, 49, 33)]   \n",
       "22                                             [35x1]   \n",
       "23                         [360 110 110, 360 110 110]   \n",
       "24         [(47, 41, 92), (47, 41, 92), (43, 29, 64)]   \n",
       "25                                                 []   \n",
       "26                                      [(43, 30x16)]   \n",
       "27                                   [10\" x 4\" x 4\" ]   \n",
       "28                                     [(80, 60, 80)]   \n",
       "29                                   [(3, 6, 0, bob)]   \n",
       "30                                  [(122, 102, 145)]   \n",
       "31                                     [(67, 67, 88)]   \n",
       "32                                    [(3, 0, 0, kg)]   \n",
       "33                       [(48, 40, 43), (48, 40, 40)]   \n",
       "34                                  [(100, 120, 220)]   \n",
       "35                                  [(102, 122, 127)]   \n",
       "36                                    [(140, 62, 58)]   \n",
       "37                               [48x48x84, 52x48x84]   \n",
       "38  [(47, 41, 92), (47, 41, 92), (37, 68, 92), (78...   \n",
       "39                                                 []   \n",
       "40         [(47, 41, 92), (47, 41, 92), (43, 29, 64)]   \n",
       "41         [(53, 44, 14), (53, 44, 76), (53, 44, 21)]   \n",
       "42                                  [(71, h, 44, 44)]   \n",
       "43                                [110\" x 58\" x 34\" ]   \n",
       "44                                     [(48, 40, 44)]   \n",
       "45  [(220, 175, 140, cm), (310, 220, 165, cm), (31...   \n",
       "46                                [(5274, 9, 0, cph)]   \n",
       "47  [(134, 112, 35), (134, 112, 193), (134, 112, 53)]   \n",
       "48                                    [(122, 81, 46)]   \n",
       "\n",
       "                                 predicted_Dimensions  \\\n",
       "0                                                       \n",
       "1                                  ('44', '44', '42')   \n",
       "2                                  ('48', '48', '50')   \n",
       "3    48*40*70 in,48*40*70 in,48 *40*60 in,48*40*76 in   \n",
       "4                            ('0.80', '1.20', '1.15')   \n",
       "5                                  ('75', '51', '67')   \n",
       "6         ('122', '102', '161'),('122', '102', '121')   \n",
       "7        ('89', '112', '132'),('91.5', '91.5', '104')   \n",
       "8                130\" x 90\" x 98 in,80\" x 58\" x 84 in   \n",
       "9                                         122*102*211   \n",
       "10                              ('120', '100', '141')   \n",
       "11                245*160*105,160*130*105,228*140*105   \n",
       "12                              ('1', '5', '0', 'kg')   \n",
       "13                      ('120.00', '80.00', '150.00')   \n",
       "14                       13 \"x 13\"x 4\" ,10\"x 10\"x 4\"    \n",
       "15                ('82', '48', '6'),('82', '48', '6')   \n",
       "16                               ('80', '120', '190')   \n",
       "17                                                      \n",
       "18                            ('72.2', '71.4', '183')   \n",
       "19                                    ('4', '4', '4')   \n",
       "20                                                      \n",
       "21              ('94', '25', '33'),('94', '49', '33')   \n",
       "22                                               35x1   \n",
       "23                            360 110 110,360 110 110   \n",
       "24  ('47', '41', '92'),('47', '41', '92'),('43', '...   \n",
       "25                                                      \n",
       "26                                    ('43', '30x16')   \n",
       "27                                     10\" x 4\" x 4\"    \n",
       "28                                 ('80', '60', '80')   \n",
       "29                             ('3', '6', '0', 'bob')   \n",
       "30                              ('122', '102', '145')   \n",
       "31                                 ('67', '67', '88')   \n",
       "32                              ('3', '0', '0', 'kg')   \n",
       "33              ('48', '40', '43'),('48', '40', '40')   \n",
       "34                              ('100', '120', '220')   \n",
       "35                              ('102', '122', '127')   \n",
       "36                                ('140', '62', '58')   \n",
       "37                                  48x48x84,52x48x84   \n",
       "38  ('47', '41', '92'),('47', '41', '92'),('37', '...   \n",
       "39                                                      \n",
       "40  ('47', '41', '92'),('47', '41', '92'),('43', '...   \n",
       "41  ('53', '44', '14'),('53', '44', '76'),('53', '...   \n",
       "42                            ('71', 'h', '44', '44')   \n",
       "43                                  110\" x 58\" x 34\"    \n",
       "44                                 ('48', '40', '44')   \n",
       "45  ('220', '175', '140', 'cm'),('310', '220', '16...   \n",
       "46                          ('5274', '9', '0', 'cph')   \n",
       "47  ('134', '112', '35'),('134', '112', '193'),('1...   \n",
       "48                                ('122', '81', '46')   \n",
       "\n",
       "                                  expected_Dimensions  \n",
       "0                                                 nan  \n",
       "1                                          44 x44 x42  \n",
       "2                                           48x48 x50  \n",
       "3   48*40*70 in,  48*40*70 in, 48 *40*60 in, 48*40...  \n",
       "4                               0.80x 1.20 x 1.15 h m  \n",
       "5                                            75x51x67  \n",
       "6              122 x 102 x 161 cm, 122 x 102 x 121 cm  \n",
       "7          89 x 112 x 132 cms, 91.5 x 91.5 x 104 cms   \n",
       "8               130\" x 90\" x 98 in, 80\" x 58\" x 84 in  \n",
       "9                                       122*102*211cm  \n",
       "10                                        120x100x141  \n",
       "11    cm 245*160*105,  cm 160*130*105, cm 228*140*105  \n",
       "12                                                nan  \n",
       "13               l: 120.00 | w: 80.00 | h: 150.00 cm   \n",
       "14                      13 \"x 13\"x 4\" ,  10\"x 10\"x 4\"  \n",
       "15                                          82x48x6in  \n",
       "16                                  80 x 120 x 190 cm  \n",
       "17                                                nan  \n",
       "18  height : 72.2 inches width : 71.4 inches lengt...  \n",
       "19                                              4x4x4  \n",
       "20                                                nan  \n",
       "21                         94 x 25 x 33, 94 x 49 x 33  \n",
       "22                                                nan  \n",
       "23                           360x110x110, 360x110x110  \n",
       "24        47x41x92 inch, 47x41x92 inch, 43x29x64 inch  \n",
       "25                                                nan  \n",
       "26                                          43\"x30x16  \n",
       "27                                     10' x 4' x 4'   \n",
       "28                                         80x60x80cm  \n",
       "29                                   120 x 80 x100 cm  \n",
       "30                                        122x102x145  \n",
       "31                                 67x67x88(h) inches  \n",
       "32                                                nan  \n",
       "33                                 48x40x43, 48x40x40  \n",
       "34                                     100x120x220 cm  \n",
       "35                                     102x122x127 cm  \n",
       "36                                   140x62x58 inches  \n",
       "37                                 48x48x84, 52x48x84  \n",
       "38             47x41x92, 47x41x92, 37x68x92, 78x41x70  \n",
       "39                                                nan  \n",
       "40        47x41x92 inch, 47x41x92 inch, 43x29x64 inch  \n",
       "41                       53x44x14, 53x44x76, 52x44x21  \n",
       "42                                        71(h)x44x44  \n",
       "43                                       110\"x58\"x34\"  \n",
       "44                                           48x40x44  \n",
       "45     220x175x140 cm, 310x220x165 cm, 310x220x160 cm  \n",
       "46                                                nan  \n",
       "47             134x112x35, 134x112x193, 134x112x53 cm  \n",
       "48                                       122x81x46 cm  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation \n",
    "\n",
    "df[['emailid', 'email_processed','predicted_Dimensions_raw', 'predicted_Dimensions', 'expected_Dimensions']]#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69486a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_quantity(text):\n",
    "    text = text.replace(\"'\", '')\n",
    "\n",
    "    regex_patterns = [\n",
    "        r'\\d+ x pallets', r'\\d+ x pallet',r'\\d+ plt',r'\\d+ plts',r'\\d+ pallet', r'\\d+ pallets',\n",
    "        r'\\d+xpallets', r'\\d+xpallet',r'\\d+plt',r'\\d+plts',r'\\d+pallet', r'\\d+pallets',\n",
    "         r'\\d+ x skid', r'\\d+ x skids', r'\\d+ skid',r'\\d+ skids' ,\n",
    "        r'\\d+xskid', r'\\d+xskids', r'\\d+skid',r'\\d+skids' ,\n",
    "        r'\\d+ x crate', r'\\d+ x crates', r'\\d+ crate',r'\\d+ crates',\n",
    "        r'\\d+ w/crate',r'\\d+ w/crates', r'\\d+ wooden crate',r'\\d+ wooden crates',r'\\d+ wc',\n",
    "        r'\\d+xcrate', r'\\d+xcrates', r'\\d+crate',r'\\d+crates',\n",
    "        r'\\d+xpkg', r'\\d+x pkg', r'\\d+ pkgs',r'\\d+ pkgs',r'\\d+ pkg',\n",
    "        r'\\d+ pieces', r'\\d+ total pieces',r'total pieces \\d+',r'palets totals: \\d+',\n",
    "        \n",
    "        r'\\b\\w+\\s+wooden crate\\b'\n",
    "        ]\n",
    "\n",
    "    qty = []\n",
    "    for pattern in regex_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            qty.extend(matches)\n",
    "            break\n",
    "\n",
    "    return qty\n",
    "\n",
    "\n",
    "# Function to calculate total quantity from comma-separated values\n",
    "def calculate_total_quantity(value_str):\n",
    "    if((',') in value_str):\n",
    "        values = value_str.split(',')\n",
    "        if(len(values)>1):\n",
    "            quantity = sum(int(val.split()[0]) for val in values)\n",
    "        else:\n",
    "            quantity = value_str\n",
    "    else:\n",
    "        quantity = value_str\n",
    "    return quantity\n",
    "\n",
    "number_mapping = {\n",
    "    \"one\": 1,\n",
    "    \"two\": 2,\n",
    "    \"three\": 3,\n",
    "    \"four\": 4,\n",
    "    \"five\": 5,\n",
    "    \"six\": 6,\n",
    "    \"seven\": 7,\n",
    "    \"eight\": 8,\n",
    "    \"nine\": 9,\n",
    "    \"ten\": 10\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "def replace_alphabetical_numbers(text):\n",
    "    words = text.lower().split(' ')  # Split the text into lowercase words\n",
    "    replaced_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in number_mapping:\n",
    "            replaced_words.append(str(number_mapping[word]))\n",
    "        else:\n",
    "            replaced_words.append(word)\n",
    "\n",
    "    replaced_text = ' '.join(replaced_words)\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "df['predicted_quantity_raw'] = df['email_processed'].apply(process_quantity)\n",
    "# Removing Square brackets - list\n",
    "df['predicted_quantity_raw'] = [','.join(map(str, l)) for l in df['predicted_quantity_raw']]\n",
    "df['predicted_quantity_raw'] = df['predicted_quantity_raw'].apply(lambda text: replace_alphabetical_numbers(text))\n",
    "\n",
    "\n",
    "\n",
    "# Fix scenarios where multiple digits are extracted\n",
    "df['predicted_quantity_raw_1'] = df['predicted_quantity_raw'].apply(calculate_total_quantity)\n",
    "df['predicted_quantity'] = df['predicted_quantity_raw_1']\n",
    "df['predicted_quantity'] = df['predicted_quantity'].astype(str).apply(remove_non_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "941510ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailid</th>\n",
       "      <th>email_processed</th>\n",
       "      <th>predicted_quantity_raw</th>\n",
       "      <th>predicted_quantity_raw_1</th>\n",
       "      <th>predicted_quantity</th>\n",
       "      <th>expected_Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>dap rate hello could i have dap charges for th...</td>\n",
       "      <td>4 x pallets</td>\n",
       "      <td>4 x pallets</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>exw rate hello could i have exw charges for th...</td>\n",
       "      <td>1 plt</td>\n",
       "      <td>1 plt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>dap ratehello could i have dap charges for the...</td>\n",
       "      <td>total pieces 13</td>\n",
       "      <td>total pieces 13</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>\"Berner, Nadine / Kuehne + Nagel / Tor ZS-BA\"\\...</td>\n",
       "      <td>kn rate request/ vancouver to barraquilla, co/...</td>\n",
       "      <td>2 crate</td>\n",
       "      <td>2 crate</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Bob Everett (UK-LON ECU Worldwide)\" &lt;BobEvere...</td>\n",
       "      <td>fw: freight quote - jkf - canada - sea - ac go...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>\"Denislav Chipev (BG-SOF ECU Worldwide)\"\\n\\t&lt;D...</td>\n",
       "      <td>dap charges l4m 4y8, canada / 3 pcs hi, please...</td>\n",
       "      <td>1 pallet,1 pallet,1 pallet</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Denislav Chipev (BG-SOF ECU Worldwide)\"_x000D...</td>\n",
       "      <td>dap to v7w-1w1 west vancouver, british columbi...</td>\n",
       "      <td>1 pallet,1 pallet</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>\"JANI, TWISHA\" &lt;twisha.jani@dbschenker.com&gt;</td>\n",
       "      <td>re: new booking from montreal to shanghai hi a...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Karina Alonso (ES-BCN ECU Worldwide)\" &lt;Karina...</td>\n",
       "      <td>hi all pls could you send us your better dap r...</td>\n",
       "      <td>palets totals: 2</td>\n",
       "      <td>palets totals: 2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Krizia  Mejia\" &lt;krizia.mejia@etheinsen.com.do&gt;</td>\n",
       "      <td>rate request // lcl /canada - rd // raver rovr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Muna Khalifa (DHL CA)\" &lt;muna.khalifa@dhl.com&gt;</td>\n",
       "      <td>quote request ** po12 newmont project hello al...</td>\n",
       "      <td>2 pieces</td>\n",
       "      <td>2 pieces</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Ramon Souza (BR-SAO ECU Worldwide)\" &lt;RamonSou...</td>\n",
       "      <td>tfa cargo logistics - cotação exportação marít...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Rhea Pereira (DHL CA)\" &lt;rhea.pereira@dhl.com&gt;</td>\n",
       "      <td>rate request- network kw hi ecu, please advise...</td>\n",
       "      <td>1 wooden crate</td>\n",
       "      <td>1 wooden crate</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Sales Milan (IT-MIL ECU Worldwide)\" &lt;SalesMil...</td>\n",
       "      <td>edmonton via montreal dsv hi dear we have to q...</td>\n",
       "      <td>2 pkg,1 pkg,1 pkg</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>\"leparent@fffc.ca\" &lt;leparent@fffc.ca&gt;</td>\n",
       "      <td>rate request terminal toronto to door tainan c...</td>\n",
       "      <td>1 crate</td>\n",
       "      <td>1 crate</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>\"matt@aufreight.com\" &lt;matt@aufreight.com&gt;</td>\n",
       "      <td>re: export quote you don't often get email fro...</td>\n",
       "      <td>3 skid</td>\n",
       "      <td>3 skid</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>\"matt@aufreight.com\" &lt;matt@aufreight.com&gt;</td>\n",
       "      <td>re: export question you don't often get email ...</td>\n",
       "      <td>3 skid</td>\n",
       "      <td>3 skid</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>hawai you don't often get email from sbgaudet ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Alisha Lokhandwala &lt;Alisha.Lokhandwala@flyingf...</td>\n",
       "      <td>req ca to uk ocean q7556, sfi hello francois a...</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Anastasiia Howard &lt;anastasiia.howard@leggett.com&gt;</td>\n",
       "      <td>lcl / po-1302 / sundern-amecke to london, cana...</td>\n",
       "      <td>2 pallet</td>\n",
       "      <td>2 pallet</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Brian | 1UP Cargo &lt;brian@1upcargo.com&gt;</td>\n",
       "      <td>re: alonso : exw : lcl : canada to india :21 h...</td>\n",
       "      <td>5 pallet</td>\n",
       "      <td>5 pallet</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>David Keung &lt;David.Keung@hellmann.com&gt;</td>\n",
       "      <td>dg shipment to guayaquil ( quotation &amp; booking...</td>\n",
       "      <td>2 skid</td>\n",
       "      <td>2 skid</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Diego - Brytor International &lt;diego@ims.brytor...</td>\n",
       "      <td>re: urgent request -lcl quote ref:mtr- | from ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Edgar Azar &lt;edgar.azar@coleintl.com&gt;</td>\n",
       "      <td>fob lcl rate request /jebel ali to toronto hel...</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gaston Rioux &lt;ricointernational@videotron.ca&gt;</td>\n",
       "      <td>urgent lcl quote require from ex-works to term...</td>\n",
       "      <td>14 pallet</td>\n",
       "      <td>14 pallet</td>\n",
       "      <td>14</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gelena | Sealion Cargo &lt;gelena@sealioncargo.com&gt;</td>\n",
       "      <td>ltl request toronto-new zealand/ q19989 hello,...</td>\n",
       "      <td>1 skid</td>\n",
       "      <td>1 skid</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Grace Curro &lt;Grace@atlanticandpacific.com&gt;</td>\n",
       "      <td>urgent - pls advise lcl quote ref:tor- | from ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HLIWA Bozena &lt;Bozena.HLIWA@bollore.com&gt;</td>\n",
       "      <td>rate request lcl cargo non stackable cfs termi...</td>\n",
       "      <td>2 pallet</td>\n",
       "      <td>2 pallet</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>JALI Ussama &lt;ussama.jali@bollore.com&gt;</td>\n",
       "      <td>uj lcl dap munchen hi team, please quote from ...</td>\n",
       "      <td>1 wc,1 wc,1 wc,1 wc</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jude Menezes &lt;jude@zodiacimpex.com&gt;</td>\n",
       "      <td>lcl rate request toronto to singapore you don'...</td>\n",
       "      <td>2 skid</td>\n",
       "      <td>2 skid</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Junaid Jawed &lt;Junaid.Jawed@chrobinson.com&gt;</td>\n",
       "      <td>export quote required from toronto to southamp...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Kaushik Patel &lt;Kaushik.Patel@lei.ca&gt;</td>\n",
       "      <td>cfs toronto to door 44550 montoir-de-bretagne ...</td>\n",
       "      <td>1 plt</td>\n",
       "      <td>1 plt</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>rate request | vancouver, canada to darlington...</td>\n",
       "      <td>1 pallet,1 pallet</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>rate request | vancouver, canada to highton, a...</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Linda Hernandez &lt;pricing@allegrofreight.com&gt;</td>\n",
       "      <td>lcl quote ref 011858 hi, please quote. commodi...</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1 pallet</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linda-Nancy Colizza &lt;Linda-Nancy.Colizza@hellm...</td>\n",
       "      <td>santos good morning, need rate please, termina...</td>\n",
       "      <td>2 skid</td>\n",
       "      <td>2 skid</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maria Aragao &lt;Maria.Aragao@craneww.com&gt;</td>\n",
       "      <td>rfq ocean freight - helsinki finland to anjou ...</td>\n",
       "      <td>1 pallet,1 pallet</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Maria DeBellis &lt;debellism@delmar.ca&gt;</td>\n",
       "      <td>re: re lcl service from france to canada you d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle Guo &lt;michelle.guo@sparxlogistics.com&gt;</td>\n",
       "      <td>re: quote from toronto terminal to london term...</td>\n",
       "      <td>4 plt</td>\n",
       "      <td>4 plt</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Mike Gray &lt;info@mglogistics.ca&gt;</td>\n",
       "      <td>ocean rate request - lcl (door delivery) good ...</td>\n",
       "      <td>5 skid</td>\n",
       "      <td>5 skid</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mouhcine - Brytor International &lt;mouhcine@ims....</td>\n",
       "      <td>dthc ! good day, i would need a rate from cfs ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Operation - OTX Canada &lt;operation@otxcanada.com&gt;</td>\n",
       "      <td>montreal to port klang some people who receive...</td>\n",
       "      <td>3 w/crate</td>\n",
       "      <td>3 w/crate</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Operation - OTX Canada &lt;operation@otxcanada.com&gt;</td>\n",
       "      <td>re: lcl montreal to port klang some people who...</td>\n",
       "      <td>3 w/crate</td>\n",
       "      <td>3 w/crate</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sally Chieng &lt;sallyc@itn-logistics.ca&gt;</td>\n",
       "      <td>rfq / itn door winnipeg, mb to cfs shanghai, c...</td>\n",
       "      <td>4 skid</td>\n",
       "      <td>4 skid</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sandro Avilez Caldart &lt;savilez@willsonintl.com&gt;</td>\n",
       "      <td>lcl import | term toronto, on, ca to port of s...</td>\n",
       "      <td>2 crate,2 crate</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sarah | 1UP Cargo Pricing &lt;Sarah@1upcargo.com&gt;</td>\n",
       "      <td>fw: canada exw rates form door to nansha,guang...</td>\n",
       "      <td>7plt</td>\n",
       "      <td>7plt</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sendy Duplan &lt;Sendy@Falconfreight.ca&gt;</td>\n",
       "      <td>rate request // 3 dg skids @ 1984 kgs // from ...</td>\n",
       "      <td>3 pallet</td>\n",
       "      <td>3 pallet</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simon Holt &lt;Simon.holt@nordic-on.com&gt;</td>\n",
       "      <td>vs: lcl quote request | from location : britis...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Trinet Team Kilo &lt;kilo@gotrinet.com&gt;</td>\n",
       "      <td>urgent rate request //techpack you don't often...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              emailid  \\\n",
       "34  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "30  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "13  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "36  \"Berner, Nadine / Kuehne + Nagel / Tor ZS-BA\"\\...   \n",
       "29  \"Bob Everett (UK-LON ECU Worldwide)\" <BobEvere...   \n",
       "45  \"Denislav Chipev (BG-SOF ECU Worldwide)\"\\n\\t<D...   \n",
       "12  \"Denislav Chipev (BG-SOF ECU Worldwide)\"_x000D...   \n",
       "39        \"JANI, TWISHA\" <twisha.jani@dbschenker.com>   \n",
       "23  \"Karina Alonso (ES-BCN ECU Worldwide)\" <Karina...   \n",
       "18    \"Krizia  Mejia\" <krizia.mejia@etheinsen.com.do>   \n",
       "14     \"Muna Khalifa (DHL CA)\" <muna.khalifa@dhl.com>   \n",
       "22  \"Ramon Souza (BR-SAO ECU Worldwide)\" <RamonSou...   \n",
       "26     \"Rhea Pereira (DHL CA)\" <rhea.pereira@dhl.com>   \n",
       "11  \"Sales Milan (IT-MIL ECU Worldwide)\" <SalesMil...   \n",
       "43              \"leparent@fffc.ca\" <leparent@fffc.ca>   \n",
       "41          \"matt@aufreight.com\" <matt@aufreight.com>   \n",
       "47          \"matt@aufreight.com\" <matt@aufreight.com>   \n",
       "15                                                  0   \n",
       "44  Alisha Lokhandwala <Alisha.Lokhandwala@flyingf...   \n",
       "28  Anastasiia Howard <anastasiia.howard@leggett.com>   \n",
       "42             Brian | 1UP Cargo <brian@1upcargo.com>   \n",
       "33             David Keung <David.Keung@hellmann.com>   \n",
       "46  Diego - Brytor International <diego@ims.brytor...   \n",
       "10               Edgar Azar <edgar.azar@coleintl.com>   \n",
       "16      Gaston Rioux <ricointernational@videotron.ca>   \n",
       "27   Gelena | Sealion Cargo <gelena@sealioncargo.com>   \n",
       "25         Grace Curro <Grace@atlanticandpacific.com>   \n",
       "4             HLIWA Bozena <Bozena.HLIWA@bollore.com>   \n",
       "38              JALI Ussama <ussama.jali@bollore.com>   \n",
       "7                 Jude Menezes <jude@zodiacimpex.com>   \n",
       "31         Junaid Jawed <Junaid.Jawed@chrobinson.com>   \n",
       "48               Kaushik Patel <Kaushik.Patel@lei.ca>   \n",
       "6               LCL Operations <lcl@goworldcargo.com>   \n",
       "5               LCL Operations <lcl@goworldcargo.com>   \n",
       "35       Linda Hernandez <pricing@allegrofreight.com>   \n",
       "1   Linda-Nancy Colizza <Linda-Nancy.Colizza@hellm...   \n",
       "21            Maria Aragao <Maria.Aragao@craneww.com>   \n",
       "17               Maria DeBellis <debellism@delmar.ca>   \n",
       "3      Michelle Guo <michelle.guo@sparxlogistics.com>   \n",
       "37                    Mike Gray <info@mglogistics.ca>   \n",
       "32  Mouhcine - Brytor International <mouhcine@ims....   \n",
       "24   Operation - OTX Canada <operation@otxcanada.com>   \n",
       "40   Operation - OTX Canada <operation@otxcanada.com>   \n",
       "2              Sally Chieng <sallyc@itn-logistics.ca>   \n",
       "8     Sandro Avilez Caldart <savilez@willsonintl.com>   \n",
       "9      Sarah | 1UP Cargo Pricing <Sarah@1upcargo.com>   \n",
       "19              Sendy Duplan <Sendy@Falconfreight.ca>   \n",
       "0               Simon Holt <Simon.holt@nordic-on.com>   \n",
       "20               Trinet Team Kilo <kilo@gotrinet.com>   \n",
       "\n",
       "                                      email_processed  \\\n",
       "34  dap rate hello could i have dap charges for th...   \n",
       "30  exw rate hello could i have exw charges for th...   \n",
       "13  dap ratehello could i have dap charges for the...   \n",
       "36  kn rate request/ vancouver to barraquilla, co/...   \n",
       "29  fw: freight quote - jkf - canada - sea - ac go...   \n",
       "45  dap charges l4m 4y8, canada / 3 pcs hi, please...   \n",
       "12  dap to v7w-1w1 west vancouver, british columbi...   \n",
       "39  re: new booking from montreal to shanghai hi a...   \n",
       "23  hi all pls could you send us your better dap r...   \n",
       "18  rate request // lcl /canada - rd // raver rovr...   \n",
       "14  quote request ** po12 newmont project hello al...   \n",
       "22  tfa cargo logistics - cotação exportação marít...   \n",
       "26  rate request- network kw hi ecu, please advise...   \n",
       "11  edmonton via montreal dsv hi dear we have to q...   \n",
       "43  rate request terminal toronto to door tainan c...   \n",
       "41  re: export quote you don't often get email fro...   \n",
       "47  re: export question you don't often get email ...   \n",
       "15  hawai you don't often get email from sbgaudet ...   \n",
       "44  req ca to uk ocean q7556, sfi hello francois a...   \n",
       "28  lcl / po-1302 / sundern-amecke to london, cana...   \n",
       "42  re: alonso : exw : lcl : canada to india :21 h...   \n",
       "33  dg shipment to guayaquil ( quotation & booking...   \n",
       "46  re: urgent request -lcl quote ref:mtr- | from ...   \n",
       "10  fob lcl rate request /jebel ali to toronto hel...   \n",
       "16  urgent lcl quote require from ex-works to term...   \n",
       "27  ltl request toronto-new zealand/ q19989 hello,...   \n",
       "25  urgent - pls advise lcl quote ref:tor- | from ...   \n",
       "4   rate request lcl cargo non stackable cfs termi...   \n",
       "38  uj lcl dap munchen hi team, please quote from ...   \n",
       "7   lcl rate request toronto to singapore you don'...   \n",
       "31  export quote required from toronto to southamp...   \n",
       "48  cfs toronto to door 44550 montoir-de-bretagne ...   \n",
       "6   rate request | vancouver, canada to darlington...   \n",
       "5   rate request | vancouver, canada to highton, a...   \n",
       "35  lcl quote ref 011858 hi, please quote. commodi...   \n",
       "1   santos good morning, need rate please, termina...   \n",
       "21  rfq ocean freight - helsinki finland to anjou ...   \n",
       "17  re: re lcl service from france to canada you d...   \n",
       "3   re: quote from toronto terminal to london term...   \n",
       "37  ocean rate request - lcl (door delivery) good ...   \n",
       "32  dthc ! good day, i would need a rate from cfs ...   \n",
       "24  montreal to port klang some people who receive...   \n",
       "40  re: lcl montreal to port klang some people who...   \n",
       "2   rfq / itn door winnipeg, mb to cfs shanghai, c...   \n",
       "8   lcl import | term toronto, on, ca to port of s...   \n",
       "9   fw: canada exw rates form door to nansha,guang...   \n",
       "19  rate request // 3 dg skids @ 1984 kgs // from ...   \n",
       "0   vs: lcl quote request | from location : britis...   \n",
       "20  urgent rate request //techpack you don't often...   \n",
       "\n",
       "        predicted_quantity_raw predicted_quantity_raw_1 predicted_quantity  \\\n",
       "34                 4 x pallets              4 x pallets                  4   \n",
       "30                       1 plt                    1 plt                  1   \n",
       "13             total pieces 13          total pieces 13                 13   \n",
       "36                     2 crate                  2 crate                  2   \n",
       "29                                                                           \n",
       "45  1 pallet,1 pallet,1 pallet                        3                  3   \n",
       "12           1 pallet,1 pallet                        2                  2   \n",
       "39                                                                           \n",
       "23            palets totals: 2         palets totals: 2                  2   \n",
       "18                                                                           \n",
       "14                    2 pieces                 2 pieces                  2   \n",
       "22                                                                           \n",
       "26              1 wooden crate           1 wooden crate                  1   \n",
       "11           2 pkg,1 pkg,1 pkg                        4                  4   \n",
       "43                     1 crate                  1 crate                  1   \n",
       "41                      3 skid                   3 skid                  3   \n",
       "47                      3 skid                   3 skid                  3   \n",
       "15                                                                           \n",
       "44                    1 pallet                 1 pallet                  1   \n",
       "28                    2 pallet                 2 pallet                  2   \n",
       "42                    5 pallet                 5 pallet                  5   \n",
       "33                      2 skid                   2 skid                  2   \n",
       "46                                                                           \n",
       "10                    1 pallet                 1 pallet                  1   \n",
       "16                   14 pallet                14 pallet                 14   \n",
       "27                      1 skid                   1 skid                  1   \n",
       "25                                                                           \n",
       "4                     2 pallet                 2 pallet                  2   \n",
       "38         1 wc,1 wc,1 wc,1 wc                        4                  4   \n",
       "7                       2 skid                   2 skid                  2   \n",
       "31                                                                           \n",
       "48                       1 plt                    1 plt                  1   \n",
       "6            1 pallet,1 pallet                        2                  2   \n",
       "5                     1 pallet                 1 pallet                  1   \n",
       "35                    1 pallet                 1 pallet                  1   \n",
       "1                       2 skid                   2 skid                  2   \n",
       "21           1 pallet,1 pallet                        2                  2   \n",
       "17                                                                           \n",
       "3                        4 plt                    4 plt                  4   \n",
       "37                      5 skid                   5 skid                  5   \n",
       "32                                                                           \n",
       "24                   3 w/crate                3 w/crate                  3   \n",
       "40                   3 w/crate                3 w/crate                  3   \n",
       "2                       4 skid                   4 skid                  4   \n",
       "8              2 crate,2 crate                        4                  4   \n",
       "9                         7plt                     7plt                  7   \n",
       "19                    3 pallet                 3 pallet                  3   \n",
       "0                                                                            \n",
       "20                                                                           \n",
       "\n",
       "   expected_Quantity  \n",
       "34               4.0  \n",
       "30               1.0  \n",
       "13              13.0  \n",
       "36               2.0  \n",
       "29               1.0  \n",
       "45               3.0  \n",
       "12               1.0  \n",
       "39               nan  \n",
       "23               2.0  \n",
       "18               1.0  \n",
       "14               2.0  \n",
       "22               nan  \n",
       "26               1.0  \n",
       "11               4.0  \n",
       "43               1.0  \n",
       "41               3.0  \n",
       "47               3.0  \n",
       "15               1.0  \n",
       "44              17.0  \n",
       "28               2.0  \n",
       "42               5.0  \n",
       "33               2.0  \n",
       "46               nan  \n",
       "10               1.0  \n",
       "16              14.0  \n",
       "27               1.0  \n",
       "25               nan  \n",
       "4                2.0  \n",
       "38               4.0  \n",
       "7                2.0  \n",
       "31               1.0  \n",
       "48               1.0  \n",
       "6                nan  \n",
       "5                1.0  \n",
       "35               1.0  \n",
       "1                2.0  \n",
       "21               2.0  \n",
       "17               nan  \n",
       "3                4.0  \n",
       "37               5.0  \n",
       "32               nan  \n",
       "24               3.0  \n",
       "40               3.0  \n",
       "2                4.0  \n",
       "8                4.0  \n",
       "9                7.0  \n",
       "19               3.0  \n",
       "0                nan  \n",
       "20               nan  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row 45 - addition of comma separated values, and \"one wooden carret\" include alphabetical combination\n",
    "df[['emailid', 'email_processed','predicted_quantity_raw', 'predicted_quantity_raw_1', 'predicted_quantity','expected_Quantity']].sort_values(by = 'emailid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0c01b6",
   "metadata": {},
   "source": [
    "### quanity indicator - mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9105144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['', '2', '1', '4', '3', '5', '7', '13', '14'], dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['predicted_quantity'].value_counts().index# indicator - quan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ddf3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_quantity'] = df['predicted_quantity'].astype(str)\n",
    "df['predicted_quantity'] = df['predicted_quantity'].apply(remove_letters)\n",
    "df['predicted_quantity'] = df['predicted_quantity'].apply(remove_non_digits)\n",
    "df.loc[df['predicted_quantity']=='', 'predicted_quantity'] = np.nan\n",
    "df['predicted_quantity'] = df['predicted_quantity'].fillna(0)\n",
    "df['predicted_quantity'] = df['predicted_quantity'].apply(lambda x: float(x))\n",
    "df.loc[df['predicted_quantity']>200, 'predicted_quantity'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a44ed111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.71% match of quantity\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns={'predicted_quantity': 'Quantity_predicted'})\n",
    "df['Quantity_predicted'] = df['Quantity_predicted'].fillna(0)\n",
    "df.loc[df['expected_Quantity']=='nan', 'expected_Quantity'] = 0\n",
    "df['expected_Quantity'] = df['expected_Quantity'].astype(str).apply(remove_non_digits)\n",
    "df['expected_Quantity'] = df['expected_Quantity'].astype(float, errors='ignore')\n",
    "df['expected_Quantity'] = df['expected_Quantity'].replace('', 0)\n",
    "df['expected_Quantity'] = df['expected_Quantity'].astype(float, errors='ignore')\n",
    "#df['expected_Quantity'] = df['expected_Quantity'].fillna(0)\n",
    "decimals = 1\n",
    "df['expected_Quantity'] = df['expected_Quantity'].apply(lambda x: round(x, decimals))\n",
    "df['Quantity_predicted'] = df['Quantity_predicted'].apply(lambda x: round(x, decimals))\n",
    "# df['Quantity_predicted'] = df['Quantity_predicted'].astype(int)\n",
    "# df['expected_Quantity'] = df['expected_Quantity'].astype(int)\n",
    "matched, un_matched = df[df['expected_Quantity']==df['Quantity_predicted']].shape[0],df[df['expected_Quantity']!=df['Quantity_predicted']].shape[0]\n",
    "a = matched/(matched+un_matched)\n",
    "print('{}% match of quantity'.format(round(a*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "40334140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity_predicted</th>\n",
       "      <th>expected_Quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Quantity_predicted  expected_Quantity\n",
       "6                  2.0                0.0\n",
       "12                 2.0                1.0\n",
       "15                 0.0                1.0\n",
       "18                 0.0                1.0\n",
       "29                 0.0                1.0\n",
       "31                 0.0                1.0\n",
       "44                 1.0               17.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['expected_Quantity']!=df['Quantity_predicted']][['Quantity_predicted', 'expected_Quantity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f33ee",
   "metadata": {},
   "source": [
    "I will ask chatgpt combinedly for quantity and weight at the end, if possible dimension also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d98081",
   "metadata": {},
   "source": [
    "### 4. Weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d6f5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weight(text):\n",
    "    text = text.replace(\"'\", '')\n",
    "    \n",
    "    regex_patterns = [\n",
    "    r'\\d+(?:,\\d{3})*(?:\\.\\d+)? kg ',\n",
    "    r'\\d+(?:,\\d{3})*(?:\\.\\d+)? kgs ',\n",
    "    r'\\d+(?:,\\d{3})*(?:\\.\\d+)? kg ',\n",
    "    r'\\d+(?:,\\d{3})*(?:\\.\\d+)? kgs',\n",
    "    r'(kg): \\d+(?:,\\d{3})*(?:\\.\\d+)?',\n",
    "    r'kg \\d+(?:,\\d{3})*(?:\\.\\d+)?',\n",
    "    r'kgs \\d+(?:,\\d{3})*(?:\\.\\d+)?',\n",
    "        # have to include comma cases, have included it but its not working as expected\n",
    "        # i am excluding other units except kg, to avoid false values. Its better to return none than to return anything\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)? lb',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)? lbs',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)?lb',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)?lbs',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)? pound',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)? pounds',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)?pound',\n",
    "#     r'\\d+(?:,\\d{3})*(?:\\.\\d+)?pounds',\n",
    "     ]\n",
    "\n",
    "\n",
    "#     regex_patterns = [\n",
    "#         r'\\d+ kg ', r'\\d+ kgs ', r'\\d+ kg ',r'\\d+ kgs',r'(kg): \\d+',\n",
    "#         r'kg \\d+', r'kgs \\d+',\n",
    "#         r'\\d+ lb', r'\\d+ lbs', r'\\d+lb',r'\\d+lbs',\n",
    "#         r'\\d+ pound', r'\\d+ pounds', r'\\d+pound',r'\\d+pounds',\n",
    "#         ]\n",
    "\n",
    "\n",
    "    qty = []\n",
    "    for pattern in regex_patterns:\n",
    "        matches = re.findall(pattern, text)\n",
    "        if matches:\n",
    "            qty.extend(matches)\n",
    "            break\n",
    "\n",
    "    return qty\n",
    "\n",
    "df['predicted_Weight_raw'] = df['email_processed'].apply(process_weight)\n",
    "\n",
    "df['predicted_Weight'] = [','.join(map(str, l)) for l in df['predicted_Weight_raw']]\n",
    "\n",
    "# Fix scenarios where multiple digits are extracted\n",
    "\n",
    "df['predicted_Weight'] = df['predicted_Weight'].str.extract('(\\d+[.\\d]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c0daf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailid</th>\n",
       "      <th>email_processed</th>\n",
       "      <th>predicted_Weight_raw</th>\n",
       "      <th>predicted_Weight</th>\n",
       "      <th>expected_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>dap rate hello could i have dap charges for th...</td>\n",
       "      <td>[3400 kgs ]</td>\n",
       "      <td>3400</td>\n",
       "      <td>3400 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>exw rate hello could i have exw charges for th...</td>\n",
       "      <td>[1044 kgs ]</td>\n",
       "      <td>1044</td>\n",
       "      <td>1044 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Abbie Hood (UK-LON ECU Worldwide)\" &lt;AbbieHood...</td>\n",
       "      <td>dap ratehello could i have dap charges for the...</td>\n",
       "      <td>[6500.000 kgs , 6500.00 kgs ]</td>\n",
       "      <td>6500.000</td>\n",
       "      <td>6500 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>\"Berner, Nadine / Kuehne + Nagel / Tor ZS-BA\"\\...</td>\n",
       "      <td>kn rate request/ vancouver to barraquilla, co/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1632.9 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Bob Everett (UK-LON ECU Worldwide)\" &lt;BobEvere...</td>\n",
       "      <td>fw: freight quote - jkf - canada - sea - ac go...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>997 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>\"Denislav Chipev (BG-SOF ECU Worldwide)\"\\n\\t&lt;D...</td>\n",
       "      <td>dap charges l4m 4y8, canada / 3 pcs hi, please...</td>\n",
       "      <td>[4604 kg ]</td>\n",
       "      <td>4604</td>\n",
       "      <td>4604 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Denislav Chipev (BG-SOF ECU Worldwide)\"_x000D...</td>\n",
       "      <td>dap to v7w-1w1 west vancouver, british columbi...</td>\n",
       "      <td>[150 kg ]</td>\n",
       "      <td>150</td>\n",
       "      <td>150 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>\"JANI, TWISHA\" &lt;twisha.jani@dbschenker.com&gt;</td>\n",
       "      <td>re: new booking from montreal to shanghai hi a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>\"Karina Alonso (ES-BCN ECU Worldwide)\" &lt;Karina...</td>\n",
       "      <td>hi all pls could you send us your better dap r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>\"Krizia  Mejia\" &lt;krizia.mejia@etheinsen.com.do&gt;</td>\n",
       "      <td>rate request // lcl /canada - rd // raver rovr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2101.94 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Muna Khalifa (DHL CA)\" &lt;muna.khalifa@dhl.com&gt;</td>\n",
       "      <td>quote request ** po12 newmont project hello al...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.85 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>\"Ramon Souza (BR-SAO ECU Worldwide)\" &lt;RamonSou...</td>\n",
       "      <td>tfa cargo logistics - cotação exportação marít...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"Rhea Pereira (DHL CA)\" &lt;rhea.pereira@dhl.com&gt;</td>\n",
       "      <td>rate request- network kw hi ecu, please advise...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>201.39 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Sales Milan (IT-MIL ECU Worldwide)\" &lt;SalesMil...</td>\n",
       "      <td>edmonton via montreal dsv hi dear we have to q...</td>\n",
       "      <td>[kg 6030]</td>\n",
       "      <td>6030</td>\n",
       "      <td>6030 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>\"leparent@fffc.ca\" &lt;leparent@fffc.ca&gt;</td>\n",
       "      <td>rate request terminal toronto to door tainan c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>272.15 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>\"matt@aufreight.com\" &lt;matt@aufreight.com&gt;</td>\n",
       "      <td>re: export quote you don't often get email fro...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1236.03 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>\"matt@aufreight.com\" &lt;matt@aufreight.com&gt;</td>\n",
       "      <td>re: export question you don't often get email ...</td>\n",
       "      <td>[1239 kgs ]</td>\n",
       "      <td>1239</td>\n",
       "      <td>1239 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>hawai you don't often get email from sbgaudet ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.85 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Alisha Lokhandwala &lt;Alisha.Lokhandwala@flyingf...</td>\n",
       "      <td>req ca to uk ocean q7556, sfi hello francois a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>467.2 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Anastasiia Howard &lt;anastasiia.howard@leggett.com&gt;</td>\n",
       "      <td>lcl / po-1302 / sundern-amecke to london, cana...</td>\n",
       "      <td>[464 kg ]</td>\n",
       "      <td>464</td>\n",
       "      <td>464 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Brian | 1UP Cargo &lt;brian@1upcargo.com&gt;</td>\n",
       "      <td>re: alonso : exw : lcl : canada to india :21 h...</td>\n",
       "      <td>[7160 kgs ]</td>\n",
       "      <td>7160</td>\n",
       "      <td>7160 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>David Keung &lt;David.Keung@hellmann.com&gt;</td>\n",
       "      <td>dg shipment to guayaquil ( quotation &amp; booking...</td>\n",
       "      <td>[724 kg ]</td>\n",
       "      <td>724</td>\n",
       "      <td>724 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Diego - Brytor International &lt;diego@ims.brytor...</td>\n",
       "      <td>re: urgent request -lcl quote ref:mtr- | from ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Edgar Azar &lt;edgar.azar@coleintl.com&gt;</td>\n",
       "      <td>fob lcl rate request /jebel ali to toronto hel...</td>\n",
       "      <td>[72.00 kgs ]</td>\n",
       "      <td>72.00</td>\n",
       "      <td>72 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gaston Rioux &lt;ricointernational@videotron.ca&gt;</td>\n",
       "      <td>urgent lcl quote require from ex-works to term...</td>\n",
       "      <td>[4151.70 kgs ]</td>\n",
       "      <td>4151.70</td>\n",
       "      <td>4151.70 kgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gelena | Sealion Cargo &lt;gelena@sealioncargo.com&gt;</td>\n",
       "      <td>ltl request toronto-new zealand/ q19989 hello,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>680.38 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Grace Curro &lt;Grace@atlanticandpacific.com&gt;</td>\n",
       "      <td>urgent - pls advise lcl quote ref:tor- | from ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HLIWA Bozena &lt;Bozena.HLIWA@bollore.com&gt;</td>\n",
       "      <td>rate request lcl cargo non stackable cfs termi...</td>\n",
       "      <td>[880 kgs ]</td>\n",
       "      <td>880</td>\n",
       "      <td>880 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>JALI Ussama &lt;ussama.jali@bollore.com&gt;</td>\n",
       "      <td>uj lcl dap munchen hi team, please quote from ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1657.88 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jude Menezes &lt;jude@zodiacimpex.com&gt;</td>\n",
       "      <td>lcl rate request toronto to singapore you don'...</td>\n",
       "      <td>[1054 kgs ]</td>\n",
       "      <td>1054</td>\n",
       "      <td>1054 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Junaid Jawed &lt;Junaid.Jawed@chrobinson.com&gt;</td>\n",
       "      <td>export quote required from toronto to southamp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>544.31 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Kaushik Patel &lt;Kaushik.Patel@lei.ca&gt;</td>\n",
       "      <td>cfs toronto to door 44550 montoir-de-bretagne ...</td>\n",
       "      <td>[454 kgs , 454 kgs ]</td>\n",
       "      <td>454</td>\n",
       "      <td>454 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>rate request | vancouver, canada to darlington...</td>\n",
       "      <td>[kg 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>300 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>rate request | vancouver, canada to highton, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Linda Hernandez &lt;pricing@allegrofreight.com&gt;</td>\n",
       "      <td>lcl quote ref 011858 hi, please quote. commodi...</td>\n",
       "      <td>[176.90 kgs ]</td>\n",
       "      <td>176.90</td>\n",
       "      <td>176.9 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linda-Nancy Colizza &lt;Linda-Nancy.Colizza@hellm...</td>\n",
       "      <td>santos good morning, need rate please, termina...</td>\n",
       "      <td>[2146 kg ]</td>\n",
       "      <td>2146</td>\n",
       "      <td>2146 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maria Aragao &lt;Maria.Aragao@craneww.com&gt;</td>\n",
       "      <td>rfq ocean freight - helsinki finland to anjou ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1404.32 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Maria DeBellis &lt;debellism@delmar.ca&gt;</td>\n",
       "      <td>re: re lcl service from france to canada you d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle Guo &lt;michelle.guo@sparxlogistics.com&gt;</td>\n",
       "      <td>re: quote from toronto terminal to london term...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1859.75kgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Mike Gray &lt;info@mglogistics.ca&gt;</td>\n",
       "      <td>ocean rate request - lcl (door delivery) good ...</td>\n",
       "      <td>[1,212.0 kg ]</td>\n",
       "      <td>1</td>\n",
       "      <td>1212 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mouhcine - Brytor International &lt;mouhcine@ims....</td>\n",
       "      <td>dthc ! good day, i would need a rate from cfs ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Operation - OTX Canada &lt;operation@otxcanada.com&gt;</td>\n",
       "      <td>montreal to port klang some people who receive...</td>\n",
       "      <td>[2475.00 kgs ]</td>\n",
       "      <td>2475.00</td>\n",
       "      <td>2475 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Operation - OTX Canada &lt;operation@otxcanada.com&gt;</td>\n",
       "      <td>re: lcl montreal to port klang some people who...</td>\n",
       "      <td>[2475.00 kgs ]</td>\n",
       "      <td>2475.00</td>\n",
       "      <td>2475 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sally Chieng &lt;sallyc@itn-logistics.ca&gt;</td>\n",
       "      <td>rfq / itn door winnipeg, mb to cfs shanghai, c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3991.61 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sandro Avilez Caldart &lt;savilez@willsonintl.com&gt;</td>\n",
       "      <td>lcl import | term toronto, on, ca to port of s...</td>\n",
       "      <td>[3,316 kgs , 1,434 kgs ]</td>\n",
       "      <td>3</td>\n",
       "      <td>3315.76 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sarah | 1UP Cargo Pricing &lt;Sarah@1upcargo.com&gt;</td>\n",
       "      <td>fw: canada exw rates form door to nansha,guang...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4900 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sendy Duplan &lt;Sendy@Falconfreight.ca&gt;</td>\n",
       "      <td>rate request // 3 dg skids @ 1984 kgs // from ...</td>\n",
       "      <td>[1984 kg ]</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984 kgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simon Holt &lt;Simon.holt@nordic-on.com&gt;</td>\n",
       "      <td>vs: lcl quote request | from location : britis...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Trinet Team Kilo &lt;kilo@gotrinet.com&gt;</td>\n",
       "      <td>urgent rate request //techpack you don't often...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              emailid  \\\n",
       "34  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "30  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "13  \"Abbie Hood (UK-LON ECU Worldwide)\" <AbbieHood...   \n",
       "36  \"Berner, Nadine / Kuehne + Nagel / Tor ZS-BA\"\\...   \n",
       "29  \"Bob Everett (UK-LON ECU Worldwide)\" <BobEvere...   \n",
       "45  \"Denislav Chipev (BG-SOF ECU Worldwide)\"\\n\\t<D...   \n",
       "12  \"Denislav Chipev (BG-SOF ECU Worldwide)\"_x000D...   \n",
       "39        \"JANI, TWISHA\" <twisha.jani@dbschenker.com>   \n",
       "23  \"Karina Alonso (ES-BCN ECU Worldwide)\" <Karina...   \n",
       "18    \"Krizia  Mejia\" <krizia.mejia@etheinsen.com.do>   \n",
       "14     \"Muna Khalifa (DHL CA)\" <muna.khalifa@dhl.com>   \n",
       "22  \"Ramon Souza (BR-SAO ECU Worldwide)\" <RamonSou...   \n",
       "26     \"Rhea Pereira (DHL CA)\" <rhea.pereira@dhl.com>   \n",
       "11  \"Sales Milan (IT-MIL ECU Worldwide)\" <SalesMil...   \n",
       "43              \"leparent@fffc.ca\" <leparent@fffc.ca>   \n",
       "41          \"matt@aufreight.com\" <matt@aufreight.com>   \n",
       "47          \"matt@aufreight.com\" <matt@aufreight.com>   \n",
       "15                                                  0   \n",
       "44  Alisha Lokhandwala <Alisha.Lokhandwala@flyingf...   \n",
       "28  Anastasiia Howard <anastasiia.howard@leggett.com>   \n",
       "42             Brian | 1UP Cargo <brian@1upcargo.com>   \n",
       "33             David Keung <David.Keung@hellmann.com>   \n",
       "46  Diego - Brytor International <diego@ims.brytor...   \n",
       "10               Edgar Azar <edgar.azar@coleintl.com>   \n",
       "16      Gaston Rioux <ricointernational@videotron.ca>   \n",
       "27   Gelena | Sealion Cargo <gelena@sealioncargo.com>   \n",
       "25         Grace Curro <Grace@atlanticandpacific.com>   \n",
       "4             HLIWA Bozena <Bozena.HLIWA@bollore.com>   \n",
       "38              JALI Ussama <ussama.jali@bollore.com>   \n",
       "7                 Jude Menezes <jude@zodiacimpex.com>   \n",
       "31         Junaid Jawed <Junaid.Jawed@chrobinson.com>   \n",
       "48               Kaushik Patel <Kaushik.Patel@lei.ca>   \n",
       "6               LCL Operations <lcl@goworldcargo.com>   \n",
       "5               LCL Operations <lcl@goworldcargo.com>   \n",
       "35       Linda Hernandez <pricing@allegrofreight.com>   \n",
       "1   Linda-Nancy Colizza <Linda-Nancy.Colizza@hellm...   \n",
       "21            Maria Aragao <Maria.Aragao@craneww.com>   \n",
       "17               Maria DeBellis <debellism@delmar.ca>   \n",
       "3      Michelle Guo <michelle.guo@sparxlogistics.com>   \n",
       "37                    Mike Gray <info@mglogistics.ca>   \n",
       "32  Mouhcine - Brytor International <mouhcine@ims....   \n",
       "24   Operation - OTX Canada <operation@otxcanada.com>   \n",
       "40   Operation - OTX Canada <operation@otxcanada.com>   \n",
       "2              Sally Chieng <sallyc@itn-logistics.ca>   \n",
       "8     Sandro Avilez Caldart <savilez@willsonintl.com>   \n",
       "9      Sarah | 1UP Cargo Pricing <Sarah@1upcargo.com>   \n",
       "19              Sendy Duplan <Sendy@Falconfreight.ca>   \n",
       "0               Simon Holt <Simon.holt@nordic-on.com>   \n",
       "20               Trinet Team Kilo <kilo@gotrinet.com>   \n",
       "\n",
       "                                      email_processed  \\\n",
       "34  dap rate hello could i have dap charges for th...   \n",
       "30  exw rate hello could i have exw charges for th...   \n",
       "13  dap ratehello could i have dap charges for the...   \n",
       "36  kn rate request/ vancouver to barraquilla, co/...   \n",
       "29  fw: freight quote - jkf - canada - sea - ac go...   \n",
       "45  dap charges l4m 4y8, canada / 3 pcs hi, please...   \n",
       "12  dap to v7w-1w1 west vancouver, british columbi...   \n",
       "39  re: new booking from montreal to shanghai hi a...   \n",
       "23  hi all pls could you send us your better dap r...   \n",
       "18  rate request // lcl /canada - rd // raver rovr...   \n",
       "14  quote request ** po12 newmont project hello al...   \n",
       "22  tfa cargo logistics - cotação exportação marít...   \n",
       "26  rate request- network kw hi ecu, please advise...   \n",
       "11  edmonton via montreal dsv hi dear we have to q...   \n",
       "43  rate request terminal toronto to door tainan c...   \n",
       "41  re: export quote you don't often get email fro...   \n",
       "47  re: export question you don't often get email ...   \n",
       "15  hawai you don't often get email from sbgaudet ...   \n",
       "44  req ca to uk ocean q7556, sfi hello francois a...   \n",
       "28  lcl / po-1302 / sundern-amecke to london, cana...   \n",
       "42  re: alonso : exw : lcl : canada to india :21 h...   \n",
       "33  dg shipment to guayaquil ( quotation & booking...   \n",
       "46  re: urgent request -lcl quote ref:mtr- | from ...   \n",
       "10  fob lcl rate request /jebel ali to toronto hel...   \n",
       "16  urgent lcl quote require from ex-works to term...   \n",
       "27  ltl request toronto-new zealand/ q19989 hello,...   \n",
       "25  urgent - pls advise lcl quote ref:tor- | from ...   \n",
       "4   rate request lcl cargo non stackable cfs termi...   \n",
       "38  uj lcl dap munchen hi team, please quote from ...   \n",
       "7   lcl rate request toronto to singapore you don'...   \n",
       "31  export quote required from toronto to southamp...   \n",
       "48  cfs toronto to door 44550 montoir-de-bretagne ...   \n",
       "6   rate request | vancouver, canada to darlington...   \n",
       "5   rate request | vancouver, canada to highton, a...   \n",
       "35  lcl quote ref 011858 hi, please quote. commodi...   \n",
       "1   santos good morning, need rate please, termina...   \n",
       "21  rfq ocean freight - helsinki finland to anjou ...   \n",
       "17  re: re lcl service from france to canada you d...   \n",
       "3   re: quote from toronto terminal to london term...   \n",
       "37  ocean rate request - lcl (door delivery) good ...   \n",
       "32  dthc ! good day, i would need a rate from cfs ...   \n",
       "24  montreal to port klang some people who receive...   \n",
       "40  re: lcl montreal to port klang some people who...   \n",
       "2   rfq / itn door winnipeg, mb to cfs shanghai, c...   \n",
       "8   lcl import | term toronto, on, ca to port of s...   \n",
       "9   fw: canada exw rates form door to nansha,guang...   \n",
       "19  rate request // 3 dg skids @ 1984 kgs // from ...   \n",
       "0   vs: lcl quote request | from location : britis...   \n",
       "20  urgent rate request //techpack you don't often...   \n",
       "\n",
       "             predicted_Weight_raw predicted_Weight expected_Weight  \n",
       "34                    [3400 kgs ]             3400         3400 kg  \n",
       "30                    [1044 kgs ]             1044         1044 kg  \n",
       "13  [6500.000 kgs , 6500.00 kgs ]         6500.000         6500 kg  \n",
       "36                             []              NaN       1632.9 kg  \n",
       "29                             []              NaN          997 kg  \n",
       "45                     [4604 kg ]             4604         4604 kg  \n",
       "12                      [150 kg ]              150          150 kg  \n",
       "39                             []              NaN             nan  \n",
       "23                             []              NaN            4 kg  \n",
       "18                             []              NaN      2101.94 kg  \n",
       "14                             []              NaN        25.85 kg  \n",
       "22                             []              NaN             nan  \n",
       "26                             []              NaN       201.39 kg  \n",
       "11                      [kg 6030]             6030         6030 kg  \n",
       "43                             []              NaN       272.15 kg  \n",
       "41                             []              NaN      1236.03 kg  \n",
       "47                    [1239 kgs ]             1239         1239 kg  \n",
       "15                             []              NaN        69.85 kg  \n",
       "44                             []              NaN        467.2 kg  \n",
       "28                      [464 kg ]              464          464 kg  \n",
       "42                    [7160 kgs ]             7160         7160 kg  \n",
       "33                      [724 kg ]              724          724 kg  \n",
       "46                             []              NaN             nan  \n",
       "10                   [72.00 kgs ]            72.00           72 kg  \n",
       "16                 [4151.70 kgs ]          4151.70     4151.70 kgs  \n",
       "27                             []              NaN       680.38 kg  \n",
       "25                             []              NaN             nan  \n",
       "4                      [880 kgs ]              880          880 kg  \n",
       "38                             []              NaN      1657.88 kg  \n",
       "7                     [1054 kgs ]             1054         1054 kg  \n",
       "31                             []              NaN       544.31 kg  \n",
       "48           [454 kgs , 454 kgs ]              454          454 kg  \n",
       "6                          [kg 1]                1          300 kg  \n",
       "5                              []              NaN             nan  \n",
       "35                  [176.90 kgs ]           176.90        176.9 kg  \n",
       "1                      [2146 kg ]             2146         2146 kg  \n",
       "21                             []              NaN      1404.32 kg  \n",
       "17                             []              NaN             nan  \n",
       "3                              []              NaN      1859.75kgs  \n",
       "37                  [1,212.0 kg ]                1         1212 kg  \n",
       "32                             []              NaN          300 kg  \n",
       "24                 [2475.00 kgs ]          2475.00         2475 kg  \n",
       "40                 [2475.00 kgs ]          2475.00         2475 kg  \n",
       "2                              []              NaN      3991.61 kg  \n",
       "8        [3,316 kgs , 1,434 kgs ]                3      3315.76 kg  \n",
       "9                              []              NaN         4900 kg  \n",
       "19                     [1984 kg ]             1984        1984 kgs  \n",
       "0                              []              NaN             nan  \n",
       "20                             []              NaN             nan  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 13 - 65.00 - account for decimal\n",
    "df[['emailid', 'email_processed','predicted_Weight_raw','predicted_Weight','expected_Weight']].sort_values(by = 'emailid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "226c8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_Weight'] = df['predicted_Weight'].astype(float)\n",
    "df.loc[df['predicted_Weight'].notnull(), 'predicted_Weight_unit']='kg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "419960cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['predicted_Weight'].isnull(), 'predicted_Weight']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d10f854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 87)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['predicted_Weight']==''].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef897bd9",
   "metadata": {},
   "source": [
    "## ChatGPT for weight and quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "746e960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weight_from_quotations(email):\n",
    "    prompt = f\"\"\"\n",
    "    Extract these informations from the given quotation email.\n",
    "    \n",
    "    Weight - Compute total weight in kg if there are multiple packages. Consider gross weight over net weight.\n",
    "    unit of weight - Compute the unit of weight.\n",
    "    \n",
    "    Weight: [Weight]\n",
    "    Unit of Weight: [unit of weight]\n",
    "    \n",
    "    The response should include only 2 lines with 2 fields mentioned above in the same order. Return fields with \"None\" if there is insufficient information.\n",
    "        \n",
    "    Review: ```{email}```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "#     weight = response.split(sep='\\n')[0]\n",
    "#     weight_unit = response.split(sep='\\n')[1]\n",
    "    \n",
    "    return response\n",
    "\n",
    "quotation_columns = ['weight', 'weight_unit']\n",
    "#df[quotation_columns] = ['', '']\n",
    "\n",
    "for i, value in enumerate(df['email_processed']):\n",
    "    weight = df.loc[i, 'predicted_Weight']\n",
    "    if((weight=='')):\n",
    "        result = extract_weight_from_quotations(value)\n",
    "        df.loc[i, 'predicted_Weight'] = result.split(sep='\\n')[0]\n",
    "        df.loc[i, 'predicted_Weight_unit'] = result.split(sep='\\n')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8262a7",
   "metadata": {},
   "source": [
    "### Save intermediate pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a3280b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle('weight_extraction_24_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2c655635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('weight_extraction_24_8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49121463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_Weight</th>\n",
       "      <th>predicted_Weight_unit</th>\n",
       "      <th>expected_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2146.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>2146 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weight: 8800 lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>3991.61 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weight: 14,759.75 kgs</td>\n",
       "      <td>Unit of Weight: kgs</td>\n",
       "      <td>1859.75kgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>880.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>880 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>300 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1054.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>1054 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>3315.76 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weight: 4900kg</td>\n",
       "      <td>Unit of Weight: kg</td>\n",
       "      <td>4900 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>72.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>72 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6030.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>6030 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>150 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6500.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>6500 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Weight: 57 pound</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>25.85 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Weight: 154 pounds</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>69.85 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4151.7</td>\n",
       "      <td>kg</td>\n",
       "      <td>4151.70 kgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Weight: 4,634 pounds</td>\n",
       "      <td>Unit of Weight: pounds</td>\n",
       "      <td>2101.94 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1984.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>1984 kgs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Weight: 3196 lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>1404.32 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Weight: 4.000</td>\n",
       "      <td>Unit of Weight: kg</td>\n",
       "      <td>4 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2475.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>2475 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Weight: 444 lb</td>\n",
       "      <td>Unit of Weight: lb</td>\n",
       "      <td>201.39 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Weight: 1500</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>680.38 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>464.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>464 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>997 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1044.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>1044 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Weight: 1,200 lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>544.31 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Weight: 300 kg</td>\n",
       "      <td>Unit of Weight: kg</td>\n",
       "      <td>300 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>724.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>724 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3400.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>3400 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>176.9</td>\n",
       "      <td>kg</td>\n",
       "      <td>176.9 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Weight: 3600 lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>1632.9 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>1212 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Weight: 3155 kg</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>1657.88 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2475.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>2475 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Weight: 2725 lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>1236.03 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7160.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>7160 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Weight: 600lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>272.15 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Weight: 1,030 lbs</td>\n",
       "      <td>Unit of Weight: lbs</td>\n",
       "      <td>467.2 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4604.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>4604 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Weight: None</td>\n",
       "      <td>Unit of Weight: None</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1239.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>1239 kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>454.0</td>\n",
       "      <td>kg</td>\n",
       "      <td>454 kg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predicted_Weight   predicted_Weight_unit expected_Weight\n",
       "0            Weight: None    Unit of Weight: None             nan\n",
       "1                  2146.0                      kg         2146 kg\n",
       "2        Weight: 8800 lbs     Unit of Weight: lbs      3991.61 kg\n",
       "3   Weight: 14,759.75 kgs     Unit of Weight: kgs      1859.75kgs\n",
       "4                   880.0                      kg          880 kg\n",
       "5            Weight: None    Unit of Weight: None             nan\n",
       "6                     1.0                      kg          300 kg\n",
       "7                  1054.0                      kg         1054 kg\n",
       "8                     3.0                      kg      3315.76 kg\n",
       "9          Weight: 4900kg      Unit of Weight: kg         4900 kg\n",
       "10                   72.0                      kg           72 kg\n",
       "11                 6030.0                      kg         6030 kg\n",
       "12                  150.0                      kg          150 kg\n",
       "13                 6500.0                      kg         6500 kg\n",
       "14       Weight: 57 pound    Unit of Weight: None        25.85 kg\n",
       "15     Weight: 154 pounds    Unit of Weight: None        69.85 kg\n",
       "16                 4151.7                      kg     4151.70 kgs\n",
       "17           Weight: None    Unit of Weight: None             nan\n",
       "18   Weight: 4,634 pounds  Unit of Weight: pounds      2101.94 kg\n",
       "19                 1984.0                      kg        1984 kgs\n",
       "20           Weight: None    Unit of Weight: None             nan\n",
       "21       Weight: 3196 lbs     Unit of Weight: lbs      1404.32 kg\n",
       "22           Weight: None    Unit of Weight: None             nan\n",
       "23          Weight: 4.000      Unit of Weight: kg            4 kg\n",
       "24                 2475.0                      kg         2475 kg\n",
       "25           Weight: None    Unit of Weight: None             nan\n",
       "26         Weight: 444 lb      Unit of Weight: lb       201.39 kg\n",
       "27           Weight: 1500     Unit of Weight: lbs       680.38 kg\n",
       "28                  464.0                      kg          464 kg\n",
       "29           Weight: None    Unit of Weight: None          997 kg\n",
       "30                 1044.0                      kg         1044 kg\n",
       "31      Weight: 1,200 lbs     Unit of Weight: lbs       544.31 kg\n",
       "32         Weight: 300 kg      Unit of Weight: kg          300 kg\n",
       "33                  724.0                      kg          724 kg\n",
       "34                 3400.0                      kg         3400 kg\n",
       "35                  176.9                      kg        176.9 kg\n",
       "36       Weight: 3600 lbs     Unit of Weight: lbs       1632.9 kg\n",
       "37                    1.0                      kg         1212 kg\n",
       "38        Weight: 3155 kg     Unit of Weight: lbs      1657.88 kg\n",
       "39           Weight: None    Unit of Weight: None             nan\n",
       "40                 2475.0                      kg         2475 kg\n",
       "41       Weight: 2725 lbs     Unit of Weight: lbs      1236.03 kg\n",
       "42                 7160.0                      kg         7160 kg\n",
       "43         Weight: 600lbs     Unit of Weight: lbs       272.15 kg\n",
       "44      Weight: 1,030 lbs     Unit of Weight: lbs        467.2 kg\n",
       "45                 4604.0                      kg         4604 kg\n",
       "46           Weight: None    Unit of Weight: None             nan\n",
       "47                 1239.0                      kg         1239 kg\n",
       "48                  454.0                      kg          454 kg"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['predicted_Weight', 'predicted_Weight_unit', 'expected_Weight']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ee8d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pounds_to_kgs(weight_in_lbs):\n",
    "    pounds_per_kg = 0.45359237\n",
    "    weight_in_kgs = weight_in_lbs * pounds_per_kg\n",
    "    return weight_in_kgs\n",
    "\n",
    "df['predicted_Weight'] = df['predicted_Weight'].astype(str)\n",
    "df['predicted_Weight'] = df['predicted_Weight'].str.replace('Weight: ','')\n",
    "df['predicted_Weight'] = df['predicted_Weight'].str.replace('Weight:','')\n",
    "df['predicted_Weight_unit'] = df['predicted_Weight_unit'].str.replace('Unit of Weight: ', '')\n",
    "df['predicted_Weight_unit'] = df['predicted_Weight_unit'].str.replace('Unit of Weight:', '')\n",
    "df['predicted_Weight'] = df['predicted_Weight'].fillna('')\n",
    "df['predicted_Weight'] = df['predicted_Weight'].str.replace('None', '')\n",
    "df.loc[df['predicted_Weight'].str.contains('lb|lbs|pounds'), 'predicted_Weight_unit'] = 'pounds'\n",
    "df['predicted_Weight'] = df['predicted_Weight'].apply(remove_non_digits)\n",
    "df.loc[df['predicted_Weight'] == '', 'predicted_Weight'] = np.nan\n",
    "df.loc[df['predicted_Weight'].str.count('[.]')>1, 'predicted_Weight']= np.nan\n",
    "df['predicted_Weight'] = df['predicted_Weight'].apply(float)\n",
    "df.loc[df['predicted_Weight']>100000, 'predicted_Weight'] = 1\n",
    "df.loc[df['predicted_Weight_unit'] == 'pounds', 'predicted_Weight'] = df['predicted_Weight'].apply(convert_pounds_to_kgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4acc5f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.55% match of weight\n"
     ]
    }
   ],
   "source": [
    "df['predicted_Weight'] = df['predicted_Weight'].astype(str)\n",
    "df['predicted_Weight'] = df['predicted_Weight'].apply(remove_letters)\n",
    "df['predicted_Weight'] = df['predicted_Weight'].apply(remove_non_digits)\n",
    "df.loc[df['predicted_Weight']=='', 'predicted_Weight'] = np.nan\n",
    "df['predicted_Weight'] = df['predicted_Weight'].fillna(0)\n",
    "df['predicted_Weight'] = df['predicted_Weight'].apply(lambda x: float(x))\n",
    "\n",
    "df = df.rename(columns={\"predicted_Weight\": 'weight_predicted'})\n",
    "df['weight_predicted'] = df['weight_predicted'].fillna(0)\n",
    "df.loc[df['expected_Weight']=='nan', 'expected_Weight'] = 0\n",
    "df['expected_Weight'] = df['expected_Weight'].astype(str).apply(remove_non_digits)\n",
    "df['expected_Weight'] = df['expected_Weight'].astype(float, errors='ignore')\n",
    "df['expected_Weight'] = df['expected_Weight'].replace('', 0)\n",
    "df['expected_Weight'] = df['expected_Weight'].astype(float, errors='ignore')\n",
    "decimals = 1\n",
    "df['expected_Weight'] = df['expected_Weight'].apply(lambda x: round(x, decimals))\n",
    "df['weight_predicted'] = df['weight_predicted'].apply(lambda x: round(x, decimals))\n",
    "matched, un_matched = df[df['expected_Weight']==df['weight_predicted']].shape[0],df[df['expected_Weight']!=df['weight_predicted']].shape[0]\n",
    "a = matched/(matched+un_matched)\n",
    "print('{}% match of weight'.format(round(a*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "152e8ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8367346938775511"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(41/49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1bb0055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailid</th>\n",
       "      <th>predicted_Weight_raw</th>\n",
       "      <th>weight_predicted</th>\n",
       "      <th>expected_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle Guo &lt;michelle.guo@sparxlogistics.com&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>14759.8</td>\n",
       "      <td>1859.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LCL Operations &lt;lcl@goworldcargo.com&gt;</td>\n",
       "      <td>[kg 1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sandro Avilez Caldart &lt;savilez@willsonintl.com&gt;</td>\n",
       "      <td>[3,316 kgs , 1,434 kgs ]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3315.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"Muna Khalifa (DHL CA)\" &lt;muna.khalifa@dhl.com&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>57.0</td>\n",
       "      <td>25.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>69.9</td>\n",
       "      <td>69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maria Aragao &lt;Maria.Aragao@craneww.com&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1449.7</td>\n",
       "      <td>1404.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Gelena | Sealion Cargo &lt;gelena@sealioncargo.com&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>680.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\"Bob Everett (UK-LON ECU Worldwide)\" &lt;BobEvere...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Mike Gray &lt;info@mglogistics.ca&gt;</td>\n",
       "      <td>[1,212.0 kg ]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>JALI Ussama &lt;ussama.jali@bollore.com&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>3155.0</td>\n",
       "      <td>1657.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>\"leparent@fffc.ca\" &lt;leparent@fffc.ca&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>272.2</td>\n",
       "      <td>272.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              emailid  \\\n",
       "3      Michelle Guo <michelle.guo@sparxlogistics.com>   \n",
       "6               LCL Operations <lcl@goworldcargo.com>   \n",
       "8     Sandro Avilez Caldart <savilez@willsonintl.com>   \n",
       "14     \"Muna Khalifa (DHL CA)\" <muna.khalifa@dhl.com>   \n",
       "15                                                  0   \n",
       "21            Maria Aragao <Maria.Aragao@craneww.com>   \n",
       "27   Gelena | Sealion Cargo <gelena@sealioncargo.com>   \n",
       "29  \"Bob Everett (UK-LON ECU Worldwide)\" <BobEvere...   \n",
       "37                    Mike Gray <info@mglogistics.ca>   \n",
       "38              JALI Ussama <ussama.jali@bollore.com>   \n",
       "43              \"leparent@fffc.ca\" <leparent@fffc.ca>   \n",
       "\n",
       "        predicted_Weight_raw  weight_predicted  expected_Weight  \n",
       "3                         []           14759.8           1859.8  \n",
       "6                     [kg 1]               1.0            300.0  \n",
       "8   [3,316 kgs , 1,434 kgs ]               3.0           3315.8  \n",
       "14                        []              57.0             25.9  \n",
       "15                        []              69.9             69.8  \n",
       "21                        []            1449.7           1404.3  \n",
       "27                        []            1500.0            680.4  \n",
       "29                        []               0.0            997.0  \n",
       "37             [1,212.0 kg ]               1.0           1212.0  \n",
       "38                        []            3155.0           1657.9  \n",
       "43                        []             272.2            272.1  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['expected_Weight']!=df['weight_predicted']][['emailid','predicted_Weight_raw','weight_predicted', 'expected_Weight']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
